[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Analysis for Bioscientists",
    "section": "",
    "text": "Welcome!\nfront page stuff",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "\n1¬† About this book\n",
    "section": "",
    "text": "1.1 Who is this book for?\nThis book is primarily being written to support Bioscience students at the University of York. The ultimate aim is to support the full spectrum of computational skills that a bioscience undergraduate or postgraduate at York - and elsewhere - might need. But it is a work in progress. The content included so far is described in the Overview of contents section below.\nIt is being written in the open so that it can be used by anyone who finds it useful. It is also being written in the open so that anyone can contribute to it.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>About this book</span>"
    ]
  },
  {
    "objectID": "intro.html#approach-of-this-book",
    "href": "intro.html#approach-of-this-book",
    "title": "\n1¬† About this book\n",
    "section": "\n1.2 Approach of this book",
    "text": "1.2 Approach of this book\n\nexplanations followed by worked examples",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>About this book</span>"
    ]
  },
  {
    "objectID": "intro.html#overview-of-contents",
    "href": "intro.html#overview-of-contents",
    "title": "\n1¬† About this book\n",
    "section": "\n1.3 Overview of contents",
    "text": "1.3 Overview of contents\nIt is in sections\nPart 1: What they forgot to teach you about computers\nThis chapter tries to teach the computer skills that you might have missed if you have used mainly the mobile devices. I focus on the knowledge gaps that often appear when people are learning computational data analysis. Primarily these are to do with finding and organising their files and folders in the file systems.\nPart 2 Getting started with data\nThe first steps into analysing data with R. The first chapter in this part covers important concepts about data: whether they are discrete and continuous and how we summarise them using descriptive statistics. The second chapter introduces you to R and RStudio for the first time. We start by exploring the layout and appearance then move on to coding. The third chapter describes some useful workflow patterns and tools for organising your work in RStudio. Using these will make learning R easier. Finally, we will go through a complete workflow from importing data from a file to saving a figure for reporting.\nPart 3 Statistical Analysis\nThis section is a first course in Statistical inference which is the process of inferring the characteristics of populations from samples using data analysis. In this first course we take what is called a frequentist - or classical - approach to statistical inference. This is the approach that is most commonly taught in introductory statistics courses. We will learn about the logic of hypothesis testing and confidence intervals. You will also get an introduction to statistical models, what is a statistical model and in particular a linear model.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>About this book</span>"
    ]
  },
  {
    "objectID": "intro.html#conventions-used-in-the-book",
    "href": "intro.html#conventions-used-in-the-book",
    "title": "\n1¬† About this book\n",
    "section": "\n1.4 Conventions used in the book",
    "text": "1.4 Conventions used in the book\nI use some conventions most of which I hope are intuitive. I have tried to articulate them here. If you recognise conventions I have used that are not listed here please let me know.\nCode and any output appears in blocks formatted like this:\n\n# import the chaff data\nchaff &lt;- read_table(\"data-raw/chaff.txt\")\nglimpse(chaff)\n## Rows: 40\n## Columns: 2\n## $ subspecies &lt;chr&gt; \"coelebs\", \"coelebs\", \"coelebs\", \"coelebs\", \"coelebs\", \"coe‚Ä¶\n## $ mass       &lt;dbl&gt; 18.3, 22.1, 22.4, 18.5, 22.2, 19.3, 17.8, 20.2, 22.1, 16.6,‚Ä¶\n\nLines of output start with a ## to distinguish from code comments which begin with a single #. You will learn more about comments in the Using Scripts section in First Steps in RStudio\nWithin the text: - packages are indicated in bold code font like this: ggplot2 - functions are indicated in code font with brackets after their name like this: ggplot() - R objects are indicated in code font like this: stag\nThe content of a code block can be copied using the icon in its top right corner.\nI use packages from the tidyverse (Wickham et al. 2019) including ggplot2 (Wickham 2016), dplyr (Wickham et al. 2023), tidyr (Wickham, Vaughan, and Girlich 2023) and readr (Wickham, Hester, and Bryan 2023) throughout the book. All the code assumes you have loaded the core tidyverse packages with:\n\nlibrary(tidyverse)\n\nIf you run examples and get an error like this:\n\n# Error in read_table(\"data-raw/stag.txt\") : \n#  could not find function \"read_table\"\n\nIt is likely you need to load the tidyverse as shown above.\nAll other packages will be loaded explicitly with library() statements where needed.\nWhen you see ‚Äúüé¨ Your turn!‚Äù indicates that you might want to code along with examples or that there is an opportunity to check your understanding by answering a question. Questions are answered in words or with a piece of code. The answers are given in collapsed sections so you can try to answer them before checking the answer. For example, a question answered in words looks like this:\nüé¨ Your turn! Use the file system above to answer these questions.\n\nWhat is the absolute path for the documentdoc4.txt on a Mac computer?\n\n\n\n\n\n\n\nüìñ\n\n\n\n\n\n\n/home/user1/docs/data/doc4.txt\n\n\n\n\nAnd a question answered with a piece of code looks like this:\nüé¨ Your turn! Assign the value of 4 to a variable called y:\n\nCodey &lt;- 4",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>About this book</span>"
    ]
  },
  {
    "objectID": "intro.html#annotating-this-book",
    "href": "intro.html#annotating-this-book",
    "title": "\n1¬† About this book\n",
    "section": "\n1.5 Annotating this book",
    "text": "1.5 Annotating this book\nThis page has annotating with Hypothesis enabled. Hypothesis allows you to annotate this book with your own private notes or make notes shared with friends. You need to create a free personal account. You can make annotations that are public, private only to you or shared with a private group. Please follow the code of conduct in your annotations.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>About this book</span>"
    ]
  },
  {
    "objectID": "intro.html#code-of-conduct",
    "href": "intro.html#code-of-conduct",
    "title": "\n1¬† About this book\n",
    "section": "\n1.6 Code of Conduct",
    "text": "1.6 Code of Conduct\nWe are dedicated to providing a welcoming and supportive learning environment for all readers, regardless of background or identity. As such, we do not tolerate comments that are disrespectful to fellow learners or that excludes, intimidates, or causes discomfort to others. The following bullet points set out explicitly what we hope you will consider to be appropriate community guidelines:\n\nBe respectful of different viewpoints and experiences. Do not use in homophobic, racist, transphobic, ageist, ableist, sexist, or otherwise exclusionary language.\nUse welcoming and inclusive language. Do not address others in an angry, intimidating, or demeaning manner. Be considerate of the ways the words you choose may impact others. Be patient and respectful of the fact that English is a second (or third or fourth!) language for many.\nRespect the privacy and safety of others. Do not share their information without their express permission.\nAs an overriding general rule, please be intentional in your actions and humble in your mistakes.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>About this book</span>"
    ]
  },
  {
    "objectID": "intro.html#contributing",
    "href": "intro.html#contributing",
    "title": "\n1¬† About this book\n",
    "section": "\n1.7 Contributing",
    "text": "1.7 Contributing\nThis book is being written in the open so that anyone can contribute to it. If you find a mistake, or have a suggestion for improvement you can create an issue.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>About this book</span>"
    ]
  },
  {
    "objectID": "intro.html#license",
    "href": "intro.html#license",
    "title": "\n1¬† About this book\n",
    "section": "\n1.8 License",
    "text": "1.8 License\n\nThis work is licensed under CC BY-NC 4.0 This license requires that reusers give credit to the creator. It allows reusers to distribute, remix, adapt, and build upon the material in any medium or format, for noncommercial purposes only.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>About this book</span>"
    ]
  },
  {
    "objectID": "intro.html#please-cite-as",
    "href": "intro.html#please-cite-as",
    "title": "\n1¬† About this book\n",
    "section": "\n1.9 Please cite as",
    "text": "1.9 Please cite as\nPlease cite this book as:\nRand, E. (2023). Computational Analysis for Bioscientists (Version 0.1) https://3mmarand.github.io/comp4biosci/",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>About this book</span>"
    ]
  },
  {
    "objectID": "intro.html#credits",
    "href": "intro.html#credits",
    "title": "\n1¬† About this book\n",
    "section": "\n1.10 Credits",
    "text": "1.10 Credits\nThis book is written with R (R Core Team 2024), Quarto (Allaire et al. 2022), knitr (Xie 2022), kableExtra (Zhu 2021). My R session information is shown below:\n\nsessionInfo()\n## R version 4.4.2 (2024-10-31 ucrt)\n## Platform: x86_64-w64-mingw32/x64\n## Running under: Windows 10 x64 (build 19045)\n## \n## Matrix products: default\n## \n## \n## locale:\n## [1] LC_COLLATE=English_United Kingdom.utf8 \n## [2] LC_CTYPE=English_United Kingdom.utf8   \n## [3] LC_MONETARY=English_United Kingdom.utf8\n## [4] LC_NUMERIC=C                           \n## [5] LC_TIME=English_United Kingdom.utf8    \n## \n## time zone: Europe/London\n## tzcode source: internal\n## \n## attached base packages:\n## [1] stats     graphics  grDevices datasets  utils     methods   base     \n## \n## other attached packages:\n##  [1] patchwork_1.3.0 lubridate_1.9.4 forcats_1.0.0   stringr_1.5.1  \n##  [5] dplyr_1.1.4     purrr_1.0.2     readr_2.1.5     tidyr_1.3.1    \n##  [9] tibble_3.2.1    ggplot2_3.5.1   tidyverse_2.0.0\n## \n## loaded via a namespace (and not attached):\n##  [1] gtable_0.3.6      jsonlite_1.8.9    crayon_1.5.3      compiler_4.4.2   \n##  [5] renv_0.17.0       tidyselect_1.2.1  scales_1.3.0      yaml_2.3.10      \n##  [9] fastmap_1.2.0     R6_2.5.1          generics_0.1.3    knitr_1.49       \n## [13] munsell_0.5.1     pillar_1.10.1     tzdb_0.4.0        rlang_1.1.5      \n## [17] stringi_1.8.4     xfun_0.50         timechange_0.3.0  cli_3.6.3        \n## [21] withr_3.0.2       magrittr_2.0.3    digest_0.6.37     grid_4.4.2       \n## [25] rstudioapi_0.17.1 hms_1.1.3         lifecycle_1.0.4   vctrs_0.6.5      \n## [29] evaluate_1.0.3    glue_1.8.0        farver_2.1.2      colorspace_2.1-1 \n## [33] rmarkdown_2.29    tools_4.4.2       pkgconfig_2.0.3   htmltools_0.5.8.1\n\n\n\n\n\nAllaire, J. J., Charles Teague, Carlos Scheidegger, Yihui Xie, and Christophe Dervieux. 2022. Quarto. https://doi.org/10.5281/zenodo.5960048.\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D‚ÄôAgostino McGowan, Romain Fran√ßois, Garrett Grolemund, et al. 2019. ‚ÄúWelcome to the Tidyverse‚Äù 4: 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Romain Fran√ßois, Lionel Henry, Kirill M√ºller, and Davis Vaughan. 2023. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2023. Readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr.\n\n\nWickham, Hadley, Davis Vaughan, and Maximilian Girlich. 2023. Tidyr: Tidy Messy Data. https://CRAN.R-project.org/package=tidyr.\n\n\nXie, Yihui. 2022. ‚ÄúKnitr: A General-Purpose Package for Dynamic Report Generation in r.‚Äù https://yihui.org/knitr/.\n\n\nZhu, Hao. 2021. ‚ÄúkableExtra: Construct Complex Table with ‚ÄôKable‚Äô and Pipe Syntax.‚Äù https://CRAN.R-project.org/package=kableExtra.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>About this book</span>"
    ]
  },
  {
    "objectID": "what_they_forgot.html",
    "href": "what_they_forgot.html",
    "title": "What they forgot to teach you about computers",
    "section": "",
    "text": "Why ‚ÄúWhat they forgot‚Äù?\nMost students grew up with the presence of digital technology and are thought to be comfortable with, and fluent in, technology. But for many, this fluency is with using tablets and smart phones and these are different to computers. Several studies have shown that the increased reliance on of tablets and smartphones makes people highly proficient at online communication but hampers the development of other computing skills Drossel and Eickelmann (2017); Australian Curriculum and Authority (2015).\nThis chapter tries to teach the computer skills that you might have missed if you have used mainly the mobile devices. I focus on the knowledge gaps that often appear when people are learning computational data analysis. Primarily these are to do with finding and organising their files and folders in the file systems.\nIf you often use a computer for tasks like organising files and writing documents and spreadsheets much of this chapter might be familiar. However, if you have mainly used your mobile phone and tablets to access the internet then reading all of it will be helpful. The first chapter briefly explains why a computer rather than a tablet is most useful for a science degree and what an operating system is. The second chapter some essential concept for scientific computers: file system organisation, file types, working directories and paths. Most people joining a bioscience degree programme are not familiar with the concept of working directories but will find learning data analysis much easier once they pick up this knowledge.",
    "crumbs": [
      "What they forgot to teach you about computers"
    ]
  },
  {
    "objectID": "what_they_forgot.html#why-what-they-forgot",
    "href": "what_they_forgot.html#why-what-they-forgot",
    "title": "What they forgot to teach you about computers",
    "section": "",
    "text": "Australian Curriculum, Assessment, and Reporting Authority. 2015. ‚ÄúNational Assessment Program ICT Literacy.‚Äù https://nap.edu.au/_resources/D15_8761__NAP-ICT_2014_Public_Report_Final.pdf.\n\n\nDrossel, Kerstin, and Birgit Eickelmann. 2017. ‚ÄúThe Use of Tablets in Secondary Schools and Its Relationship with Computer Literacy.‚Äù In, 114‚Äì24. Springer International Publishing. https://doi.org/10.1007/978-3-319-74310-3_14.",
    "crumbs": [
      "What they forgot to teach you about computers"
    ]
  },
  {
    "objectID": "machine_and_os.html",
    "href": "machine_and_os.html",
    "title": "\n2¬† Machines and operating systems\n",
    "section": "",
    "text": "2.1 Computer or tablet?\nComputers (laptop or desktop) and tablets can both be useful for doing a science degree but if you have to choose just one, I would recommend a computer. This is because\nTablets are more portable and have touch-screens and stylus support and this makes them easier to use for note taking in lectures and annotating documents. Some laptops also have touch-screens but they can be expensive. Overtime computers will adopt more of the advantages of tablets and tablets will adopt more of the advantages of computers.",
    "crumbs": [
      "What they forgot to teach you about computers",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Machines and operating systems</span>"
    ]
  },
  {
    "objectID": "machine_and_os.html#computer-or-tablet",
    "href": "machine_and_os.html#computer-or-tablet",
    "title": "\n2¬† Machines and operating systems\n",
    "section": "",
    "text": "Computers typically offer more processing power, storage capacity, and flexibility for running complex software all of which are important in science.\nScientific and academic software often cannot be installed on tablets - at least not easily.\nYour institution most likely has a networked computer system with computers that you use in taught workshop and which are available to you throughout your degree. Many people find it easier to use a similar computer at home so they can follow instructions from the workshop exactly to continue the work at home\nWe often need to multitask and use several programs at once. The greater power and screen size of a computer usually makes this easier.",
    "crumbs": [
      "What they forgot to teach you about computers",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Machines and operating systems</span>"
    ]
  },
  {
    "objectID": "machine_and_os.html#what-is-an-operating-system",
    "href": "machine_and_os.html#what-is-an-operating-system",
    "title": "\n2¬† Machines and operating systems\n",
    "section": "\n2.2 What is an operating system?",
    "text": "2.2 What is an operating system?\nEvery computer or mobile device has an operating system (OS) which controls how all the other software on a computer interacts with the computer hardware. You may already have a preference for a particular OS. They start the computer up, manage the memory and all the processes, and control all the devices connected to the computer. As a consequence, all the other software on a computer has to be able to work with the operating system installed.\nMost people do not install the operating system but buy their machine with it pre-installed.",
    "crumbs": [
      "What they forgot to teach you about computers",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Machines and operating systems</span>"
    ]
  },
  {
    "objectID": "machine_and_os.html#types-of-operating-system",
    "href": "machine_and_os.html#types-of-operating-system",
    "title": "\n2¬† Machines and operating systems\n",
    "section": "\n2.3 Types of operating system",
    "text": "2.3 Types of operating system\n\n2.3.1 Microsoft Windows and Apple macOS\nWindows and Mac are the two most widely used OS on personal computers. Apple Mac computers use only Apple macOS but Windows OS are used by many machine manufacturers. Both OS are proprietor meaning that they are designed and sold by the companies and are not meant to be altered by users. It also means the other applications they make are designed to work best with their OS.\nYou might already have had - or seen - a Mac versus Windows debate but in reality it is down to preference for most users. If you already have experience with one, you will probably prefer to keep using it but if you are buying for the first time, I recommend using the same as your institution uses. This makes it more likely that instructions for taught material will just work on your machine and more likely that people around you will be able to help.\nOne of the differences uses most notice between Windows and Mac machine is the keyboard keys names. Table Table¬†2.1 gives some of the most notable.\n\n\nTable¬†2.1: Keyboard equivalents for Windows and Mac\n\n\n\nWindows\nMac\n\n\n\nEnter\nReturn\n\n\nControl\nCommand\n\n\nAlt\nOption\n\n\n\n\n\n\nWhen using RStudio, the section on Keyboard Shortcuts and tips will help.\n\n2.3.2 Linux\nLinux is a family of open source OS. Open source means that anyone can modify and share the software.\n(TODO - expand)",
    "crumbs": [
      "What they forgot to teach you about computers",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Machines and operating systems</span>"
    ]
  },
  {
    "objectID": "file_systems.html",
    "href": "file_systems.html",
    "title": "\n3¬† Understanding file systems\n",
    "section": "",
    "text": "3.1 Files\nA file system is made up of files and folders organised in a hierarchical way. Understanding file types and being able to navigate your computer‚Äôs file system and purposefully organise and manipulate files are essential computational skills. We will first talk about different file types before covering their organisation in a file system. This will lead us on to the concepts of working directories and paths. It is hard to overstate how useful an understanding of working directories and paths are for comfortable computing. Without this understanding, you can write basically correct code, such as for importing data, which will not work because you don‚Äôt know how to tell the computer where the file is.\nA file is a unit of storage on a computer with a name that uniquely identifies it. Files can be of different types depending on the sort of information held in them. The file name very often consists of two parts, separated by a dot:\nSome examples are report.docx, analysis.R, culture.csv and readme.txt. There is a relationship between the file extension and the file type",
    "crumbs": [
      "What they forgot to teach you about computers",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Understanding file systems</span>"
    ]
  },
  {
    "objectID": "file_systems.html#files",
    "href": "file_systems.html#files",
    "title": "\n3¬† Understanding file systems\n",
    "section": "",
    "text": "the name - the base name of the file\nan extension that should indicate the format or content of the file.",
    "crumbs": [
      "What they forgot to teach you about computers",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Understanding file systems</span>"
    ]
  },
  {
    "objectID": "file_systems.html#plain-text-files",
    "href": "file_systems.html#plain-text-files",
    "title": "\n3¬† Understanding file systems\n",
    "section": "\n3.2 Plain text files",
    "text": "3.2 Plain text files\nOne of the simplest types of file is a ‚Äútext file‚Äù, also known as ASCII files, which contains only text characters and no formatting, images or colours. Text files have many different extensions to indicate what the text content represents. A few of these are given in Table¬†3.1.\n\n\n\nTable¬†3.1: Some common text file extensions and the content the extension usually indicates.\n\n\n\n\nExtension\nContent usually inside\n\n\n\n.txt\nCould be any kind of text, including data\n\n\n.tab\nValues, often data, separated by tabs\n\n\n.csv\nValues, often data, separated by commas\n\n\n.R\nR commands and comments\n\n\n.py\nPython commands and comments\n\n\n.fasta\nNucleotide or amino acid sequences\n\n\n\n\n\n\n\n\nPlain text files are extremely portable which means they can be opened on any computer by a very large number of programs. Simple text editors which exist on any system like Windows Notepad or Mac‚Äôs TextEdit will open text files where as they will not open program-specific files like, for example, .xlsx (Excel) or .docx (Word) files (Figure¬†3.1). It is usually easy for application designers to make an application manage plain text files. This means\n\n\n\n\n\nFigure¬†3.1: Notepad will open an Excel file but the contents are a mess of strange characters\n\n\nData is commonly held in text files because of their portability.",
    "crumbs": [
      "What they forgot to teach you about computers",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Understanding file systems</span>"
    ]
  },
  {
    "objectID": "file_systems.html#plain-text-files-with-markup",
    "href": "file_systems.html#plain-text-files-with-markup",
    "title": "\n3¬† Understanding file systems\n",
    "section": "\n3.3 Plain text files with markup",
    "text": "3.3 Plain text files with markup\nText files can also include ‚Äúmarkup‚Äù which are text characters used to annotate text to control how it is displayed or processed. Such files retain their portability and are human readable. You will have used them often! (Table¬†3.2)\n\n\n\nTable¬†3.2: Some common markup file extensions and the content the extension usually indicates.\n\n\n\n\nExtension\nUsed for\n\n\n\n.html\nHypertext Markup Language - for documents designed to be displayed in a web browser.\n\n\n.md\nmarkdown - a lightweight markup language designed to be human readable\n\n\n.Rmd\nR markdown - like markdown but can include R code\n\n\n.JSON\nJavaScript Object Notation - commonly used for transmitting data in web applications using attribute-value pairs.\n\n\n.tex\nTeX - a typesetting langauge especially where the writer needs precise spacing and/or unusual fonts and characters such as in maths, linguistics and music.\n\n\n.qmd\nQuarto markdown - next-generation version of R Markdown designed to work with *any* programming langauge.",
    "crumbs": [
      "What they forgot to teach you about computers",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Understanding file systems</span>"
    ]
  },
  {
    "objectID": "file_systems.html#the-relationship-between-file-extensions-and-programs",
    "href": "file_systems.html#the-relationship-between-file-extensions-and-programs",
    "title": "\n3¬† Understanding file systems\n",
    "section": "\n3.4 The relationship between file extensions and programs",
    "text": "3.4 The relationship between file extensions and programs\nWhilst a file extension is intended to indicate the content and format, it is important to remember a few things. The extension is just part of the name and it is certainly possible to called a file myfile.csv without its contents being formatted as comma-separated values. Programs vary in their behaviour to file extensions. Most text editors will attempt to open any file regardless of the extension and make it possible to save a file with any extension. In program like MS Word you cannot save a Word format file with any extension other than .docx, you can only save it in another format to change the extension. Some programs will not open fies with a the wrong extension even if the contents are in the correct format.",
    "crumbs": [
      "What they forgot to teach you about computers",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Understanding file systems</span>"
    ]
  },
  {
    "objectID": "file_systems.html#file-systems",
    "href": "file_systems.html#file-systems",
    "title": "\n3¬† Understanding file systems\n",
    "section": "\n3.5 File systems",
    "text": "3.5 File systems\nIn a file system, the files are organised into directories. Directory is the old word for what many now call a folder but commands that act on folders in most programming languages and environments reflect this history\nFor example, all of these mean ‚Äútell me my working directory‚Äù:\n\n\ngetwd() get working directory in R\n\npwd print working directory on Linux\n\nos.getcwd() get current working directory in Python\n\nConsequently it is common to use the the word directory in scientific computing.\nFolders can contain sub-folders, which can contain their own sub-folders, and so on almost without limit.\nIt is easiest to picture a file system, or part of it, as a tree that starts at a directory and branches out from there. This is called a hierarchical structure. Figure¬†3.2 shows an example of a hierarchical file structure that starts at a directory called home:\n\n\n\n\n\nFigure¬†3.2: A file hierarchy containing 4 levels of folders and files. Figure adapted from (Rand, Emma et al. 2022).",
    "crumbs": [
      "What they forgot to teach you about computers",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Understanding file systems</span>"
    ]
  },
  {
    "objectID": "file_systems.html#using-a-file-manager",
    "href": "file_systems.html#using-a-file-manager",
    "title": "\n3¬† Understanding file systems\n",
    "section": "\n3.6 Using a file manager",
    "text": "3.6 Using a file manager\nFile managers are the basic way that you interact with the file system on your computer. They allows you to move, copy, and delete files. You can also launch applications from them and and connect to other networks. The windows file explorer is called File Explorer and on Mac it is called Finder (Figure¬†3.3).\n\n\n\n\n\n\n\n\n\n(a) File Explorer Icon (Windows)\n\n\n\n\n\n\n\n\n\n(b) Finder Icon (Mac)\n\n\n\n\n\n\nFigure¬†3.3: File manager programs in Windows and Mac\n\n\nWindows Explorer and Mac Finder do not show the file extensions on file names by default but I find it helpful to be able to see them. You can show the file extensions like this:\n\nunder View, in the Show/hide group, select the ‚ÄúFile name extensions‚Äù check box Figure¬†3.4 (a)\nchoose Finder | Preferences | Advanced then check ‚ÄúShow all filename extensions‚Äù box Figure¬†3.4 (a)\n\n\n\n\n\n\n\n\n\n\n(a) Windows\n\n\n\n\n\n\n\n\n\n\n\n(b) Mac\n\n\n\n\n\n\nFigure¬†3.4: Showing file extensions in Windows and Mac",
    "crumbs": [
      "What they forgot to teach you about computers",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Understanding file systems</span>"
    ]
  },
  {
    "objectID": "file_systems.html#root-and-home-directories",
    "href": "file_systems.html#root-and-home-directories",
    "title": "\n3¬† Understanding file systems\n",
    "section": "\n3.7 Root and home directories",
    "text": "3.7 Root and home directories\nThe top-level of directory on a computer system is known as the ‚Äúroot directory‚Äù. The root is represented as a / in Mac and Linux operating systems. In Windows the root directory is also known as a drive. In most cases, this will be the C:\\ drive.\nEven though the root directory is at the base of the file tree (or the top, depending on how you view it), it is not necessarily where our journey through the file system starts when we launch a new session on our computer. Instead our journey begins in the so called ‚Äúhome directory‚Äù. You home directory is not usually named home but with your username for that computer. Your personal files and directories can be found inside this folder. This is where your computer assumes you want to start when you open your file manager. On Windows and Mac your home directory is a directory inside the directory called Users immediately under the root and named with your username (Figure¬†3.5).\n\n\n\n\n\nFigure¬†3.5: The hierarchy from the root. The top level is C:\\ in Windows and / in Mac. Below that is the Users directory which has a folder for each user. Your home directory is named with your username inside the Users folder. Figure adapted from (Rand, Emma et al. 2022).\n\n\nThere will be other folders immediately below the root directory (on the same level of the hierarchy as Users). These contain system-level files and folders that you do not usually needed to open, edit or move. For example, Program Files is where programs are installed.",
    "crumbs": [
      "What they forgot to teach you about computers",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Understanding file systems</span>"
    ]
  },
  {
    "objectID": "file_systems.html#working-directories",
    "href": "file_systems.html#working-directories",
    "title": "\n3¬† Understanding file systems\n",
    "section": "\n3.8 Working directories",
    "text": "3.8 Working directories\nThe working directory of a program is the default location a program is using. It is where the program will read and write files by default. You have only one working directory at a time. The terms ‚Äòworking directory‚Äô, ‚Äòcurrent working directory‚Äô and ‚Äòcurrent directory‚Äô all mean the same thing.",
    "crumbs": [
      "What they forgot to teach you about computers",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Understanding file systems</span>"
    ]
  },
  {
    "objectID": "file_systems.html#file-paths",
    "href": "file_systems.html#file-paths",
    "title": "\n3¬† Understanding file systems\n",
    "section": "\n3.9 File Paths",
    "text": "3.9 File Paths\nA path gives the address - or location - of a filesystem object, such as a file or directory. Paths appear in the address bar of your browser or file manager. We need to know a file path whenever we want to read, write or refer to a file using code rather than interactively pointing and clicking to navigate. In a file path, each directory is represented as a separate component separated by a backslash\\ ¬†or a forward slash /. Most systems use forward slashes but Windows uses backslashes1 to separate path components and that is how the path will appear in the address bar of Windows Explorer. However, in R you can use paths with forward slashes even on windows.\nA path can be absolute or relative depending on the starting point.\n\n3.9.1 Absolute paths\nAn Absolute path contains the complete list of directories needed to locate a file on your computer from the root. For example, the absolute path for the file called doc3.txt in the file system above would be /Users/user1/docs/data/doc3.txt on Mac and C:\\Users\\user1\\docs\\data\\doc3.txt on Windows. In R, even on Windows, it can be given as C:/Users/user1/docs/data/doc3.txt",
    "crumbs": [
      "What they forgot to teach you about computers",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Understanding file systems</span>"
    ]
  },
  {
    "objectID": "file_systems.html#relative-paths",
    "href": "file_systems.html#relative-paths",
    "title": "\n3¬† Understanding file systems\n",
    "section": "\n3.10 Relative paths",
    "text": "3.10 Relative paths\nA relative path gives the location of a filesystem object relative to the working directory. Whenever the file you want to reference is in the working directory you can use just its name but if it is in a different folder you need to give the relative path. Some examples:\n\nif your working directory was docs, the relative path for doc3.txt would be data/doc3.txt.\nif your working directory was docs the relative path for abe.exe files would be ../programs/abe.exe.\n\n../ allows you to look in the directory above the working directory and ../.. allows you to look in the directory two levels above the working directory and so on.\nüé¨ Your turn! Use the file system above to answer these questions.\n\nWhat is the absolute path for the documentdoc4.txt on a Mac computer?\nWhat is the absolute path for the document doc4.txt on a Windows computer?\nAssuming your working directory is docs, what is the relative path for the document doc2.txt?\nAssuming your working directory is data, what is the relative path for the document doc2.txt?\n\n\n\n\n\n\n\nüìñ\n\n\n\n\n\n\n/Users/user1/docs/data/doc4.txt\nC:/Users/user1/docs/data/doc4.txt\ndoc2.txt\n../doc2.txt?\n\n\n\n\nMost of the time you should use relative paths because that makes your work portable. You only need to use absolute paths when you are referring to filesystem outside the one you are using.",
    "crumbs": [
      "What they forgot to teach you about computers",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Understanding file systems</span>"
    ]
  },
  {
    "objectID": "file_systems.html#save-files-from-the-internet",
    "href": "file_systems.html#save-files-from-the-internet",
    "title": "\n3¬† Understanding file systems\n",
    "section": "\n3.11 Save files from the internet",
    "text": "3.11 Save files from the internet\nFiles downloaded from the internet go to a folder called Downloads by default on many browsers. This is annoying when you often want to place a file in a particular folder. I recommend you change this behaviour.\n\nChrome Go to chrome://settings/downloads and turn on ‚ÄúAsk where to save each file before downloading‚Äù\nSafari Go into Preferences and under General you can change ‚ÄúDownloads‚Äù to ‚ÄúAsk for each download‚Äù",
    "crumbs": [
      "What they forgot to teach you about computers",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Understanding file systems</span>"
    ]
  },
  {
    "objectID": "file_systems.html#summary",
    "href": "file_systems.html#summary",
    "title": "\n3¬† Understanding file systems\n",
    "section": "\n3.12 Summary",
    "text": "3.12 Summary\n\nA file system consists of files and folders organized hierarchically. It is crucial to comprehend different file types, navigate the computer‚Äôs file system, and effectively organize and manipulate files.\nPlain text files, also known as ASCII files, contain only text characters without formatting, images, or colors. They have various extensions, such as .txt, .tab, .csv, .R, .py, .fasta, and others.\nWhile file extensions are intended to indicate content and format, it‚Äôs important to remember that programs may behave differently with file extensions. Most text editors will attempt to open any file regardless of the extension, but some programs may only open files with the correct extension.\nA directory is a folder\nIn a file system, files are organized into directories. Directories can contain sub-directories, creating a hierarchical structure. The top-level directory is known as the root directory.\nFile managers, such as File Explorer in Windows and Finder in Mac, are used to interact with the file system.\nThe working directory of a program is the default location where files are read from or written to. It is important to understand working directories when referencing files in code.\nFile paths provide the address or location of a file or directory in the file system. Paths can be absolute or relative. Absolute paths contain the complete list of directories from the root, while relative paths are based on the working directory.\nUsing relative paths is recommended for portability, as it allows files to be referenced relative to the working directory.\n\n\n\n\n\nRand, Emma, Chong, James, Buenabad-Chavez, Jorge, Cansdale, Annabel, Forrester, Sarah, and Greeves, Evelyn. 2022. ‚ÄúCloud-SPAN/00genomics: Cloud-SPAN Genomics Course Overview,‚Äù May. https://doi.org/10.5281/ZENODO.6564314.",
    "crumbs": [
      "What they forgot to teach you about computers",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Understanding file systems</span>"
    ]
  },
  {
    "objectID": "file_systems.html#footnotes",
    "href": "file_systems.html#footnotes",
    "title": "\n3¬† Understanding file systems\n",
    "section": "",
    "text": "Windows uses backslashes because it did not have directories in 1981 when it‚Äôs predecessor, MS DOS, was released. At the time it used the / character for ‚Äòswitches‚Äô (instead of the existing convention - ) so when it did start using directories it couldn‚Äôt use /‚Ü©Ô∏é",
    "crumbs": [
      "What they forgot to teach you about computers",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Understanding file systems</span>"
    ]
  },
  {
    "objectID": "organising_work.html",
    "href": "organising_work.html",
    "title": "\n4¬† Organising your work\n",
    "section": "",
    "text": "4.1 Use folders",
    "crumbs": [
      "What they forgot to teach you about computers",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Organising your work</span>"
    ]
  },
  {
    "objectID": "organising_work.html#be-consistency",
    "href": "organising_work.html#be-consistency",
    "title": "\n4¬† Organising your work\n",
    "section": "\n4.2 Be consistency",
    "text": "4.2 Be consistency",
    "crumbs": [
      "What they forgot to teach you about computers",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Organising your work</span>"
    ]
  },
  {
    "objectID": "organising_work.html#naming-things",
    "href": "organising_work.html#naming-things",
    "title": "\n4¬† Organising your work\n",
    "section": "\n4.3 Naming things",
    "text": "4.3 Naming things",
    "crumbs": [
      "What they forgot to teach you about computers",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Organising your work</span>"
    ]
  },
  {
    "objectID": "getting_started_with_data.html",
    "href": "getting_started_with_data.html",
    "title": "Getting started with data",
    "section": "",
    "text": "First draft\n\n\n\nYou are reading a work in progress. This page is a first draft but should be readable.\n\n\nIn the first part of this book you learnt some foundational ideas about working with computers. Now you are ready to take your first steps into analysing data with R. The first chapter in this part covers important concepts about data: whether they are discrete and continuous and how we summarise them. The second chapter introduces you to R and RStudio for the first time. We start by exploring the layout and appearance then move on to coding. The third chapter describes some useful workflow patterns and tools for organising your work in RStudio. Using these will make learning R easier. Finally, we will go through a complete workflow from importing data from a file to saving a figure for reporting.",
    "crumbs": [
      "Getting started with data"
    ]
  },
  {
    "objectID": "ideas_about_data.html",
    "href": "ideas_about_data.html",
    "title": "\n5¬† Ideas about data\n",
    "section": "",
    "text": "5.1 Roles of variables in analysis\nThis chapter covers some important concepts data. Data is made up of:\nData is most commonly (and helpfully) organised with variables in columns and each observation on a row.\nWe can classify a variable in two main ways: by what role the variable takes in analysis and by what kinds of value it can take and how frequently its possible values occur. The type of value and the likelihood of values appearing is known as a variable‚Äôs distribution. Both of these determine how we summarise, plot and analyse data.\nWhen we do research, we typically have variables that we choose or set and variables that we measure. The variables we choose or set are called independent or explanatory variables. The variables we measure are called dependent or response variables. For example, we might measure the concentration of enzymes and hormones in blood samples of individuals with different genotypes. The genotype acts as the explanatory variable and the blood measurements are response. We would be interested in whether the blood measures differ between genotypes.",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Ideas about data</span>"
    ]
  },
  {
    "objectID": "ideas_about_data.html#distributions",
    "href": "ideas_about_data.html#distributions",
    "title": "\n5¬† Ideas about data\n",
    "section": "\n5.2 Distributions",
    "text": "5.2 Distributions\nAny variable has a distribution which describes the values the variable can take and the chance of them occurring. The distribution is a function (relationship) which captures how the values are mapped to the probability of them occurring. A distribution has a general type, given by the function and is further tuned by the parameters in the function. For example variables like height, length, concentration, mass and intensity follow a normal distribution also know as the bell-shaped curve so that they look similar. However, they have different means and standard deviations. The mean and the standard deviation are the parameters of the normal distribution.\nAn important distinction is between discrete and continuous types of data. Continuous variables are measurements that can take any value in their range (Figure¬†5.1). Discrete variables can take only specific values (Figure¬†5.2).\n\n\n\n\n\n\n\nFigure¬†5.1: A continuous variable has a smooth distribution and the probability of getting a particular value or less is given by the area under the curve. The example is a normal distribution.\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†5.2: A discrete variable has a stepped distribution and the probability of getting exactly a particular is given by the area of the bar at that value. The example is a Poisson distribtion.",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Ideas about data</span>"
    ]
  },
  {
    "objectID": "ideas_about_data.html#summarising-data",
    "href": "ideas_about_data.html#summarising-data",
    "title": "\n5¬† Ideas about data\n",
    "section": "\n5.3 Summarising data",
    "text": "5.3 Summarising data\nWe summarise data by giving a measure of central tendency and a measure of dispersion. A measure of central tendency is a single value that represents the middle or centre of a variable‚Äôs distribution. The three most common measures are:\n\nthe mean, or more accurately, the arithmetic mean, \\(\\bar{x} = \\frac{\\sum{x}}{n}\\)\nthe median: the middle value for a variable with values arranged in order of magnitude\nthe mode: the most frequent value in the variable.\n\nMeasures of dispersion describe the spread of values around the centre and indicate how much variability there is in the variable. The most common measures of dispersion are:\n\nthe range: the difference between the maximum value and the minimum value in a variable\nthe interquartile range: two values, the first quartile and the thrid quartile. The first quartile is half way between the median value and the lowest value when the values are arranged in order and the third quartile is halfway between the median value and the highest value\nthe variance: the average of the squared differences between each value and the variable‚Äôs mean, \\(s^2 = \\frac{(\\sum{x - \\bar{x})^2}}{n - 1}\\)\nthe standard deviation: the square root of the variance.",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Ideas about data</span>"
    ]
  },
  {
    "objectID": "ideas_about_data.html#discrete-data",
    "href": "ideas_about_data.html#discrete-data",
    "title": "\n5¬† Ideas about data\n",
    "section": "\n5.4 Discrete data",
    "text": "5.4 Discrete data\nDiscrete variables can take only specific values and can be categories, like genotype, or counts, like the number of petals.\n\n5.4.1 Categories: Nominal and Ordinal\nCategorical data can be nominal and ordinal depending on whether they are ordered. Nominal variable have no particular order, for example, the eye colour of Drosophila or the species of bird. When summarising data on bird species, it wouldn‚Äôt matter in what order the information was given or plotted. Ordinal variables have an order. The Likert scale Likert (1932) used in questionnaires is one example. The possible responses are Strongly agree, Agree, Disagree and Strongly disagree; these have an order that you would use when plotting them (Figure¬†5.3).\n\n\n\n\n\n\n\nFigure¬†5.3: The categories of bird species are unordered - or nominal - but those in a likert variable are ordinal.\n\n\n\n\nThe most appropriate way to summarise nominal or ordinal data is to report the mode (most frequent value) or tabulate the number of each value (Table¬†5.1).\n\n\n\nTable¬†5.1: Frequency of reposnes.\n\n\n\n\nResponse\nFrequency\n\n\n\nStrongly agree\n15\n\n\nAgree\n18\n\n\nDisagree\n10\n\n\nStrongly disagree\n2\n\n\n\n\n\n\n\n\n\n5.4.2 Counts\nCounts are one of the most common data types. They are quantitative but discrete because they can take only specific values. Counts tend to follow a distribution called the Poisson distribution meaning that there is an expected number of zeros, ones and twos etc that appear. This is true no matter what you count. The distribution of counts is not symmetrical and has a tail to the right - especially for lower count ranges.\nA mean and standard deviation are not usually the best way to summarise a count variable because their distribution is not symmetrical. Figure¬†5.4 shows the number of groundsel plants in a hundred 1 metre square quadrats1.\n\n\n\n\n\n\n\nFigure¬†5.4: Counts are discrete. They often follow a Poisson distribution.\n\n\n\n\nThe mean of 0.73 groundsel plants per quadrat does not really reflect that the majority of quadrats contained no plants. The average is being dragged up by the few quadrats containing 3 plants.\nFor count variables - and other variables that are not symmetrical - it is often better to give the median and interquartile range.\nThe median is the middle value when the values are arranged in order. The interquartile range (IQR) indicates the spread in the data. The lower quartile is half way between the lowest value and the middle value and the upper quartile is half way between the middle value and the highest value.",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Ideas about data</span>"
    ]
  },
  {
    "objectID": "ideas_about_data.html#continuous-data",
    "href": "ideas_about_data.html#continuous-data",
    "title": "\n5¬† Ideas about data\n",
    "section": "\n5.5 Continuous data",
    "text": "5.5 Continuous data\nContinuous variables are measurements that can take any value in their range so there are an infinite number of possible values. The values have decimal places. Variables like the length and mass of an organism, the volume and optical density of a solution, or the colour intensity of an image are continuous. Many response variables are continuous but continuous variables can also be explanatory. A large number of continuous variables follow the normal distribution.\n\n5.5.1 The normal distribution\nFor example, a variable like human height has values with decimal places which follow a normal distribution also known as the Gaussian distribution or the bell-shaped curve. Values of 1.65 metres occur more often than values of 2 metres and values of 3 metres never occur (Figure¬†5.5).\n\n\n\n\n\n\n\nFigure¬†5.5: Human height follows a normal distribution.\n\n\n\n\nThe mean and standard deviation are the best way to summarise a normally distributed variable.",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Ideas about data</span>"
    ]
  },
  {
    "objectID": "ideas_about_data.html#theory-and-practice",
    "href": "ideas_about_data.html#theory-and-practice",
    "title": "\n5¬† Ideas about data\n",
    "section": "\n5.6 Theory and practice",
    "text": "5.6 Theory and practice\nThe distinction between continuous and discrete values is clear in theory but in practice, the actual values you have may differ from the expected distribution for a particular variable. For example, we would expect the mass of cats to be continuous because any value in the range is possible. However, if our scales only measure to the nearest kilogram, then the values become discrete because the only the values possible are 0, 1, 2, 3, 4, 5, 6 and 7. The gap between values, 1kg, is big compared to the range of values we expect (Figure¬†5.6)\n\n\n\n\n\n\n\nFigure¬†5.6: The mass of cats in kilograms is theoretically continuous (left). If we measure only to the nearest kilogram, mass becomes discrete because 1kg covers a large chunk of the range (right).\n\n\n\n\nIn contrast, the number of hairs on a human head ranges from about 80,000 to 150,000 so the difference between having 100,203 hairs and 100,204 hairs is so tiny the variable is practically continuous (Figure¬†5.7).\n\n\n\n\n\n\n\nFigure¬†5.7: The number of hairs on a human head discrete. But one increment is a tiny proportion of the range of values so the number of hairs is practically continuous.\n\n\n\n\nA rule of thumb is that if the mean count is above about 100, then the distribution of counts closely matches the normal distribution.",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Ideas about data</span>"
    ]
  },
  {
    "objectID": "ideas_about_data.html#summary",
    "href": "ideas_about_data.html#summary",
    "title": "\n5¬† Ideas about data\n",
    "section": "\n5.7 Summary",
    "text": "5.7 Summary\n\nData: Data consists of variables (properties measured or recorded) and observations (individual things with those properties). Data is commonly organised with variables in columns and observations in rows.\nRoles of Variables in Analysis: Variables in research can be independent or explanatory variables (chosen or set by researchers) and dependent or response variables (measured by researchers).\nDistributions: A variable‚Äôs distribution describes the types of values it can take and the likelihood of each value occurring. Variables can be classified as discrete (specific values) or continuous (any value within a range). The shape of a distribution is determined by parameters.\nDiscrete Data: Discrete variables can be categories or counts. Categorical data can be nominal (no order) or ordinal (ordered) and are best summarised with the mode or a table. Counts are often best summarised by the median and interquartile range.\nContinuous Data: Continuous variables can take any value within a range. The normal distribution is the most common continuous distribution and is best summarised with the mean and standard deviation.\nTheory and Practice: While the theoretical distinction between continuous and discrete values is clear, the actual values obtained in practice may deviate from the expected distribution. Measurement precision and range affect whether a variable is considered continuous or discrete. A rule of thumb suggests that if the mean count is above approximately 100, the distribution of counts approximates a normal distribution.\n\n\n\n\n\nLikert, R. 1932. ‚ÄúA Technique for the Measurement of Attitudes.‚Äù Archives of Psychology 22 140: 55‚Äì55.",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Ideas about data</span>"
    ]
  },
  {
    "objectID": "ideas_about_data.html#footnotes",
    "href": "ideas_about_data.html#footnotes",
    "title": "\n5¬† Ideas about data\n",
    "section": "",
    "text": "A¬†quadrat¬†is a frame used in¬†ecology¬†to outline a standard unit of area for study of the distribution of plant species over a wider are. Typically, quadrats are placed randomly over the area and the number of individuals in each quadrat is recorded.‚Ü©Ô∏é",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Ideas about data</span>"
    ]
  },
  {
    "objectID": "first_steps_rstudio.html",
    "href": "first_steps_rstudio.html",
    "title": "\n6¬† First Steps in RStudio\n",
    "section": "",
    "text": "6.1 What are R and Rstudio?\nThis chapter starts by explaining what R and RStudio are and how you can install them on your own machine. We introduce you to working in RStudio, changing its appearance to suit you and to the key things you need to know about R.",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>First Steps in RStudio</span>"
    ]
  },
  {
    "objectID": "first_steps_rstudio.html#what-are-r-and-rstudio",
    "href": "first_steps_rstudio.html#what-are-r-and-rstudio",
    "title": "\n6¬† First Steps in RStudio\n",
    "section": "",
    "text": "6.1.1 What is R?\nR is a programming language and environment for statistical computing and graphics which is free and open source. It is widely used in industry and academia. It is what is known as a ‚Äúdomain-specific‚Äù language meaning that it is designed especially for doing data analysis and visualisation rather than a ‚Äúgeneral-purpose‚Äù programming language like Python and C++. It makes doing the sorts of things that bioscientists do a bit easier than in a general purpose-language.\n\n6.1.2 What is RStudio?\nRStudio is what is known as an ‚Äúintegrated development environment‚Äù (IDE) for R made by Posit. IDEs have features that make it easier to do coding like syntax highlighting, code completion and viewers for files, code objects, packages and plots. You don‚Äôt have to use RStudio to use R but it is very helpful.\n\n6.1.3 Why is it better to use R than Excel, googlesheets or some other spreadsheet program?\nSpreadsheet programs are not statistical packages so although you can carry out some analysis tasks in them they are limited, get things wrong (known about since 1994) and teach you bad data habits. Spreadsheets encourage you to do things that are going to make analysis difficult.\n\n6.1.4 Why is it better to use R than SPSS, Minitab or some other menu-driven statistics program?\n\nR is free and open source which it will always be available to you .\nCarrying out data analysis using coding makes everything you do reproducible\nThe skills and expertise you gain through learning R are highly transferable ‚Äì much more so than those acquired using SPSS.\nSee Thomas Mock‚Äôs demonstration of doing some data analysis in R including ‚ÄúThe Kick Ass Curve‚Äù.\n\nThere are other good options such as Julia and Python and you are encouraged to explore these. We chose R in part because of the R community which is one of R‚Äôs greatest assets, being vibrant, inclusive and supportive of users at all levels. https://ropensci.org/blog/2017/06/23/community/",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>First Steps in RStudio</span>"
    ]
  },
  {
    "objectID": "first_steps_rstudio.html#installing-r-and-rstudio",
    "href": "first_steps_rstudio.html#installing-r-and-rstudio",
    "title": "\n6¬† First Steps in RStudio\n",
    "section": "\n6.2 Installing R and Rstudio",
    "text": "6.2 Installing R and Rstudio\nYou will need to install both R and RStudio to use them on your own machine. Installation is normally straightforward but you can follow a tutorial\n\n6.2.1 Installing R\nGo to https://cloud.r-project.org/ and download the ‚ÄúPrecompiled binary distributions of the base system and contributed packages‚Äù appropriate for your machine.\n\n6.2.1.1 For Windows\nClick ‚ÄúDownload R for Windows‚Äù, then ‚Äúbase‚Äù, then ‚ÄúDownload R 4.#.# for Windows‚Äù. This will download an .exe file. Once downloaded, open (double click) that file to start the installation.\n\n6.2.1.2 For Mac\nClick ‚ÄúDownload R for (Mac) OS X‚Äù, then ‚ÄúR-4.#.#.pkg‚Äù to download the installer. Run the installer to complete installation.\n\n6.2.1.3 For Linux\nClick ‚ÄúDownload R for Linux‚Äù. Instructions on installing are given for Debian, Redhat, Suse and Ubuntu distributions. Where there is a choice, install both r-base and r-base-dev.\n\n6.2.2 Installing R Studio\nGo to https://posit.co/download/rstudio-desktop/",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>First Steps in RStudio</span>"
    ]
  },
  {
    "objectID": "first_steps_rstudio.html#packages",
    "href": "first_steps_rstudio.html#packages",
    "title": "\n6¬† First Steps in RStudio\n",
    "section": "\n6.3 Packages",
    "text": "6.3 Packages\nTODO some text\nInstall tidyverse:\n\ninstall.packages(\"tidyverse\")\ninstall.packages(\"readxl\")",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>First Steps in RStudio</span>"
    ]
  },
  {
    "objectID": "first_steps_rstudio.html#introduction-to-rstudio",
    "href": "first_steps_rstudio.html#introduction-to-rstudio",
    "title": "\n6¬† First Steps in RStudio\n",
    "section": "\n6.4 Introduction to RStudio",
    "text": "6.4 Introduction to RStudio\nIn this section we will introduce you to working in RStudio. We will explain the windows that you see when you first open RStudio and how to change its appearance to suit you. Then we will see how we use R as a calculator and how assign values to R objects.\n\n6.4.1 Changing the appearance\nWhen you first open RStudio it will display three panes and have a white background Figure¬†6.1\n\n\n\n\n\nFigure¬†6.1: When you first open RStudio it will be white with three panes\n\n\nWe will talk more about these three panes soon but first, let‚Äôs get into character - the character of a programmer! You might have noticed that people comfortable around computers are often using dark backgrounds. A dark background reduces eye strain and often makes ‚Äúcode syntax‚Äù more obvious making it faster to learn and understand at a glance. Code syntax is the set of rules that define what the various combinations of symbols mean. It takes time to learn these rules and you will learn best by repeated exposure to writing, reading and copying code. You have done this before when you learned your first spoken language. All languages have syntax rules governing the order of words and we rarely think about these consciously, instead relying on what sounds and looks right. And what sounds and looks right grows out repeated exposure. For example, 35% of languages, including English, Chinese, Yoruba and Polish use the Subject-Verb-Object syntax rule:\n\nEnglish: Emma likes R\nChinese: ËâæÁéõÂñúÊ¨¢R Emma x«êhuƒÅn R\nYoruba: Emma f·∫πran R\nPolish: Emma lubi R\n\nand 40% use Subject-Object-Verb including Turkish and Korean\n\nTurkish: Emma R‚Äôyi seviyor\nKorean: Ïó†ÎßàÎäî RÏùÑ Ï¢ãÏïÑÌïúÎã§ emmaneun Reul joh-ahanda\n\nYou learned this rule in your language very early, long before you were conscious of it, just by being exposed to it frequently. In this book I try to tell you the syntax rules, but you will learn most from looking at, and copying code. Because of this, it is well worth tinkering with the appearance of RStudio to see what Editor theme makes code elements most obvious to you.\nThere is a tool bar at the top of RStudio. Choose the Tools option and then Global options. This will open a window where many options can be changed Figure¬†6.2.\n\n\n\n\n\nFigure¬†6.2: Tools | Global Options opens a window. One of the options is Appearance\n\n\nGo to the Appearance Options and choose and Editor theme you like, followed by OK.\nThe default theme is Textmate. You will notice that all the Editor themes have syntax highlighting so that keywords, variable names, operators, etc are coloured but some themes have stronger contrasts than others. For beginners, I recommend Vibrant Ink, Chaos or Merbivore rather than Dreamweaver or Gob which have little contrast between some elements. However, individuals differ so experiment for yourself. I tend to vary between Solarised light and dark.\nYou can also turn one Screen Reader Support in the Accessibility Options in Tools | Global Options.\nBack to the Panes. You should be looking at three windows: One on the left and two on the right1.\nThe window on the left, labelled Console, is where R commands are executed. In a moment we will start by typing commands in this window. Over on the right hand side, at the top, have several tabs, with the Environment tab showing. This is where all the objects and data that you create will be listed. Behind the Environment tab is the History and later you will be able to view this to see a history of all your commands.\nOn the bottom right hand side, we have a tab called Plots which is where your plots will go, a tab called Files which is a file explorer just like Windows Explorer or Mac Finder, and a Packages tab where you can see all the packages that are installed. The Packages tab also provides a way to install additional packages. The Help tab has access to all the manual pages.\nRight, let‚Äôs start coding!\n\n6.4.2 Your first piece of code\nWe can use R just like a calculator. Put your cursor after the &gt; in the Console, type 3 + 4 and ‚Üµ Enter to send that command:\n\n3+4\n## [1] 7\n\nThe &gt; is called the ‚Äúprompt‚Äù. You do not have to type it, it tells you that R is ready for input.\nWhere I‚Äôve written 3+4, I have no spaces. However, you can have spaces, and in fact, it‚Äôs good practice to use spaces around your operators because it makes your code easier to read. So a better way of writing this would be:\n\n3 + 4\n## [1] 7\n\nIn the output we have the number 7, which, obviously, is the answer. From now on, you should assume commands typed at the console should be followed by ‚Üµ Enter to send them.\nThe one in parentheses, [1], is an index. It is telling you that the 7 is the first element of the output. We can see this more clear if we create something with more output. For example, 50:100 will print the numbers from 50 to 100.\n\n50:100\n##  [1]  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68\n## [20]  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87\n## [39]  88  89  90  91  92  93  94  95  96  97  98  99 100\n\nThe numbers in the square parentheses at the beginning of the line give you the index of the first element in the line. R is telling you where you are in the output.\n\n6.4.3 Assigning variables\nVery often we want to keep input values or output for future use. We do this with ‚Äòassignment‚Äô An assignment is a statement in programming that is used to set a value to a variable name. In R, the operator used to do assignment is &lt;-. It assigns the value on the right-hand to the value on the left-hand side.\nTo assign the value 3 to x we do:\n\nx &lt;- 3\n\nand ‚Üµ Enter to send that command.\nThe assignment operator is made of two characters, the &lt; and the - and there is a keyboard short cut: Alt+- (windows) or Option+- (Mac). Using the shortcut means you‚Äôll automatically get spaces. You won‚Äôt see any output when the command has been executed because there is no output. However, you will see x listed under Values in the Environment tab (top right).\nYour turn! Assign the value of 4 to a variable called y:\n\nCodey &lt;- 4\n\n\nCheck you can see y listed in the Environment tab.\nType x and ‚Üµ Enter to print the contents of x in the console:\n\nx\n## [1] 3\n\nWe can use these values in calculations just like we could in in maths and algebra.\n\nx + y\n## [1] 7\n\nWe get the output of 7 just as we expect. Suppose we make a mistake when typing, for example, accidentally pressing the u button instead of the y button:\n\nx + u\n## Error: object 'u' not found\n\nWe get an error. We will probably see this error quite often - it means we have tried to use a variable that is not in our Environment. So when you get that error, have a quick look up at your environment2.\nWe made a typo and will want to try again. We usefully have access to all the commands that previously entered when we use the ‚Üë Up Arrow. This is known as command recall. Pressing the ‚Üë Up Arrow once recalls the last command; pressing it twice recalls the command before the last one and so on.\nRecall the x + u command (you may need to use the ‚Üì Down Arrow to get back to get it) and use the Back space key to remove the u and then add a y.\nA lot of what we type is going to be wrong - that is not because you are a beginner, it is same for everybody! On the whole, you type it wrong until you get it right and then you move to the next part. This means you are going to have to access your previous commands often. The History - behind the Environment tab - contains everything you can see with the ‚Üë Up Arrow. You can imagine that as you increase the number of commands you run in a session, having access the this record of everything you did is useful. However, the thing about the history is, that it has everything you typed, including all the wrong things!\nWhat we really want is a record of everything we did that was right! This is why we use scripts instead of typing directly into the console.\n\n6.4.4 Using Scripts\nAn R script is a plain text file with a .R extension and containing R code. Instead of typing into the console, we normally type into a script and then send the commands to the console when we are ready to run them. Then if we‚Äôve made any mistakes, we just correct our script and by the end of the session, it contains a record of everything we typed that worked.\nYou have several options open a new script:\n\nbutton: Green circle with a white cross, top left and choose R Script\nmenu: File | New File | R Script\nkeyboard shortcut: Ctrl+Shift+N (Windows) / Shift+Command+N (Mac)\n\nOpen a script and add the two assignments to it:\n\nx &lt;- 3\ny &lt;- 4\n\nTo send the first line to the console, we place our cursor on the line (anywhere) and press Ctrl-Enter (Windows) / Command-Return. That line will be executed in the console and in the script, our cursor will jump to the next line. Now, send the second command to the console in the same way.\nFrom this point forward, you should assume commands should be typed into the script and sent to the console.\nAdd the incorrect command attempting to sum the two variables:\n\nx + u\n## Error: object 'u' not found\n\nTo correct this, we do not add another line to the script but instead edit the existing command:\n\nx + y\n## [1] 7\n\nIn addition to making it easy to keep a record of your work, scripts have another big advantage, you can include ‚Äòcomments‚Äô - pieces of text that describe what the code is doing. Comments are indicated with a # in front of them. You can write anything you like after a # and R will recognise that it is not code and doesn‚Äôt need to be run.\n\n# This script performs the sum of two values\n\nx &lt;- 3    # can be altered\ny &lt;- 4    # can be altered\n\n# perform sum\nx + y\n## [1] 7\n\nThe comments should be highlighted in a different colour than the code. They will be italic in some Editor themes.\nYou have several options save a script:\n\nbutton: use the floppy disc icon\nmenu: File | Save\nkeyboard shortcut: Ctrl+S (Windows) / Command+S (Mac)\n\nYou could use a name like test1.R - note the .R extension wil be added automatically.\n\n6.4.5 Other types of file in RStudio\n\n\n.R script code but not the objects. You always want to save this\n\n.Rdata also known as the workspace or session, the objects, but not the code. You usually do not want to save this. Some exceptions e.g., if it takes quite a long time to run the commands.\ntext files\n\n6.4.6 Changing some defaults to make life easier\nI recommend changing some of the default settings to make your life a little easier. Go back into the Global Options window with Tools | Global Options. The top tab is General Figure¬†6.3.\n\n\n\n\n\nFigure¬†6.3: Tools | Global Options opens a window. One of the options is General. This where you can change the default behaviour of RStudio. Highlighted is the default (start up) directory and the option to Save and Restore the workspace.\n\n\nFirst, we will set our default working directory. Under ‚ÄòDefault working directory (when not in a project3):‚Äô click Browse and navigate to a through your file system to a folder where you want to work. You may want to create a folder specifically for studying this book.\nSecond, we will change the Workspace options. Turn off ‚ÄòRestore .RData into workspace at startup‚Äô and change ‚ÄòSave workspace to .RData on exit‚Äô to ‚ÄòNever‚Äô. These options mean R will start up clean each time.",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>First Steps in RStudio</span>"
    ]
  },
  {
    "objectID": "first_steps_rstudio.html#recap-of-rstudio-anatomy",
    "href": "first_steps_rstudio.html#recap-of-rstudio-anatomy",
    "title": "\n6¬† First Steps in RStudio\n",
    "section": "\n6.5 Recap of RStudio anatomy",
    "text": "6.5 Recap of RStudio anatomy\nThis figure (see Figure¬†6.44) summarises shows what each of the four RStudio panes and what they are used for to summarise much of what we have covered so far.\n\n\n\n\n\n\n\nFigure¬†6.4: A screenshot of RStudio‚Äôs four panes annotated with what each pane is for.",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>First Steps in RStudio</span>"
    ]
  },
  {
    "objectID": "first_steps_rstudio.html#data-types-and-structures-in-r",
    "href": "first_steps_rstudio.html#data-types-and-structures-in-r",
    "title": "\n6¬† First Steps in RStudio\n",
    "section": "\n6.6 Data types and structures in R",
    "text": "6.6 Data types and structures in R\nIn this section, we are going to introduce you to some of R‚Äôs data types and structures. We won‚Äôt be covering all of them now, just those you are going to use often in this part of the book. These are numerics (numbers), characters, ‚Äòlogicals‚Äô, vectors and dataframes. We can do a lot with just these. We will also cover using functions.\nWe are going to consider\n\ntypes of value also known as data types\nfunctions\ndata structures\n\n\n6.6.1 Data types\nThis refers to the type of value that something is. They might be numerics or characters or ‚Äòlogical‚Äô (either true of false). We assign a number, like the value of 23 to a variable called x like this:\n\nx &lt;- 23\nx\n## [1] 23\n\nWe do not need to use quotes for numbers but we do need to use them for characters and can assign the word banana to the variable a like this:\n\na &lt;- \"banana\"\na\n## [1] \"banana\"\n\nQuotes are needed because otherwise R wouldn‚Äôt know whether you were referring to a value or a existing object called banana. This is also why you can‚Äôt have variable name like 14 - R would not be able to tell the difference between the number 14 and an object named 14 since numbers and objects don‚Äôt need quotes.\nAnything composed of non-numeric characters, including single characters, need to have quotes around it. You can even force a number to be a character by putting quotes around it:\n\nb &lt;- \"23\"\nb\n## [1] \"23\"\n\nNotice that things inside quotes appear in a different colour (the colour will depend on the Editor theme you choose). This will help you identify when you have forgotten some closing quotes5:\n\na &lt;- \"banana\nx &lt;- 23\n\nAlthough the data type is ‚Äòcharacter‚Äô we often use the term ‚Äòstring‚Äô for collections - strings - of characters\nWe also have special values called ‚Äòlogicals‚Äô which take a value of either TRUE or FALSE.\n\nc &lt;- TRUE\nc\n## [1] TRUE\n\nAlthough TRUE is a word, R recognises it as special word. It appears in a different colour and no quotes are needed. This is the same for FALSE.\n\nc &lt;- FALSE\nc\n## [1] FALSE\n\nAs you type FALSE, the colour changes as it recognises the special word FALSE. Try to pay attention to the editor theme‚Äôs colouring - it is trying to help you!\n\n6.6.2 Functions\nThe aim of this section is to help you understand the logic of using a function. Functions have a name and then a set of parentheses. The function name minimally explains what the function does. Inside the parentheses are ‚Äòarguments‚Äô - the pieces of information you give to the function. When coding, we often talk about passing arguments to functions and calling functions. A simple function call looks like this:\nfunction_name(argument)\nA function can take zero to many arguments. Where you give several arguments, they are separated by commas:\nfunction_name(argument1, argument2, argument3, ...)\nThe first function you are going to use is str() which gives the structure of an object:\n\nstr(x)\n##  num 23\n\nIt‚Äôs telling me that x is a number and contains 23.\nYour turn! Use str() on b:\n\nCodestr(b)\n##  chr \"23\"\n\n\nstr() is a function I use a lot to check what sort of R object I have.\nWe must give str() at least one argument, the object we want the structure of, but additional arguments are also possible. Later we will discover how to find out and use a function‚Äôs arguments.\nSo far we our objects have consisted of a single thing but usually we have several bits of data that we want to collect together into a data structure.\n\n6.6.3 Data structures: vectors\nImagine we have the ages of six people. Since all the numbers are ages, we would want to keep them together. The minimal data structure is called a vector. We can create a vector, which collects together several numbers using a function, c(). This is one of only a few functions in R with a single-letter name. Because it has a single letter, sometimes people get confused about it but we can tell it is a function because it has a set of parentheses.\nTo create a vector called ages of several numbers we use\n\nages &lt;- c(23, 42, 7, 9, 54, 12)\n\nType ages and run if you want to print the contents to the console:\n\nages\n## [1] 23 42  7  9 54 12\n\nUsing str() on ages\n\nstr(ages)\n##  num [1:6] 23 42 7 9 54 12\n\ntells us we have numbers with the indices of 1 to 6.\nWe can also create a vector of strings. Suppose we have names to go with the ages.\n\nnames &lt;- c(\"Rowan\", \"Aang\", \"Zain\", \"Charlie\", \"Jules\", \"Efe\")\n\nRStudio has a super useful feature for putting items in quotes, brackets or parentheses if you initially forget them: If you select something and type the opening element, that thing will be surrounded rather than over written. For example, if you had written:\n\nnames &lt;- Rowan, Aang, Zain, Charlie, Jules, Efe\n\nSelect one of the names and then type an opening quote - you should see the name is then surrounded by quotes rather than overwritten. You can repeat this process for all the names (note that double clicking on the name will select it). Selecting the whole list and typing an opening parenthesis will put the whole list in parentheses. This is a feature you get to love so much and use so often that other programs will annoy you when you overwrite something you meant to quote!\nSo we can also have vectors of logical values. For example, we might have a vector that indicates that Rowan, Aang and Charlie like chocolate, but Zain, Jules and Efe do not:\n\nchocolate &lt;- c(TRUE, TRUE, FALSE, TRUE, FALSE, FALSE)\n\nRemember, because TRUE and FALSE are special words so we do not need quote them.\n\n6.6.4 Indexing vectors\nYou might be wondering how to get a single element out of a vector if typing the vector‚Äôs name prints the entire vector. This is by ‚Äòindexing‚Äô. An index is a number from 16 to the length of a vector which gives the position in the vector and is denoted by square brackets. For example, to pull out the second element of ages:\n\nages[2]\n## [1] 42\n\nYour turn! Print the last element of names:\n\nCodenames[6]\n## [1] \"Efe\"\n\n\nWe an extract more than one element by giving more than one index. If the indices are adjacent like 3rd, 4th and 5th, we have use the colon:\n\nnames[3:5]\n## [1] \"Zain\"    \"Charlie\" \"Jules\"\n\nIf the indices are not adjacent, like 2 and 6 with need to combine them with c():\n\nnames[c(2, 6)]\n## [1] \"Aang\" \"Efe\"\n\nWe can also use a logical vector to extract elements. Suppose you want to extract the names and ages of the people who like chocolate:\n\nnames[chocolate]\n## [1] \"Rowan\"   \"Aang\"    \"Charlie\"\nages[chocolate]\n## [1] 23 42  9\n\nAt each of the indices in chocolate that contain TRUE, the name and age are returned.\n\n6.6.5 Changing the defaults for a function\nThe functions we have used so far, c() and str() have worked without us having to change default behaviour. For example, if we want to calculate the mean age of our people we can use the mean() function in default form:\n\nmean(ages)\n## [1] 24.5\n\nImagine Charlie would like their age removed from the dataset or that we never knew their age. We would not want a vector containing just five elements because the ages would not match the people in the same position in the vector. Instead, we would have a missing value at that position. Missing values are NA (not applicable) in R and NA is another special word, like TRUE and FALSE, that doesn‚Äôt need quotes. We can set Charlie‚Äôs age to NA using indexing:\n\nages[names == \"Charlie\"] &lt;- NA\nages\n## [1] 23 42  7 NA 54 12\n\nThe == means ‚Äúis equal to‚Äù and the result of names == \"Charlie\" is a vector of logicals: FALSE FALSE FALSE  TRUE FALSE FALSE. This means ages[names == \"Charlie\"] references the age in ages at the index of Charlie in names\nIf we now try to calculate the mean age:\n\nmean(ages)\n## [1] NA\n\nWe get an NA! What we really want is an average of the ages we do have. The mean() function has an argument that allows you to cope with that situation called na.rm. By default, na.rm is set to FALSE but we can set it to TRUE using\n\nmean(ages, na.rm = TRUE)\n## [1] 27.6\n\n\n6.6.6 Data structures: dataframes\nWe have three vectors, names, ages and chocolate which are all part of the same dataset. By far the most common way of organising data in R is within a ‚Äúdataframe‚Äù. A dataframe, has rows and columns: each column represents a variable and each row represents a case. To make a dataframe using our three vectors we use:\n\npeople &lt;- data.frame(names, ages, chocolate)\n\nYou will see people listed in the Global environment under Data. To open a spreadsheet-like view of the dataframe click its name in the Global Environment Figure¬†6.5\n\n\n\n\n\nFigure¬†6.5: To open a spreadsheet-like view of the dataframe click its name in the Global Environment",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>First Steps in RStudio</span>"
    ]
  },
  {
    "objectID": "first_steps_rstudio.html#summary",
    "href": "first_steps_rstudio.html#summary",
    "title": "\n6¬† First Steps in RStudio\n",
    "section": "\n6.7 Summary",
    "text": "6.7 Summary\nTODO",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>First Steps in RStudio</span>"
    ]
  },
  {
    "objectID": "first_steps_rstudio.html#footnotes",
    "href": "first_steps_rstudio.html#footnotes",
    "title": "\n6¬† First Steps in RStudio\n",
    "section": "",
    "text": "If this is not a fresh install of RStudio, you might be looking at fours windows, two on the left and two on the right. That‚Äôs fine - we will al be using four shortly. For the time being, you might want to close the ‚ÄúScript‚Äù window using the small cross next to ‚ÄúUntitled1‚Äù.‚Ü©Ô∏é\nWhen we are using scripts, it is very easy to write code but forget to run it. Very often when you see this error it will because you have written the code to create an object but forgotten to execute it.‚Ü©Ô∏é\nWe will find out what an RStudio Project is very soon. You will want to use a project for most of your work - they make everything a little easier.‚Ü©Ô∏é\nYou can zoom into this at the Direct link‚Ü©Ô∏é\nRStudio makes it hard for you to forget closing quotes and parentheses because when you type an opening quote or parenthesis, it automatically adds its closing partner. When people are learning they are sometimes tempted to delete these so they can type what goes inside the quotes/parentheses and then manually add the closing partner. I strongly recommend you don‚Äôt not delete them. RStudio adds the closing character but it leaves your cursor in the right position to complete the contents.‚Ü©Ô∏é\nWe start counting from 1 in R. Most programming languages count from zero.‚Ü©Ô∏é",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>First Steps in RStudio</span>"
    ]
  },
  {
    "objectID": "workflow_rstudio.html",
    "href": "workflow_rstudio.html",
    "title": "\n7¬† Workflow in RStudio\n",
    "section": "",
    "text": "7.1 RStudio Projects\nThis chapter give you some tips for make your workflow in RStudio easier.",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Workflow in RStudio</span>"
    ]
  },
  {
    "objectID": "workflow_rstudio.html#rstudio-projects",
    "href": "workflow_rstudio.html#rstudio-projects",
    "title": "\n7¬† Workflow in RStudio\n",
    "section": "",
    "text": "7.1.1 What is an RStudio Project?\nUsing an RStudio Project will help you organise your analysis work, make it much easier to manage working directories and paths and also to collaborate with others including yourself on another computer! An RStudio Project is a folder that contains a file with the extension .RProj and all the code, data, and other files associated with a particular piece of work.\nFor example, if you were analysing some data on stem cells you might have an RStudio Project called ‚Äústem-cells‚Äù. This would be a folder called stem-cells, known as the project folder, which contains the stem-cells.Rproj file - both of these are created automatically. Then you might create folders for the data and for figures from the analysis along with the script that contains code for importing the data, doing the analysis, creating the figures and writing the figures to file.\n-- stem-cells\n   |__stem-cells.Rproj\n   |__analysis.R\n   |__data-raw\n      |__2019-03-21_donor_1.csv\n      |__2019-03-21_donor_2.csv\n      |__2019-03-21_donor_3.csv\n   |__figures\n      |__01_volcano_donor_1_vs_donor_2.png\n      |__02_volcano_donor_1_vs_donor_3.png\n\nWhen you open an RStudio Project, it automatically sets the project directory as the working directory. This means when you write your code with paths relative to the project directory your code will work the same on any computer you send that RStudio Project to.\n\n7.1.2 Creating an RStudio Project\n\nClick on the drop-down menu on top right where it says ‚ÄúProject: (None)‚Äù and choose New Project\nA dialogue box will appear. Choose ‚ÄúNew Directory‚Äù, then ‚ÄúNew Project‚Äù\nClick the Browse button next to ‚ÄúCreate project as a subdirectory of:‚Äù to navigate to a place in your file system where you want to create the project folder.\nType a name the ‚ÄúDirectory name‚Äù. This should be something that helps you identify the contents. Follow the advice in Naming things",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Workflow in RStudio</span>"
    ]
  },
  {
    "objectID": "workflow_rstudio.html#some-useful-settings",
    "href": "workflow_rstudio.html#some-useful-settings",
    "title": "\n7¬† Workflow in RStudio\n",
    "section": "\n7.2 Some useful settings",
    "text": "7.2 Some useful settings\nYou can adjust some of the default settings in RStudio you suit your own needs better. The settings are accessed through the Tools Menu under Global options. I like the following settings:\n\nWhen using RStudio Projects the working directory is the Project directory but when you start RStudio up and want to make a new project you might find the default location doesn‚Äôt suit you. You can change the default directory when not in a project.\nTo ensure you have a fresh session with no R objects in the workspace you can change Workspace options.\n\nIn the Code options\n\nDisplay - check Use rainbow brackets which makes it easier to see which bracket are pairs\nDisplay - check Show margin which will add a line at 80 characters to help you use new lines more often and not create very long lines of code that are difficult to read\nDiagnostics - check Show diagnostics for R which will put a marker on the line that includes a syntax error and make a suggestion what the error is\nDiagnostics - check Provide R style diagnostics (e.g.¬†whitespace) which will help you layout your code",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Workflow in RStudio</span>"
    ]
  },
  {
    "objectID": "workflow_rstudio.html#handy-housekeeping-command",
    "href": "workflow_rstudio.html#handy-housekeeping-command",
    "title": "\n7¬† Workflow in RStudio\n",
    "section": "\n7.3 Handy housekeeping command",
    "text": "7.3 Handy housekeeping command\n\n7.3.1 Where am I?\nThere are several ways you can find out what your working directory is.\n\nCode. The getwd() (get working directory)\n\n\ngetwd()\n\n\nAlong the top of the Console window. There is also a little arrow you can click to show your working directory in the Files pane.\nIn the Files pane, provided to have not navigated around in there. If you have, you can view your working directory using blue wheel and choosing ‚ÄúGo To Working Directory‚Äù or by using the arrow on the top of the Console window\n\n7.3.2 What files can I see?\nThere are several ways you can see the files and folders in your working directory.\n\n\nCode. The dir() (directory)\n\ndir()\n\n\n\ndir() list the contents of the working directory\n\ndir(\"..\") list the contents of the directory above the working directory\n\ndir(../..) list the contents of the directory two directories above the working directory\n\ndir(\"data-raw\") list the contents of a folder call data-raw which is in the working directory.\n\n\nLook in the the Files pane, provided to have not navigated around in there. If you have, you can view your working directory using blue wheel and choosing ‚ÄúGo To Working Directory‚Äù or by using the arrow on the top of the Console window\n\n7.3.3 What R objects can I see?\n\n\nCode. The ls() (list)\n\nls()\n\n\nLook in the the Environment pane",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Workflow in RStudio</span>"
    ]
  },
  {
    "objectID": "workflow_rstudio.html#understanding-the-pipe",
    "href": "workflow_rstudio.html#understanding-the-pipe",
    "title": "\n7¬† Workflow in RStudio\n",
    "section": "\n7.4 Understanding the pipe |>\n",
    "text": "7.4 Understanding the pipe |&gt;\n\nThe pipe operator improves code readability by:\n\nstructuring sequences of data operations left-to-right and top to bottom rather than from inside and out),\nminimizing the need for intermediates,\nmaking it easy to add steps anywhere in the sequence of operations.\n\nFor example, suppose we want to apply a log-square root transformation which is sometimes applied to make a flat distribution more normal. There are two approaches we could use without the pipe: nesting the functions and creating an intermediate. We will consider both of these. First, let us generate a few numbers of work with:\n\n# generate some numbers\n# this will give me ten random numbers between 1 and 100\nnums &lt;- sample(1:100, size = 10)\n\nTo apply the transformation we can nest the functions so the output put of sqrt(nums) becomes the input of log():\n\n# apply a log-square root transformation\ntnums &lt;- log(sqrt(nums))\ntnums\n##  [1] 2.021526 1.994492 2.297560 2.232954 1.472219 1.748254 2.209420 2.063567\n##  [9] 2.221326 1.039721\n\nThe first function to be applied is innermost. When we are using just two functions, the level of nesting does not cause too much difficulty in reading the code. However, you can image this gets more unreadable as the number of functions applied increases. It also makes it harder to debug and find out where an error might be. One solution is to create intermediate variables so the commands a given in order:\n\n# apply a log-square root transformation\nsqrtnums &lt;- sqrt(nums)\ntnums &lt;- log(sqrtnums)\n\nUsing intermediates make your code easier to follow at first but clutters up your environment and code with variables you don‚Äôt care about. You also start of run out names!\nThe pipe is a more elegant and readable solution. It allows you to send the output of one operation as input to the next function. The pipe has long been used by Unix operating systems (where the pipe operator is |). The R pipe operator is |&gt;, a short cut for which is Ctrl+Shift+M.\nUsing the pipe, we can apply out transformation with:\n\ntnums &lt;- nums |&gt; \n  sqrt() |&gt; \n  log()\n\nThe command are in the order applied, there are no intermediates and the code is easier to debug and to build-up step-by-step..\nNote that |&gt; is the pipe that comes with base R which was only added in the last couple of years. Before it existed, **tidyverse** had a pipe operator provided by the *magrittr** package. The magrittr pipe is %&gt;%. In your googling, you may well see code written using the %&gt;%. In most cases, the pipes are interchangeable.\nWhat They Forgot to Teach You About R Bryan et al. (n.d.)\n\n\n\n\nBryan, Jennifer, Jim Hester, Shannon Pileggi, and E. David Aja. n.d. What They Forgot to Teach You About R. Accessed September 26, 2019.",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Workflow in RStudio</span>"
    ]
  },
  {
    "objectID": "import_to_report.html",
    "href": "import_to_report.html",
    "title": "\n8¬† From importing to reporting\n",
    "section": "",
    "text": "8.1 Importing data from files\nIn First Steps in RStudio we typed data into R. This is not very practical when you have a lot of data! Instead, we much more commonly import data from a file. In this chapter we go through the workflow from importing, through summarising and plotting to saving a saving the figure.\nThere are two things you need to know before you can import data from a file.\nüé¨ Your turn! If you want to code along you will need to start a new RStudio project then a new script.\nThis chapter covers reading .txt files and .csv files using tidyverse (Wickham et al. 2019) functions and excel files using the readxl (Wickham and Bryan 2023) package. We will demonstrate what needs to be done differently if the file is not in your working directory.\nlibrary(tidyverse)\nlibrary(readxl)",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>From importing to reporting</span>"
    ]
  },
  {
    "objectID": "import_to_report.html#importing-data-from-files",
    "href": "import_to_report.html#importing-data-from-files",
    "title": "\n8¬† From importing to reporting\n",
    "section": "",
    "text": "What format the data are in\nThe format of the data determines what function you will use to import it. Often the file extension indicates format.\n\n\n.txt a plain text file1, where the columns are often separated by a space but might also be separated by a tab, a backslash or forward slash, or some other character\n\n.csv a plain text file where the columns are separated by commas\n\n.xlsx an Excel file More detail on file types was covered in Understanding file systems\n\n\nHowever, you should always check the file to make sure it is in the format you expect because there is little to force a match between a file‚Äôs contents and the extension in its name. You can check by opening the file in a text editor (e.g., Notepad on Windows, TextEdit on Mac) or in RStudio (see below).\n\n\nWhere the file is relative to your working directory\nR can only read in a file if you say where it is, i.e., you give its relative path. More detail on relative file paths and working directories was covered in Understanding file systems\n\n\n\n\n\n\n8.1.1 Importing data from .txt file\nThe data in adipocytes.txt give the concentration of a hormone called adiponectin in some cells. There are two columns: the first gives the adiponectin concentration and the second, treatment, indicates whether the cells were treated with nicotinic acid or not. Save this file to the project folder.\nA .txt extension suggests this is plain text file with columns separated by spaces. However, before we attempt to read it in, when should take a look at it. We can do this from RStudio by clicking on the file in the Files pane. Any plain text file will open in the top left pane.\n\n\n\n\n\nFigure¬†8.1: The adipocytes.txt data file open. We can see the columns are separated by spaces\n\n\nThe files are separated by spaces as we suspected. We use the read_table() command to read in plain text files of single columns, or where the columns are separated by spaces:\n\nadipo &lt;- read_table(\"adipocytes.txt\")\n\nThe data from the file has been read into a dataframe called adipo. You will and you will be able to see it in the Environment window. Clicking on it in the Environment window will open a spreadsheet-like view of the dataframe.\n\n8.1.2 Importing a from a.csv file\nThe data seal.csv give the myoglobin concentration of skeletal muscle for three species of seal. There are two columns: the first gives the myoglobin concentration and the second indicates species.\nThe .csv extension suggests this is plain text file with columns separated by commas. We will again check this before we attempt to read it in. Click on the file in the Files pane - a pop-up will appear for files ending .csv or .xlsx. Choose View File2.\n\n\nRstudio Files pane showing the data files and the View File option that appears when you click on the a particular file\n\nCSV files will open in the top left pane (Excel files will launch Excel). We can see that the file does contain comma separated values. There is aread_csv() function which works very like read_table()[^working_with_data_rstudio-3]:\n\nseal &lt;- read_csv(\"seals.csv\")\n\n\n8.1.3 Importing a from a.xlsx file\nThe data in blood.xlsx are measurements of several blood parameters from fifty people with Crohn‚Äôs disease, a lifelong condition where parts of the digestive system become inflamed. Twenty-five of people are in the early stages of diagnosis and 25 have started treatment.\n\nblood &lt;- read_excel(\"blood.xlsx\")\n\n\n8.1.4 Importing data from a file not in your working directory\nWhen using an RStudio project, your working directory is the project folder (the folder containing the .Rproj file. Suppose our file adipocytes.txt is in a folder, data-raw, in our working directory.\n-- myproject\n   |__myproject.Rproj\n   |__import.R\n   |__data-raw\n      |__adipocytes.txt\n      |__blood.xlsx\n      |__seal.csv\nWe need to adjust the code to give the relative path to the datafile:\n\nadipo &lt;- read_table(\"data-raw/adipocytes.txt\")\n\nThe relative path is the path from the working directory to the file. In this case, the working directory is myproject and the relative path is data-raw/adipocytes.txt.\nüé¨ Your turn! Create a folder called data-raw inside the project folder and move the data files to it. Now modify the data import code to import seal.csv from data-raw.\n\nCodeseal &lt;- read_csv(\"data-raw/seal.csv\")",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>From importing to reporting</span>"
    ]
  },
  {
    "objectID": "import_to_report.html#summarising-data",
    "href": "import_to_report.html#summarising-data",
    "title": "\n8¬† From importing to reporting\n",
    "section": "\n8.2 Summarising data",
    "text": "8.2 Summarising data\nWe summarise data using the the dplyr (Wickham et al. 2023) package, which provides a set of functions designed for efficient data manipulation. This is a tidyverse (Wickham et al. 2019) package which you already loaded. The approach replies on the data being in a tidy format, meaning each column represents a variable, each row represents an observation, and each cell contains a single value. The pipeline is:\n\nGroup the data: If you want to summarize your data based on certain groups, you can use the group_by()\nSummarise: Once your data grouped (if necessary), you use summarise() with functions likemean(), median(), sd(), min()and max() within it.\n\nWe will demonstrate summarising using the adipo dataframe. adiponectin is the response and is continuous and treatment is an explanatory with categorical with two levels (groups).\nThe most useful summary statistics for a continuous variable like adiponectin are the means, standard deviations, sample sizes and standard errors. We use the group_by() and summarise() functions along with the functions that do the calculations.\nTo create a data frame called adipo_summary that contains the means, standard deviations, sample sizes and standard errors for the control and nicotinic acid treated samples:\n\nadipo_summary &lt;- adipo  |&gt; \n  group_by(treatment) |&gt;\n  summarise(mean = mean(adiponectin),\n            std = sd(adiponectin),\n            n = length(adiponectin),\n            se = std/sqrt(n))\n\nYou can type:\n\nadipo_summary\n## # A tibble: 2 √ó 5\n##   treatment  mean   std     n    se\n##   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n## 1 control    5.55  1.48    15 0.381\n## 2 nicotinic  7.51  1.79    15 0.463\n\nor click on environment to open a spreadsheet-like view of the dataframe.\n\n8.2.1 Visualise data\nMost commonly, we put the explanatory variable on the x axis and the response variable on the y axis. A continuous response, particularly one that follows the normal distribution, is best summarised with the mean and the standard error. In my opinion, you should also show all the raw data points if possible.\nWe are going to create a figure like this:\n\n\n\n\n\n\n\n\n\n8.2.1.1 ggplot2\n\nggplot2 (Wickham 2016) is a powerful data visualisation package in R that is part of the tidyverse. It provides a flexible and layered approach to creating high-quality and customizable graphics.\nThe core concept of ggplot2 is to build a plot layer by layer. The basic structure consists of three main components:\n\nData: the data frame to be used for plotting.\nAesthetic mappings (aes): how variables in the data map to visual elements such as x and y positions, colours, shapes, etc.\nGeometric objects (geoms): the actual graphical elements used to visualize the data, such as points, lines, bars, etc.\n\nTo create a basic plot, you start with the ggplot() function and provide the data and aesthetic mappings.\nggplot(data = adipo, aes(x = treatment, y = adiponectin))\n\nYou can add geometric layers to the plot using specific functions such as geom_point(), geom_line(), geom_bar(), etc. These functions define the type of plot you want to create:\nggplot(data = adipo, aes(x = treatment, y = adiponectin)) +\n  geom_point()\n\nIn the figure we are aiming for, we are plotting two dataframes: the adipo dataframe which contains the data points themselves; and the adipo_summary dataframe containing and the means and standard errors.\nThe dataframes and aesthetics for ggplot can be specified within a geom_xxxx (rather than in the ggplot()). This is very useful if the geom only applies to some of the data you want to plot.\n\n\n\n\n\n\nTip: ggplot()\n\n\n\nYou put the data argument and aes() inside ggplot() if you want all the geoms to use that dataframe and variables. If you want a different dataframe for a geom, put the data argument and aes() inside the geom_xxxx()\n\n\nI will build the plot up in small steps you should edit your existing ggplot() command as we go.\nWe will plot the data points first. Notice that we have given the data argument and the aesthetics inside the geom_point(). The variables treatment and adiponectin are in the adipo dataframe\n\nggplot() +\n  geom_point(data = adipo, \n             aes(x = treatment, y = adiponectin))\n\n\n\n\n\n\n\nSo the data points don‚Äôt overlap, we can add some random jitter in the x direction (edit your existing code):\n\nggplot() +\n  geom_point(data = adipo, \n             aes(x = treatment, y = adiponectin),\n             position = position_jitter(width = 0.1, height = 0))\n\n\n\n\n\n\n\nNote that position = position_jitter(width = 0.1, height = 0) is inside the geom_point() parentheses, after the aes() and a comma.\nWe‚Äôve set the vertical jitter to 0 because, in contrast to the categorical x-axis, movement on the y-axis has meaning (the adiponectin levels).\nLet‚Äôs make the points a light grey (edit your existing code):\n\nggplot() +\n  geom_point(data = adipo, \n             aes(x = treatment, y = adiponectin),\n             position = position_jitter(width = 0.1, height = 0),\n             colour = \"grey50\")\n\n\n\n\n\n\n\nNow to add the errorbars. These go from one standard error below the mean to one standard error above the mean.\nAdd a geom_errorbar() for errorbars (edit your existing code):\n\nggplot() +\n  geom_point(data = adipo, aes(x = treatment, y = adiponectin),\n             position = position_jitter(width = 0.1, height = 0),\n             colour = \"grey50\") +\n  geom_errorbar(data = adipo_summary, \n                aes(x = treatment, ymin = mean - se, ymax = mean + se),\n                width = 0.3) \n\n\n\n\n\n\n\nWe have specified the adipo_summary dataframe and the variables treatment, mean and se are in that.\nThere are several ways you could add the mean. You could use geom_point() but I like to use geom_errorbar() again with the ymin and ymax both set to the mean.\nAdd a geom_errorbar() for the mean (edit your existing code):\n\nggplot() +\n  geom_point(data = adipo, aes(x = treatment, y = adiponectin),\n             position = position_jitter(width = 0.1, height = 0),\n             colour = \"grey50\") +\n  geom_errorbar(data = adipo_summary, \n                aes(x = treatment, ymin = mean - se, ymax = mean + se),\n                width = 0.3) +\n  geom_errorbar(data = adipo_summary, \n                aes(x = treatment, ymin = mean, ymax = mean),\n                width = 0.2)\n\n\n\n\n\n\n\nAlter the axis labels and limits using scale_y_continuous() and scale_x_discrete() (edit your existing code):\n\nggplot() +\n  geom_point(data = adipo, aes(x = treatment, y = adiponectin),\n             position = position_jitter(width = 0.1, height = 0),\n             colour = \"grey50\") +\n  geom_errorbar(data = adipo_summary, \n                aes(x = treatment, ymin = mean - se, ymax = mean + se),\n                width = 0.3) +\n  geom_errorbar(data = adipo_summary, \n                aes(x = treatment, ymin = mean, ymax = mean),\n                width = 0.2) +\n  scale_y_continuous(name = \"Adiponectin (pg/mL)\", \n                     limits = c(0, 12), \n                     expand = c(0, 0)) +\n  scale_x_discrete(name = \"Treatment\", \n                   labels = c(\"Control\", \"Nicotinic acid\"))\n\n\n\n\n\n\n\nYou only need to use scale_y_continuous() and scale_x_discrete() to use labels that are different from those in the dataset. Often this is to use proper terminology and captialisation.\nFormat the figure in a way that is more suitable for including in a report using theme_classic() (edit your existing code):\n\nggplot() +\n  geom_point(data = adipo, aes(x = treatment, y = adiponectin),\n             position = position_jitter(width = 0.1, height = 0),\n             colour = \"gray50\") +\n  geom_errorbar(data = adipo_summary, \n                aes(x = treatment, ymin = mean - se, ymax = mean + se),\n                width = 0.3) +\n  geom_errorbar(data = adipo_summary, \n                aes(x = treatment, ymin = mean, ymax = mean),\n                width = 0.2) +\n  scale_y_continuous(name = \"Adiponectin (pg/mL)\", \n                     limits = c(0, 12), \n                     expand = c(0, 0)) +\n  scale_x_discrete(name = \"Treatment\", \n                   labels = c(\"Control\", \"Nicotinic acid\")) +\n  theme_classic()\n\n\n\n\n\n\n\nThe ggsave() function is used to save a ggplot object as an image file. It provides a convenient way to export your plots to various file formats, such as PNG, PDF, SVG, or JPEG.\nThe basic syntax of ggsave() is as follows:\nggsave(filename,\n       plot,\n       device,\n       width,\n       height,\n       units,\n       dpi)\nYou must give a file name for the output file but all the other options have defaults.\n\nplot: The ggplot object you want to save. Defaults to the last created plot.\ndevice: one of ‚Äúpng‚Äù, ‚Äúeps‚Äù, ‚Äúps‚Äù, ‚Äútex‚Äù, ‚Äúpdf‚Äù, ‚Äújpeg‚Äù, ‚Äútiff‚Äù, ‚Äúpng‚Äù, ‚Äúbmp‚Äù, ‚Äúsvg‚Äù or ‚Äúwmf‚Äù (windows only). Defaults to the format given by the file extension in filename\nwidth, height units: Plot size in units (‚Äúin‚Äù, ‚Äúcm‚Äù, ‚Äúmm‚Äù, or ‚Äúpx‚Äù). Defaults to the size of the plot in the Plots window.\ndpi: Plot resolution.\n\nWe can save the figure we just created as a 3 inch x 3 inch png file as follows:\n\nggsave(\"adipocytes.png\",\n       device = \"png\",\n       width = 3,\n       height = 3,\n       units = \"in\",\n       dpi = 300)\n\nIt is often a good idea to explicitly assign the ggplot object to a variable and use that in the ggsave()\n\nfig1 &lt;- ggplot() +\n  geom_point(data = adipo, aes(x = treatment, y = adiponectin),\n             position = position_jitter(width = 0.1, height = 0),\n             colour = \"gray50\") +\n  geom_errorbar(data = adipo_summary, \n                aes(x = treatment, ymin = mean - se, ymax = mean + se),\n                width = 0.3) +\n  geom_errorbar(data = adipo_summary, \n                aes(x = treatment, ymin = mean, ymax = mean),\n                width = 0.2) +\n  scale_y_continuous(name = \"Adiponectin (pg/mL)\", \n                     limits = c(0, 12), \n                     expand = c(0, 0)) +\n  scale_x_discrete(name = \"Treatment\", \n                   labels = c(\"Control\", \"Nicotinic acid\")) +\n  theme_classic()\n\nggsave(\"adipocytes.png\",\n       plot = fig1,\n       device = \"png\",\n       width = 3,\n       height = 3,\n       units = \"in\",\n       dpi = 300)",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>From importing to reporting</span>"
    ]
  },
  {
    "objectID": "import_to_report.html#tidy-data",
    "href": "import_to_report.html#tidy-data",
    "title": "\n8¬† From importing to reporting\n",
    "section": "\n8.3 Tidy data",
    "text": "8.3 Tidy data\nData that is in a format that is easy to work with is often referred to as ‚Äútidy data‚Äù. Tidy data is a concept that was introduced by Hadley Wickham in his paper Tidy Data (Wickham 2014). The paper is available online at http://vita.had.co.nz/papers/tidy-data.pdf.\nTidy data:\n\nEach variable should be in one column.\nEach different observation of that variable should be in a different row.\nThere should be one table for each ‚Äúkind‚Äù of data.\nIf you have multiple tables, they should include a column in the table that allows them to be linked.\n\nThese concepts have been around for a long time and underlie formats enforced in several statistical packages such as SPSS, minitab and SAS.\nRecognising the structure of your data and organising it in tidy format is one of the most important step in data analysis.\n\n\n\n\nWickham, Hadley. 2014. ‚ÄúTidy Data.‚Äù Journal of Statistical Software, Articles 59 (10): 1‚Äì23. https://vita.had.co.nz/papers/tidy-data.pdf.\n\n\n‚Äî‚Äî‚Äî. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D‚ÄôAgostino McGowan, Romain Fran√ßois, Garrett Grolemund, et al. 2019. ‚ÄúWelcome to the Tidyverse‚Äù 4: 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, and Jennifer Bryan. 2023. ‚ÄúReadxl: Read Excel Files.‚Äù https://CRAN.R-project.org/package=readxl.\n\n\nWickham, Hadley, Romain Fran√ßois, Lionel Henry, Kirill M√ºller, and Davis Vaughan. 2023. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>From importing to reporting</span>"
    ]
  },
  {
    "objectID": "import_to_report.html#footnotes",
    "href": "import_to_report.html#footnotes",
    "title": "\n8¬† From importing to reporting\n",
    "section": "",
    "text": "Plain text files can be opened in notepad or other similar editor and still be readable.‚Ü©Ô∏é\nThere is also and option to import the dataset. Do not¬†be tempted to import data this way! Unless you are careful and know what you are doing, your data import will not be scripted or will not be scripted correctly.‚Ü©Ô∏é",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>From importing to reporting</span>"
    ]
  },
  {
    "objectID": "statistical_analysis_1.html",
    "href": "statistical_analysis_1.html",
    "title": "Statistical Analysis - Part 1",
    "section": "",
    "text": "What this section covers\nThis section introduces the fundamentals of Statistical Inference, the process of drawing conclusions about a population‚Äôs characteristics based on sample data. In this first course, we focus on the frequentist (or classical) approach to statistical inference, which is the most commonly taught method in introductory statistics courses.",
    "crumbs": [
      "Statistical Analysis - Part 1"
    ]
  },
  {
    "objectID": "statistical_analysis_1.html#what-this-section-covers",
    "href": "statistical_analysis_1.html#what-this-section-covers",
    "title": "Statistical Analysis - Part 1",
    "section": "",
    "text": "Why do we need statistical inference?\nIn biosciences - and many other fields - we need statistical inference because it is usually impossible to study and measure entire populations. Instead, we need to take a random sample to draw conclusions about the population of interest. Statistical inference provides the tools to make these conclusions reliably, accounting for the variability and uncertainty inherent in sampling.\nWhat tools will we cover?\nWe begin by explaining the logic of hypothesis testing and its role in making statistical inferences. Following this, we explore confidence intervals and provide an introduction to statistical models, with a particular focus on linear models.\nThe remaining chapters delve into regression, t-tests, and ANOVA, which are special cases of a broader statistical framework known as the General Linear Model (GLM). While t-tests and ANOVA are often taught using the t.test() and aov() functions in R, respectively, this book approaches them using the lm() function. This emphasizes their shared foundation within the GLM framework and helps you understand that regression, t-tests, and ANOVA are all variations of the same underlying model.\nBy learning these concepts through the lm() function, you will become familiar with the terminology and language of statistical modelling. This approach also makes it easier to build upon your knowledge, as the output of lm() is representative of the structure and results of statistical modelling functions in R more broadly.\nThe GLM framework makes some assumptions about the data to calculate probabilities. These are known as parametric assumptions and the GLM is a parametric test. As we apply each test we will check the assumptions are met. You learn how to apply non-paramtric tests when the parametric assumptions are not met. These are applied with wilcox.test() and kruskal.test()",
    "crumbs": [
      "Statistical Analysis - Part 1"
    ]
  },
  {
    "objectID": "logic_hyopthesis_testing.html",
    "href": "logic_hyopthesis_testing.html",
    "title": "\n9¬† The logic of hyothesis testing\n",
    "section": "",
    "text": "9.1 What is Hypothesis testing?\nHypothesis testing is a statistical method that helps us draw conclusions about a population based on a sample. Since we usually can‚Äôt measure every individual in a population, we take a smaller sample and analyze it.\nFor example, suppose we want to know if babies born to mothers in poverty have lower birth weights than the national average. Measuring the birth weight of every baby in the country would be impossible, so we take a sample. However, even if poverty has no real effect, our sample‚Äôs average birth weight might be different from the national average just by chance. Hypothesis testing helps us determine whether this difference is real or just random variation.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>The logic of hyothesis testing</span>"
    ]
  },
  {
    "objectID": "logic_hyopthesis_testing.html#samples-and-populations",
    "href": "logic_hyopthesis_testing.html#samples-and-populations",
    "title": "\n9¬† The logic of hyothesis testing\n",
    "section": "\n9.2 Samples and populations",
    "text": "9.2 Samples and populations\nBefore we dive deeper into hypothesis testing, let‚Äôs clarify two key terms:\n\nA population is the entire group we are interested in studying (e.g., all babies in a country).\nA sample is a smaller group selected from the population (e.g., a few hundred babies chosen for the study).\n\nWe use samples because studying an entire population is often impractical. The challenge is making sure our sample accurately represents the population.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>The logic of hyothesis testing</span>"
    ]
  },
  {
    "objectID": "logic_hyopthesis_testing.html#logic-of-hypothesis-testing",
    "href": "logic_hyopthesis_testing.html#logic-of-hypothesis-testing",
    "title": "\n9¬† The logic of hyothesis testing\n",
    "section": "\n9.3 Logic of hypothesis testing",
    "text": "9.3 Logic of hypothesis testing\nThe logic behind hypothesis testing follows these general steps:\n\nFormulating a ‚ÄúNull Hypothesis‚Äù denoted \\(H_0\\). The null hypothesis is what we expect to happen if nothing interesting is happening. It states that there is no difference between groups or no relationship between variables. In contrast, the ‚ÄúAlternative Hypothesis‚Äù (\\(H_1\\)) states that there is a significant difference between groups or a relationship between variables.\nDesigning an experiment and/or collecting data to test the null hypothesis.\nFinding the probability (the p-value) of getting our experimental data, or data more extreme, if \\(H_0\\) is true.\nDeciding whether to reject or not reject the \\(H_0\\) based on that probability:\n\nIf \\(p ‚â§ 0.05\\) we reject \\(H_0\\)\n\nIf \\(p &gt; 0.05\\) do not reject \\(H_0\\)\n\n\n\n\nIf the null hypothesis is rejected it means we have evidence that \\(H_0\\) is untrue and support for \\(H_1\\). If the null hypothesis is not rejected, it means there is insufficient evidence to support the alternative hypothesis. It is important to recognise that not rejecting the null hypothesis does not mean \\(H_0\\) is definitely true. It just means just that \\(H_0\\) cannot be discounted.\nThere is a real state to \\(H_0\\), that is, \\(H_0\\) is either true or it is not true. The statistical test has us decide whether to reject or not reject \\(H_0\\) based on the p-value. This means we can make mistakes when testing a hypothesis. These are called Type I and Type II errors.\n\n9.3.1 Type I and type II errors\nType I and type II errors describe the cases when we make the wrong decision about the null hypothesis. These errors are inherent in the approach rather than mistakes you can prevent.\n\nA type I error occurs when we reject a null hypothesis that is true. This can be thought of as a false positive. It is a real error in that we have a real difference or effect. Since we use a probability of 0.05 to reject the null hypothesis, we will make a type I error 5% of the time.\nA type II error occurs when we do not reject a null hypothesis that is false. This is a false negative. It is not a real error in the sense that we only conclude we do not have enough evidence to reject the null hypothesis.\nIf we reject a null hypothesis that is false we have not made an error.\nIf we do not reject a null hypothesis that is true we have not made an error.\n\n\n\nType I and Type II errors.\n\nWe can decrease our chance of making a type I error by reducing the the p-value required to reject the null hypothesis. However, this will increase our chance of making a type II error. We can decrease our chance of making a type II error by collecting enough data. The amount of data needed will depend on the the size of the effect relative to the random variation in the data.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>The logic of hyothesis testing</span>"
    ]
  },
  {
    "objectID": "logic_hyopthesis_testing.html#sampling-distribution-of-the-mean",
    "href": "logic_hyopthesis_testing.html#sampling-distribution-of-the-mean",
    "title": "\n9¬† The logic of hyothesis testing\n",
    "section": "\n9.4 Sampling distribution of the mean",
    "text": "9.4 Sampling distribution of the mean\nThe sampling distribution of the mean is a fundamental concept in hypothesis testing and constructing confidence intervals. Parametric tests such as regression, two-sample tests and ANOVA (all applied with lm()) are based on the sampling distribution of the mean. It is a theoretical distribution that describes the distribution of the sample means if an infinite number of samples were taken.\nThe key characteristics of the sampling distribution of the mean are:\n\nThe mean of the sampling distribution of the mean is equal to the population mean\nThe standard deviation of the sampling distribution of the mean is known the standard error of the mean and is always smaller than the standard deviation of the values. There is a fixed relationship between the standard deviation of a sample or population and the standard error of the mean: \\(s.e. = \\frac{s.d.}{\\sqrt{n}}\\)\n\nüí° Why does this matter? When we calculate a p-value, we‚Äôre really asking: ‚ÄúHow likely is it to get a sample mean like ours (or more extreme) if \\(H_0\\) is true. This is why understanding the sampling distribution is so important - it helps us determine what results we should expect by random chance.\n\n\n\n\n\n\n\nFigure¬†9.1: The sampling distribution of the means is a theoretical distribution that describes the distribution of the sample means if an infinite number of samples of size n were taken. The sampling distribution of the mean has a standard devation which is smaller than the population and is called the standard error.\n\n\n\n\n\n9.4.1 Example\nLet‚Äôs work through this logic using an example.\nQuestion: National average birth weight is 3300 grams with an s.d. = 900 grams. Does maternal poverty influence birth weight?\n\nSet up the null hypothesis. The null hypothesis is what we expect to happen if nothing interesting is happening. In this case, that there is no effect of maternal poverty on birth weight, i.e., the mean of a sample of babies born into poverty is equal to the national average (Figure¬†9.2). This is written as \\(H_0: \\bar{x} = 3300\\). The alternative hypothesis is that the sample mean is not equal to the national average. This is written as \\(H_1: \\bar{x} \\neq 3300\\).\n\n\n\n\n\n\n\n\nFigure¬†9.2: Distribution of the population of birth weights has a mean of 3300 g and a standard deviation of 900. The null hypothesis is that the mean birth weight of babies born to women in poverty is the same as the national average of 3300 g.\n\n\n\n\n\nDesign an experiment that generates data to test the null hypothesis1. We take a sample of \\(n = 12\\) women who live in poverty and determine the mean birth weight of their babies. We calculate \\(\\bar{x} = 3000 g\\). This is lower than the national average but might we get a sample like that even if the null hypothesis is true?\n\n\nDetermine the probability (the p-value) of getting our experimental data, or more extreme data, if \\(H_0\\) is true.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†9.3: The probability of getting a sample mean of 3000 or less is the area under the sampling distribution of the mean to the left of 3000.\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†9.4: The probability of getting a sample mean of 3000 or more extreme is the area under the sampling distribution of the mean to the left of 3000 plus that to the right of 3600. This is because 3600 is as extreme (as far away from the mean) as 3000.\n\n\n\n\n\nDecide whether to reject or not reject the \\(H_0\\) based on that probability. If the shaded area is less than 0.05 we reject the null hypothesis and conclude there is a difference in the birth weights between the two groups. If the shaded area is more than 0.05 we do not reject the null hypothesis.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>The logic of hyothesis testing</span>"
    ]
  },
  {
    "objectID": "logic_hyopthesis_testing.html#footnotes",
    "href": "logic_hyopthesis_testing.html#footnotes",
    "title": "\n9¬† The logic of hyothesis testing\n",
    "section": "",
    "text": "In fact this is an observational study rather than a real experimental study. This means we can potential conclude birth weight differs in the two groups but we cannot say maternal poverty causes that difference.‚Ü©Ô∏é",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>The logic of hyothesis testing</span>"
    ]
  },
  {
    "objectID": "confidence_intervals.html",
    "href": "confidence_intervals.html",
    "title": "\n10¬† Confidence Intervals\n",
    "section": "",
    "text": "10.1 What is a confidence interval?\nWhen we calculate a mean from a sample, we are using it to estimate the mean of the population. Confidence intervals are a range of values and are a way to quantify the uncertainty in our estimate. When we report a mean with its 95% confidence interval we give the mean plus and minus some variation. We are saying that 95% of the time, that range will contain the population mean.\nThe confidence interval is calculated from the sample mean and the standard error of the mean. The standard error of the mean is the standard deviation of the sampling distribution of the mean.\nTo understand confidence intervals we need to understand some properties of the normal distribution.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "confidence_intervals.html#the-normal-distribution",
    "href": "confidence_intervals.html#the-normal-distribution",
    "title": "\n10¬† Confidence Intervals\n",
    "section": "\n10.2 The normal distribution",
    "text": "10.2 The normal distribution\nA distribution describes the values the variable can take and the chance of them occurring. A distribution has a general type, given by the function, and is further tuned by the parameters in the function. For the normal distribution these parameters are the mean and the standard deviation. Every variable that follows the normal distribution has the same bell shaped curve and the distributions differ only in their means and/or standard deviations. The mean determines where the centre of the distribution is, the standard deviation determines the spread (Figure¬†10.1).\n\n\n\n\n\n\n\nFigure¬†10.1: The mean determines where the centre of the distribution is, the standard deviation determines the spread. The distributions on the left have the same mean but different standard deviations. The distributions on the right have the same standard deviation but different means.\n\n\n\n\nWhilst normal distributions vary in the location on the horizontal axis and their width, they all share some properties and it is these shared properties that allow the calculation of confidence intervals with some standard formulae. The properties are that a fix percentage of values lie between a given number of standard deviations. For example, 68.2% values lie between plus and minus one standard deviation from the mean and 95% values lie between \\(\\pm\\) 1.96 standard deviations. Another way of saying this is that there is a 95% chance that a randomly selected value will lie between \\(\\pm\\) 1.96 standard deviations from the mean. This is illustrated in Figure¬†10.2.\n\n\n\n\n\n\n\nFigure¬†10.2: Normal distributions share some properties regardless of the mean and standard deviation. 68% of the values are within 1 standard deviation of the mean and 95% are within 1.96 standard deviations.\n\n\n\n\nR has some useful functions associated with distributions, including the normal distribution.\n\n10.2.1 Distributions: the R functions\nFor any distribution, R has four functions:\n\nthe density function, which gives the height of the function at a given value.\nthe distribution function, which gives the probability that a variable takes a particular value or less.\nthe quantile function which is the inverse of the Distribution function, i.e., it returns the value (‚Äòquantile‚Äô) for a given probability.\nthe random number generating function\n\nThe functions are named with a letter d, p, q or r preceding the distribution name. Table¬†10.1 shows these four functions for the normal, binomial, Poisson and t distributions.\n\n\nTable¬†10.1: R functions that provide values for some example distributions\n\n\n\n\n\n\n\n\n\n\nDistribution\nDensity\nDistribution\nQuantile\nRandom number generating\n\n\n\nNormal\ndnorm()\npnorm()\nqnorm()\nrnorm()\n\n\nBinomial\ndbinom()\npbinom()\nqbinom()\nrbinom()\n\n\nPoisson\ndpois()\nppois()\nqpois()\nrpois()\n\n\nt\ndt()\npt()\nqt()\nrt()\n\n\n\n\n\n\nSearching for the manual with ?normal or any one of the functions (?pnorm) will bring up a single help page for all four associated functions.\n\n\n\n\n\n\nFigure¬†10.3: The R Manual Page for the normal distribution which shows the four functions associated functions.\n\n\nThe functions which are of most use to us are pnorm() and qnorm() and these are illustrated in Figure¬†10.4.\n\n\n\n\n\n\n\nFigure¬†10.4: The pnorm() function calculates the probability that a value is less than or equal to a given value.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "confidence_intervals.html#confidence-intervals-on-large-samples",
    "href": "confidence_intervals.html#confidence-intervals-on-large-samples",
    "title": "\n10¬† Confidence Intervals\n",
    "section": "\n10.3 Confidence intervals on large samples",
    "text": "10.3 Confidence intervals on large samples\n\\[\n\\bar{x} \\pm 1.96 \\times s.e.\n\\tag{10.1}\\]\n95% of confidence intervals calculated in this way will contain the true population mean.\nDo you have to remember the value of 1.96? Not if you have R!\n\nqnorm(0.975)\n## [1] 1.959964\n\nNotice that it is qnorm(0.975) and not qnorm(0.95) for a 95% confidence interval. This is because the functions are defined as giving the area under to the curve to the left of the value given. If we gave 0.95, we would get the value that put 0.05 in one tail. We want 0.025 in each tail, so we need to use 0.975 in qnorm().\nTO-DO pic",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "confidence_intervals.html#confidence-intervals-on-small-samples",
    "href": "confidence_intervals.html#confidence-intervals-on-small-samples",
    "title": "\n10¬† Confidence Intervals\n",
    "section": "\n10.4 Confidence intervals on small samples",
    "text": "10.4 Confidence intervals on small samples\nThe calculation of confidence intervals on small samples is very similar but we use the t-distribution rather than the normal distribution. The formula is:\n\\[\n\\bar{x} \\pm t_{[d.f.]} \\times s.e.\n\\tag{10.2}\\]\nThe t-distibution is a modified version of the normal distribution and we use it because the sampling distribution of the mean is not quite normal when the sample size is small. The t-distribution has an additional parameter called the degrees of freedom which is the sample size minus one (\\(n -1\\)). Like the normal distribution, the t-distribution has a mean of zero and is symmetrical. However, The t-distribution has fatter tails than the normal distribution and this means that the probability of getting a value in the tails is higher than for the normal distribution. The degrees of freedom determine how much fatter the tails are. The smaller the sample size, the fatter the tails. As the sample size increases, the t-distribution becomes more and more like the normal distribution.\n\n10.4.1 What are degrees of freedom?\nDegrees of freedom (usually abbreivated d.f.) describe how much independent information is available to estimate variability when calculating probabilities. In a population, all values are known, so no parameters need estimating. In a sample, we estimate parameters such as the mean. In estimating a mean, only \\(n - 1\\) values can vary independently because one value is constrained by the sample mean. In hypothesis tests and confidence intervals, d.f. adjust for uncertainty when generalizing from a sample to a population, affecting probability distributions like the t-distribution. More d.f. mean better probability estimates, while fewer d.f. reflect greater uncertainty in estimating population parameters.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "confidence_intervals.html#your-turn",
    "href": "confidence_intervals.html#your-turn",
    "title": "\n10¬† Confidence Intervals\n",
    "section": "\n10.5 üé¨ Your turn!",
    "text": "10.5 üé¨ Your turn!\nIf you want to code along you will need to start a new RStudio project, add a data-raw folder and open a new script. You will also need to load the tidyverse package (Wickham et al. 2019).",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "confidence_intervals.html#large-samples",
    "href": "confidence_intervals.html#large-samples",
    "title": "\n10¬† Confidence Intervals\n",
    "section": "\n10.6 Large samples",
    "text": "10.6 Large samples\nA team of biomedical researchers is studying the concentration of Creatine Kinase (CK) in the blood of patients with muscle disorders. They collect a large random sample (n = 100) of blood samples from patients and measure the CK concentration in UL-1 (units per litre).\nThe goal is to estimate the average CK concentration in this patient population and calculate a 95% confidence interval for the mean concentration. The data are in ck_concen.csv\n\n10.6.1 Import\n\ncreatinekinase &lt;- read_csv(\"data-raw/ck_concen.csv\")\n\n\n10.6.2 Calculate sample statistics\nCalculate the mean, standard deviation, sample size and standard error and assign as variables:\n\n# mean\nm &lt;- mean(creatinekinase$ck_conc)\n\n# standard deviation\nsd &lt;- sd(creatinekinase$ck_conc)\n\n# sample size (needed for the se)\nn &lt;- length(creatinekinase$ck_conc)\n\n# standard error\nse &lt;- sd / sqrt(n)\n\n\n10.6.3 Calcuate C.I.\n\nTo calculate the 95% confidence interval we need to look up the quantile (multiplier) using qnorm():\n\nq &lt;- qnorm(0.975)\n\nNow we can use it in our confidence interval calculation:\n\nlcl &lt;- m - q * se\nucl &lt;- m + q * se\n\nI used the names lcl and ucl to stand for ‚Äúlower confidence limit‚Äù and ‚Äúupper confidence limit‚Äù respectively.\nPrint the values:\n\nlcl\n## [1] 284.3891\nucl\n## [1] 319.9483\n\nThis means we are 95% confident the population mean lies between 284.39 UL-1 and 319.95 UL-1. The amount we have added/subtracted from the mean (q * se) is 17.78 thus we sometimes see this written as 302.17 \\(\\pm\\) 17.78 mm.\n\n10.6.4 Report\nThe mean creatine kinase concentration in patients with muscle disorders is 302.17 UL-1, 95% C.I. [284.39, 319.95]",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "confidence_intervals.html#small-samples",
    "href": "confidence_intervals.html#small-samples",
    "title": "\n10¬† Confidence Intervals\n",
    "section": "\n10.7 Small samples",
    "text": "10.7 Small samples",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "confidence_intervals.html#summary",
    "href": "confidence_intervals.html#summary",
    "title": "\n10¬† Confidence Intervals\n",
    "section": "\n10.8 Summary",
    "text": "10.8 Summary",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "confidence_intervals.html#summary-of-confidence-intervals",
    "href": "confidence_intervals.html#summary-of-confidence-intervals",
    "title": "\n10¬† Confidence Intervals\n",
    "section": "\n10.9 Summary of Confidence Intervals",
    "text": "10.9 Summary of Confidence Intervals\n\nA confidence interval gives a range of plausible values for a population mean from a sample and is calculated from a sample mean and standard error.\nThey are possible because all normal distributions have the same properties.\n\n95% Confidence Intervals for Large Samples:\n\nFormula:\\[\n\\bar{x} \\pm 1.96 \\times s.e.\n\\]\n\n95% of confidence intervals computed this way will contain the true population mean.\n\n\n\n95% Confidence Intervals for Small Samples:\n\nUses the t-distribution instead of the normal distribution.\n\nFormula:\\[\n\\bar{x} \\pm t_{[d.f.]} \\times s.e.\n\\]\n\nThe t-distribution has fatter tails and varies based on degrees of freedom (n-1), approaching the normal distribution as n increases.\n\n\n\n\n\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D‚ÄôAgostino McGowan, Romain Fran√ßois, Garrett Grolemund, et al. 2019. ‚ÄúWelcome to the Tidyverse‚Äù 4: 1686. https://doi.org/10.21105/joss.01686.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "what_statistical_model.html",
    "href": "what_statistical_model.html",
    "title": "\n11¬† What is a statistical model?\n",
    "section": "",
    "text": "11.1 Overview\nThis section discusses statistical models which are equations representing relationships between variables. Statistical models help us test hypotheses and make predictions. The process involves estimating model ‚Äúparameters‚Äù from data and assessing ‚Äúmodel fit‚Äù. Linear models include regression, t-tests, and ANOVA, known collectively as the General Linear Model. The assumptions of the general linear model are that the ‚Äúresiduals‚Äù are normally distribution and variance is homogeneous. If the assumptions are violated we can use non-parametric tests.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>What is a statistical model?</span>"
    ]
  },
  {
    "objectID": "what_statistical_model.html#what-is-a-statistical-model",
    "href": "what_statistical_model.html#what-is-a-statistical-model",
    "title": "\n11¬† What is a statistical model?\n",
    "section": "\n11.2 What is a statistical model?",
    "text": "11.2 What is a statistical model?\nA statistical model is a mathematical equation that helps us understand the relationships between variables. We evaluate how well our data fit a particular model so we can infer something about how the values arose or make predictions about future values.\nThe equation states what has to be done to the explanatory values to get the response value. For example, a simple model of plant growth might be that a plant grows by 2 cm a day week after it is two weeks old. This model would be written as:\n\\[\nh_{(t)} = h_{(0) }+ 2t\n\\tag{11.1}\\]\nWhere:\n\n\n\\(t\\) is the time in days after the plant is two weeks old\n\n\\(h_{(t)}\\) is the height of the plant at time \\(t\\)\n\n\n\\(h_{(0)}\\) is the height of the plant at \\(t=0\\), i.e., at two weeks\n\nThis model is a linear model because the relationship between the response variable, height, and the explanatory variable, time, is linear (See Figure¬†11.1 (a)). In a linear model, the gradient of the line is the same no matter what the value of \\(t\\). In this case, it is fixed at 2 cm per day.\nOne alternative is a simple exponential model. In an exponential model, the height might increase by 12% each day and the gradient of the line would increase over time. This model is written as:\n\\[\nh_{(t)} = h_{(0)}1.2^t\n\\]\nWhere:\n\n\n\\(t\\) is the time in days after the plant is two weeks old\n\n\\(h_{(t)}\\) is the height of the plant at time \\(t\\)\n\n\n\\(h_{(0)}\\) is the height of the plant at \\(t=0\\), i.e., at two weeks\n\nThis model is not a straight line (See Figure¬†11.1 (b)). The gradient of the line increase as time goes on.\n\n\n\n\n\n\n\n\n\n(a) Linear\n\n\n\n\n\n\n\n\n\n(b) Expontential\n\n\n\n\n\n\nFigure¬†11.1: Two possible models of plant growth.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>What is a statistical model?</span>"
    ]
  },
  {
    "objectID": "what_statistical_model.html#statistical-models-and-hypothesis-testing",
    "href": "what_statistical_model.html#statistical-models-and-hypothesis-testing",
    "title": "\n11¬† What is a statistical model?\n",
    "section": "\n11.3 Statistical Models and Hypothesis Testing",
    "text": "11.3 Statistical Models and Hypothesis Testing\nWhen we conduct statistical analysis, we are using a statistical model ‚Äîa mathematical framework that describes the relationship between explanatory and response variables. Every model makes assumptions about this relationship, and we estimate the parameters of the model from the data. For example, in a simple linear model of plant growth, the key parameters are the intercept and the slope.\nStatistical hypothesis testing is directly tied to these models. When we test a hypothesis, we are often assessing whether a particular model parameter is significantly different from zero. This is equivalent to testing whether an explanatory variable has an effect on the response variable.\nIn the context of hypothesis testing, the null hypothesis (\\(H_0\\)) typically states that a given parameter is equal to zero, meaning there is no effect or no relationship. The alternative hypothesis (\\(H_1\\)) suggests that the parameter is different from zero, indicating an effect.\nTo evaluate this, we calculate the probability of obtaining our estimated parameter value, or a more extreme value, if the true parameter were actually zero (i.e., if \\(H_0\\) were true). This probability is the p-value.\n\n11.3.1 Assumptions and Model Fit\nBecause hypothesis tests depend on probability calculations, they also depend on certain assumptions about the data. A key assumption is that the estimated parameter follows a normal distribution, which allows us to use standard probability models to determine significance. This assumption is reasonable in many cases, but if it is violated, the p-values may be misleading.\nFor this reason, checking model assumptions is crucial. If the assumptions are not met, we might need to transform the data, choose a different model, or use non-parametric tests, which make fewer assumptions about data distribution. Non-parametric methods do not estimate parameters like an intercept and slope, which makes them useful when standard parametric assumptions are questionable.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>What is a statistical model?</span>"
    ]
  },
  {
    "objectID": "what_statistical_model.html#using-a-linear-model-in-practice.",
    "href": "what_statistical_model.html#using-a-linear-model-in-practice.",
    "title": "\n11¬† What is a statistical model?\n",
    "section": "\n11.4 Using a linear model in practice.",
    "text": "11.4 Using a linear model in practice.\nImagine we are studying a population of bacteria and want understand how nutrient availability influences its growth. We could grow the bacteria with different levels of nutrients and measure the diameter of bacterial colonies on agar plates in a controlled environment so that everything except the nutrient availability was identical. We could then plot the diameters against the nutrient levels.\nWe might expect the relationship between nutrient level and growth to be linear and add a line of best fit. See Figure¬†11.2.\n\n\n\n\n\n\n\n\n\n(a) Data without a model.\n\n\n\n\n\n\n\n\n\n(b) Data with a line of best fit\n\n\n\n\n\n\nFigure¬†11.2: The effect of nutrient level on bacterial colony diamters without (1) and with (2) a linear model.\n\n\nThe equation of this line is a statistical model that allows us to make predictions about colony diameter from nutrient levels. A line - or linear model - has the form:\n\\[\ny = \\beta_{0} + \\beta_{1}x\n\\tag{11.2}\\]\nWhere:\n\n\n\\(y\\) is the response variable and \\(x\\) is the explanatory variable.\n\n\\(\\beta_{0}\\) is the value of \\(y\\) when \\(x = 0\\) usually known as the intercept\n\n\\(\\beta_{1}\\) is the amount added to \\(y\\) for each unit increase in \\(x\\) usually known as the slope\n\n\\(\\beta_{0}\\) and \\(\\beta_{1}\\) are called the coefficients - or parameters - of the model.\nIn this case \\[\nDiameter = \\beta_{0} + \\beta_{1}Nutrient\n\\tag{11.3}\\]\nLinear models are amongst the most commonly used statistics. Regression, t-tests and ANOVA are all linear models collectively known as the General Linear Model.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>What is a statistical model?</span>"
    ]
  },
  {
    "objectID": "what_statistical_model.html#model-fitting",
    "href": "what_statistical_model.html#model-fitting",
    "title": "\n11¬† What is a statistical model?\n",
    "section": "\n11.5 Model fitting",
    "text": "11.5 Model fitting\nThe process of estimating the parameters \\(\\beta_{0}\\) and \\(\\beta_{1}\\) from data is known as fitting a linear model. The line gives the predicted values of \\(y\\). The actual measured value of \\(y\\) will differ from the predicted value and this difference is called a residual or an error. The line is a best fit in the sense that \\(\\beta_{0}\\) and \\(\\beta_{1}\\) minimise the sum of the squared residuals, \\(SSE\\).\n\\[\nSSE = \\sum(y_{i}-\\hat{y})^2\n\\tag{11.4}\\]\nWhere:\n\n\n\\(y_{i}\\) represents each of the measured \\(y\\) values from the 1st to the ith\n\n\\(\\hat{y}\\) is predicted value\n\nSince \\(\\beta_{0}\\) and \\(\\beta_{1}\\) are those that minimise the \\(SSE\\), they are described as least squares estimates. You do not need to worry about this too much but it is a useful piece of statistical jargon to have heard of because it pops up often. The mean of a sample is also a least squares estimate - the sum of the squared differences between each value and the mean is smaller than the sum of the squared differences between each value and any other value.\nThe role played by \\(SSE\\) in estimating our parameters means that it is also used in determining how well our model fits our data. Our model can be considered useful if the difference between the actual measured value of \\(y\\) and the predicted value is small but \\(SSE\\) will also depend on the size of \\(y\\) and the sample size. This means we express \\(SSE\\) as a proportion of the total variation in \\(y\\). The total variation in \\(y\\) is denoted \\(SST\\):\n\\[\n\\frac{SSE}{SST}\n\\tag{11.5}\\]\n\\(\\frac{SSE}{SST}\\) is called the residual variation. It is the proportion of variance remaining after the model fitting. In contrast, the proportion of the total variance that is explained by the model is called R-squared, \\(R^2\\). It is:\n\\[\nR^2=1-\\frac{SSE}{SST}\n\\tag{11.6}\\]\nIf there were no explanatory variables, the value we would predict for the response variable is its mean. In other words, if you did not know the nutrient level for a randomly chosen bacterial colony the best guess you could make for its eventual diameter is the mean diameter. Thus, a good model should fit the response better than the mean - that is, a good model should fit the response better than a best guess. The output of lm() includes the \\(R^2\\). It represents the proportional improvement in the predictions from the regression model relative to the mean model. It ranges from zero, the model is no better than the mean, to 1, the predictions are perfect. See Figure¬†11.3\n\n\n\n\n\nFigure¬†11.3: A linear model with different fits. A) the model is a poor fit - the explanatory variable is no better than the response mean for predicting the response. B) the model is good fit - the explanatory variable explains a high proportion of the variance in the response. C) the model is a perfect fit - the response can be predicted perfectly from the explanatory variable. Measured response values are in pink, the predictions are in green and the dashed blue line gives the mean of the response.\n\n\nSince the distribution of the responses for a given \\(x\\) is assumed to be normal and the variances of those distributions are assumed to be homogeneous, both are also true of the residuals. It is our examination of the residuals which allows us to evaluate whether the assumptions are met.\nSee Figure¬†11.4 for a graphical representation of linear modelling terms introduced so far.\n\n\n\n\n\nFigure¬†11.4: A linear model annotated with the terms used in modelling. Measured response values are in pink, the predictions are in green, and the differences between these, known as the residuals, are in blue. The estimated model parameters, \\(\\beta_{0}\\) (the intercept) and \\(\\beta_{1}\\) (the slope) are indicated.\n\n\n\n11.5.1 General linear model assumptions\nThe assumptions of the general linear model are that the residuals are normally distributed and have homogeneity of variance. A residual is the difference between the predicted and observed value\nIf we have a continuous response and a categorical explanatory variable with two groups, we usually apply the general linear model with lm() and then check the assumptions, however, we can sometimes tell when a non-parametric test would be more appropriate before that:\n\nUse common sense - the response should be continuous (or nearly continuous, see Ideas about data: Theory and practice). Consider whether you would expect the response to be continuous\nThere should decimal places and few repeated values.\n\nTo examine the assumptions after fitting the linear model, we plot the residuals and test them against the normal distribution",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>What is a statistical model?</span>"
    ]
  },
  {
    "objectID": "what_statistical_model.html#choice-of-model",
    "href": "what_statistical_model.html#choice-of-model",
    "title": "\n11¬† What is a statistical model?\n",
    "section": "\n11.6 Choice of model",
    "text": "11.6 Choice of model\nbefore: appropriate to the question, type of relationship. assumptions about the type of model\nand after assumption calculations the probability calculations\nimplications of wrong choices: doesn‚Äôt anwser the question p value is inaccurate conclusions are wrong",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>What is a statistical model?</span>"
    ]
  },
  {
    "objectID": "what_statistical_model.html#general-linear-models-in-r",
    "href": "what_statistical_model.html#general-linear-models-in-r",
    "title": "\n11¬† What is a statistical model?\n",
    "section": "\n11.7 General linear models in R",
    "text": "11.7 General linear models in R\nWe use the lm() function in R to analyse data with the general linear model. When you have one explanatory variable the command is:\n lm(data = dataframe, response ~ explanatory) \nThe response ~ explanatory part is known as the model formula. These must be the names of two column in the dataframe.\nWhen you have two explanatory variable we add the second explanatory variable to the formula using a + or a *. The command is:\n lm(data = dataframe, response ~ explanatory1 + explanatory2) \nor\n lm(data = dataframe, response ~ explanatory1 * explanatory2) \nA model with explanatory1 + explanatory2 considers the effects of the two variables independently. A model with explanatory1 * explanatory2 considers the effects of the two variables and any interaction between them. You will learn more about independent effects and interactions in Two-way ANOVA\nWe usually assign the output of an lm() command to an object and view it with summary(). The typical workflow would be:\n mod &lt;- lm(data = dataframe, response ~ explanatory)\nsummary(mod) \nThere are two sorts of statistical tests in the output of summary(mod):\n\ntests of whether each coefficient is significantly different from zero and,\nan F-test of the model fit overall\n\nThe F-test in the last line of the output indicates whether the relationship modelled between the response and the set of explanatory variables is statistically significant. i.e., whether it explains a significant amount of variation.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>What is a statistical model?</span>"
    ]
  },
  {
    "objectID": "what_statistical_model.html#checking-assumptions",
    "href": "what_statistical_model.html#checking-assumptions",
    "title": "\n11¬† What is a statistical model?\n",
    "section": "\n11.8 Checking assumptions",
    "text": "11.8 Checking assumptions\nThe assumptions relate to the type of relationship chosen and the hypothesis testing about the parameters. For a general linear model we assume the relationship between diameter and nutrients is linear and we examine this by plotting our data before running any tests.\nThe assumptions of the hypothesis testing in a general linear model are that residuals are normally distributed and have homogeneity of variance. A residual is the difference between the predicted and observed value. We usually check these assumptions after fitting the linear model by using the plot() function. This produces diagnostic plots to explore the distribution of the residuals. These cannot prove the assumptions are met but allow us to quickly determine if the assumptions are plausible, and if not, how the assumptions are violated and what data points contribute to the violation.\nThe two diagnostic plots which are most useful are the ‚ÄúQ-Q‚Äù plot (plot 2) and the ‚ÄúResiduals vs Fitted‚Äù plot (plot 1). These are given as values to the which argument of plot().\n\n11.8.1 The Q-Q plot\nThe Q-Q plot is a scatterplot of the residuals (standardised to a mean of zero and a standard deviation of 1) against what is expected if the residuals are normally distributed.\n\nplot(mod, which = 2)\n\n\n\n\n\n\n\nThe points should fall roughly on the line if the residuals are normally distributed. In the example above, the residuals appear normally distributed.\nThe following are two examples in which the residuals are not normally distributed.\n\n\n\n\n\n\n\n\nIf you see patterns like these you should find an alternative to a general linear model such as a non-parametric test or a generalised linear model. Sometimes, applying a transformation to the response variable will result in better meeting the assumptions.\n\n11.8.2 The Residuals vs Fitted plot\n\n\n\n\n\n\n\n\nThe Residuals vs Fitted plot shows if residuals have homogeneous variance or non-linear patterns. Non-linear relationships between explanatory variables and the response will usually show in this plot if the model does not capture the non-linear relationship. For the assumptions to be met, the residuals should be equally spread around a horizontal line as they are here:\n\nplot(mod, which = 1)\n\n\n\n\n\n\n\nThe following are two examples in which the residuals do not have homogeneous variance and display non-linear patterns.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>What is a statistical model?</span>"
    ]
  },
  {
    "objectID": "what_statistical_model.html#reporting",
    "href": "what_statistical_model.html#reporting",
    "title": "\n11¬† What is a statistical model?\n",
    "section": "\n11.9 Reporting",
    "text": "11.9 Reporting\nWhen reporting the results of statistical tests we need to make sure we tell the reader everything they need to know and give the evidence it to support it. What they need to know is given in statements describing what difference or effect is significant and the evidence is from the test statistic and p-value from the test. You can think the statistical test values as being the evidence for the statements in your results sections, just as citations are the evidence for the statements in your introduction.\nIn reporting the result of a test we give:\n\nthe significance of effect\nthe direction of effect\nthe magnitude of effect\n\nFigures should demonstrate the statement. Ideally they will include all the data and the ‚Äòmodel‚Äô, i.e., the means and error bars or the fitted line. Figure legends should be concise but contain all the information needed to understand the figure. I like this blog on How to craft a figure legend for scientific papers",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>What is a statistical model?</span>"
    ]
  },
  {
    "objectID": "what_statistical_model.html#summary",
    "href": "what_statistical_model.html#summary",
    "title": "\n11¬† What is a statistical model?\n",
    "section": "\n11.10 Summary",
    "text": "11.10 Summary\n\nA statistical model is an equation that describes the relationship between a response variable and one or more explanatory variables.\nA statistical model allows you to make predictions about the response variable based on the values of the explanatory variables.\nMany statistical tests are types of ‚ÄúGeneral Linear Model‚Äù including linear regression, t-tests and ANOVA.\nStatistical testing means estimating the model ‚Äúparameters‚Äù and testing whether they are significantly different from zero. The parameters, also known as coefficients, are the intercept and slope (s) in a General Linear Model. A p-value less than 0.05 for the slope means there is a significant relationship between the response and the explanatory variable.\nWe also consider the fit of the model to the data using the R-squared value and the F-test. An R-squared value close to 1 indicates a good fit and p-value less than 0.05 for the F-test indicates the model explains a significant amount of variation.\nThe assumptions of the General Linear Model must be met for the p-values to be accurate. These are: are that the relationship between the response and the explanatory variables is linear and that the residuals are normally distributed and have homogeneity of variance. We check these assumptions by plotting the data and the residuals.\nWe use the lm() function to fit a linear model in R. The summary() function gives us the test statistic, p-values and R-squared value and the plot() function gives us diagnostic plots to check the assumptions.\nIf the assumptions are not met we apply a non-parametric test using wilcox.test or kruskal.test(). These also give us a test statistic and a p-value.\nWhen reporting the results of statistical tests give the significance, direction and magnitude of the effect and use figures to demonstrate the statement.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>What is a statistical model?</span>"
    ]
  },
  {
    "objectID": "single_linear_regression.html",
    "href": "single_linear_regression.html",
    "title": "\n12¬† Single linear regression\n",
    "section": "",
    "text": "12.1 Overview\nSingle linear regression is an appropriate way to analyse data when:\nApplying a single linear regression to data means putting a line of best fit through it. The intercept and the slope of the true population relationship is estimated from the sample you have. We test whether those two parameters differ significantly from zero.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Single linear regression</span>"
    ]
  },
  {
    "objectID": "single_linear_regression.html#overview",
    "href": "single_linear_regression.html#overview",
    "title": "\n12¬† Single linear regression\n",
    "section": "",
    "text": "You have two continuous variables\nOne of the variables is explanatory and the other is a response. That is, one variable, the \\(x\\), ‚Äúcauses‚Äù the \\(y\\).\nThe explanatory variable has been chosen, set or manipulated and the other variable is the measured response. This is sometimes described as the \\(x\\) being ‚Äúsampled without error‚Äù\nThe response variable, \\(y\\), is randomly sampled for each \\(x\\) with a normal distribution and those normal distributions have the same variance.\nThe relationship between the variables is linear\n\n\n\n12.1.1 Reporting\nReporting the significance of effect, direction of effect, magnitude of effect for a single linear regression means making the following clear to the reader:\n\nthe significance of effect - whether the slope is significantly different from zero\nthe direction of effect - whether the slope is positive or negative\nthe magnitude of effect - the slope itself\n\nFigures should reflect what you have said in the statements. Ideally they should show both the raw data and the statistical model:\nWe will explore all of these ideas with an example.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Single linear regression</span>"
    ]
  },
  {
    "objectID": "single_linear_regression.html#your-turn",
    "href": "single_linear_regression.html#your-turn",
    "title": "\n12¬† Single linear regression\n",
    "section": "\n12.2 üé¨ Your turn!",
    "text": "12.2 üé¨ Your turn!\nIf you want to code along you will need to start a new RStudio project, add a data-raw folder and open a new script. You will also need to load the tidyverse package (Wickham et al. 2019).",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Single linear regression</span>"
    ]
  },
  {
    "objectID": "single_linear_regression.html#single-linear-regression",
    "href": "single_linear_regression.html#single-linear-regression",
    "title": "\n12¬† Single linear regression\n",
    "section": "\n12.3 Single linear regression",
    "text": "12.3 Single linear regression\nThree replicates water baths were set up at each of five temperatures (10, 11C, 12C, 13C, 14C). Ten Brine Shrimp (Artemia salina) were placed in each and their average respiration rate per water bath was measured (in arbitrary units). The data are in shrimp.txt. In this scenario our null hypothesis, \\(H_0\\), is that temparature has no effect on respiration rate. Another way of saying this is the slope of the line is zero. This is written as: \\(H_0: \\beta_1 = 0\\).\n\n12.3.1 Import and explore\nImport the data:\n\nshrimp &lt;- read_table(\"data-raw/shrimp.txt\")\n\n\n\n\n\n\ntemperature\nrespiration\n\n\n\n6\n11.94\n\n\n6\n9.54\n\n\n6\n10.22\n\n\n8\n11.01\n\n\n8\n12.94\n\n\n8\n11.93\n\n\n10\n12.30\n\n\n10\n15.95\n\n\n10\n14.47\n\n\n12\n14.28\n\n\n12\n17.56\n\n\n12\n15.56\n\n\n14\n16.88\n\n\n14\n18.96\n\n\n14\n17.78\n\n\n\n\n\n\n\nThese data are in tidy format (Wickham 2014) - all the respiration values are in one column with another column indicating the water bath temperature. There is only one water bath per row. This means they are well formatted for analysis and plotting.\nIn the first instance, it is sensible to create a rough plot of our data (See Figure¬†12.1). Plotting data early helps us in multiple ways:\n\nit helps identify whether there missing or extreme values\nit allows us to see if the relationship is roughly linear\nit tells us whether any relationship positive or negative\n\nScatter plots (geom_point()) are a good choice for exploratory plotting with data like these.\n\nggplot(data = shrimp,\n       aes(x = temperature, y = respiration)) +\n  geom_point()\n\n\n\n\n\n\nFigure¬†12.1: A default scatter plot of the relationship between temperature and respiration rate in brine shrimp is enough for us to see that respiration rate seems to increase with temperature approximately linearly.\n\n\n\n\nThe figure suggests that respiration rate increases with temperature and there are no particularly extreme values. We can also see that any relationship is roughly linear.\n\n12.3.2 Do a regression with lm()\n\nWe can create a single linear regression model like this:\n\nmod &lt;- lm(data = shrimp, respiration ~ temperature)\n\nAnd examine the model with:\n\nsummary(mod)\n## \n## Call:\n## lm(formula = respiration ~ temperature, data = shrimp)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.7880 -0.8780 -0.1773  0.9393  1.8620 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   4.8613     1.1703   4.154  0.00113 ** \n## temperature   0.9227     0.1126   8.194 1.72e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.234 on 13 degrees of freedom\n## Multiple R-squared:  0.8378, Adjusted R-squared:  0.8253 \n## F-statistic: 67.13 on 1 and 13 DF,  p-value: 1.719e-06\n\nWhat do all these results mean?\nThe Estimate in the Coefficients table give:\n\nthe (Intercept) known as \\(\\beta_0\\), which is the value of the y (the response) when the value of x (the explanatory) is zero.\nthe slope labelled temperature known as \\(\\beta_1\\), which is the amount of y you add for each unit of x. temperature is positive so respiration rate increases with temperature\n\nFigure¬†12.2 shows the model and its parameters.\nThe p-values on each line are tests of whether that coefficient is different from zero. Thus it is:\ntemperature  0.91850    0.09182  10.003 1.79e-07 ***\nthat tells us the slope is significantly different from zero and thus there is a significant relationship between temperature and respiration rate.\nThe F value and p-value in the last line are a test of whether the model as a whole explains a significant amount of variation in the response variable. For a regression, this is exactly equivalent to the test of the slope against zero and the two p-values will be the same.\n\n\n\n\n\n\n\nFigure¬†12.2: In a linear model, the first estimate is the intercept and the second estimate is the ‚Äòslope‚Äô.\n\n\n\n\n\n12.3.3 Check assumptions\nCheck the assumptions: All general linear models assume the ‚Äúresiduals‚Äù are normally distributed and have ‚Äúhomogeneity‚Äù of variance.\nOur first check of these assumptions is to use common sense: respiration is a continuous variable and we would expect it to be normally distributed thus we would expect the residuals to be normally distributed\nWe then proceed by plotting residuals. The plot() function can be used to plot the residuals against the fitted values (See Figure¬†12.3). This is a good way to check for homogeneity of variance.\n\nplot(mod, which = 1)\n\n\n\n\n\n\nFigure¬†12.3: A plot of the residuals against the fitted values shows no obvious pattern as the points are roughly evenly distributed around the line. This is a good sign for the assumption of homogeneity of variance.\n\n\n\n\nWe can also use a histogram to check for normality (See Figure¬†12.4).\n\nggplot(mapping = aes(x = mod$residuals)) + \n  geom_histogram(bins = 5)\n\n\n\n\n\n\nFigure¬†12.4: A histogram of residuals is symetrical and seems consistent with a normal distribution. This is a good sign for the assumption of normally distributed residuals.\n\n\n\n\nFinally, we can use the Shapiro-Wilk test to test for normality.\n\nshapiro.test(mod$residuals)\n## \n##  Shapiro-Wilk normality test\n## \n## data:  mod$residuals\n## W = 0.95115, p-value = 0.5428\n\nThe p-value is greater than 0.05 so this test of the normality assumption is not significant. Note that ‚Äúnot significant‚Äù means not significantly different from a normal distribution. It does not mean definitely normally distributed.\nTaken together, these results suggest that the assumptions of normality and homogeneity of variance are not violated.\n\n12.3.4 Report\nThe temperature explained a significant amount of the variation in respiration rate (ANOVA: F = 67; d.f. = 1, 13; p &lt; 0.001). The regression line is: Respiration rate = 4.86 + 0.92 * temperature. See Figure¬†12.5.\nCodeggplot(data = shrimp, \n                aes(x = temperature, y = respiration)) +\n  geom_point(size = 2) +   \n  geom_smooth(method = \"lm\", \n              se = FALSE,\n              colour = \"black\") +\n  scale_x_continuous(expand = c(0,0),\n                     limits = c(0, 15.5),\n                      name = \"Temperature (C)\") +\n  scale_y_continuous(expand = c(0,0), \n                     limits = c(0, 20),\n                     name = \"Respiration (units)\") +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†12.5: Respiration rate of Artemia salina increases with temperature. Three replicate water baths were set up at each of five temperatures (10, 11C, 12C, 13C, 14C). Ten Brine Shrimp (Artemia salina) were placed in each and their average respiration rate per water bath was measured. There was a significant effect of altering water bath temperature on the average respiration rate (ANOVA: F = 67; d.f. = 1, 13; p &lt; 0.001). The regression line is: Respiration rate = 4.86 + 0.92 * temperature. Data analysis was conducted in R (R Core Team 2024) with tidyverse packages (Wickham et al. 2019).",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Single linear regression</span>"
    ]
  },
  {
    "objectID": "single_linear_regression.html#summary",
    "href": "single_linear_regression.html#summary",
    "title": "\n12¬† Single linear regression\n",
    "section": "\n12.4 Summary",
    "text": "12.4 Summary\n\nSingle linear regression is an appropriate when you have one continuous explanatory variable and one continuous response and the relationship between the two is linear.\nApplying a single linear regression to data means putting a line of best fit through it. We estimate the coefficients (also called the parameters) of the model. These are the intercept, \\(\\beta_0\\), and the slope, \\(\\beta_1\\). We test whether the parameters differ significantly from zero\nWe can use lm() to a linear regression.\nIn the output of lm() the coefficients are listed in a table in the Estimates column. The p-value for each coefficient is in the test of whether it differs from zero. At the bottom of the output there is a test of the model overall. In a single linear regression this is exactly the same as the test of the \\(\\beta_1\\) and the p-values are identical. The R-squared value is the proportion of the variance in the response variable that is explained by the model.\nThe assumptions of the general linear model are that the residuals are normally distributed and have homogeneity of variance. A residual is the difference between the predicted value and the observed value.\nWe examine a histogram of the residuals and use the Shapiro-Wilk normality test to check the normality assumption. We check the variance of the residuals is the same for all fitted values with a residuals vs fitted plot.\nIf the assumptions are not met, we might need to transform the data or use a different type of model.\nWhen reporting the results of a regression we give the significance, direction and size of the effect. Often we give the equation of the best fitting line. A Figure should show the data and the line of best fit.\n\n\n\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nWickham, Hadley. 2014. ‚ÄúTidy Data.‚Äù Journal of Statistical Software, Articles 59 (10): 1‚Äì23. https://vita.had.co.nz/papers/tidy-data.pdf.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D‚ÄôAgostino McGowan, Romain Fran√ßois, Garrett Grolemund, et al. 2019. ‚ÄúWelcome to the Tidyverse‚Äù 4: 1686. https://doi.org/10.21105/joss.01686.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Single linear regression</span>"
    ]
  },
  {
    "objectID": "two_sample_tests.html",
    "href": "two_sample_tests.html",
    "title": "\n13¬† Two-Sample tests\n",
    "section": "",
    "text": "13.1 Overview\nIn the last chapter, we explored single linear regression, a technique used when the explanatory variable is continuous. Now, we shift our focus to cases where the explanatory variable is categorical with two groups. For example, we may want to determine if there is a difference in mass between two subspecies of chaffinch or compare marks in two subjects.\nTo conduct a two-sample test, we use either lm() or wilcox.test(), depending on whether the assumptions of lm() are met. General linear models applied with lm() are parametric tests, meaning they rely on the normal distribution‚Äôs parameters (mean and standard deviation) to determine statistical significance. The null hypothesis typically concerns the mean or the difference between means. For the p-values to be valid, the assumptions must be satisfied.\nIf these assumptions are not met, we turn to non-parametric tests, which rely on the ranks of values rather than the actual values themselves. Here, the null hypothesis concerns the mean rank instead of the mean. While non-parametric tests are more flexible and applicable in a wider range of scenarios, they tend to be less powerful, meaning they are less likely to detect a true difference when one exists.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Two-Sample tests</span>"
    ]
  },
  {
    "objectID": "two_sample_tests.html#overview",
    "href": "two_sample_tests.html#overview",
    "title": "\n13¬† Two-Sample tests\n",
    "section": "",
    "text": "13.1.1 Independent vs.¬†Paired Samples\nA crucial consideration when conducting tests is whether the values in one group are independent of those in the other. Non-independence occurs when the two measures are linked in some way‚Äîfor instance, if they come from the same individual, time, or location.\nFor example, when evaluating a treatment for high blood pressure, we might measure blood pressure before and after treatment on the same individuals. In this case, the before and after measurements are not independent. If pairs of observations across groups share a common factor that makes them more similar to each other than to other observations, the samples are not independent.\nWe use different testing approaches for independent and non-independent samples to account for this dependency.\n\n13.1.2 T-tests\nA linear model with one explanatory variable with two groups is also known as a two-sample t-test when the samples are independent and as a paired-samples t-test when they are not. R does have a t.test() function which allows you to fit a linear model with just two groups. However, here we teach you to use and interpret the lm() function because it is more generalisable. You can use lm() when you have three or more groups or additional explanatory variables. The output of lm() is also in the same form as many other statistical functions in R. This means what you learn in performing t-tests with lm() will help you learn other methods more easily. However, it is definitely not wrong to use t.test() rather than lm() for two-group situations - the procedures are identical and the p-values will be the same.\n\n13.1.3 Model assumptions\nThe assumptions of the general linear model are that the residuals are normally distributed and have homogeneity of variance. A residual is the difference between the predicted value and the observed value.\nIf we have a continuous response and a categorical explanatory variable with two groups, we usually apply the general linear model with lm() and then check the assumptions, however, we can sometimes tell when a non-parametric test would be more appropriate before that:\n\nUse common sense - the response should be continuous (or nearly continuous, see Ideas about data: Theory and practice). Consider whether you would expect the response to be continuous.\nWe expect decimal places and few repeated values.\n\nTo examine the assumptions after fitting the linear model, we plot the residuals and test them against the normal distribution in the same way as we did for single linear regression.\n\n13.1.4 Reporting\nIn reporting the result of two-sample test we give:\n\n\nthe significance of effect - whether there is there a difference between the groups\n\nparametric: whether there is there a difference between the groups means\nnon-parametric: whether there is there a difference between the group medians\n\n\nthe direction of effect - which of the means/medians is greater\n\nthe magnitude of effect - how big is the difference between the means/medians\n\nparametric: the means and standard errors for each group or the mean difference for paired samples\nnon-parametric: the medians for each group or the median difference for paired samples\n\n\n\nFigures should reflect what you have said in the statements. Ideally they should show both the raw data and the statistical model:\n\nparametric: means and standard errors\nnon-parametric: boxplots with medians and interquartile range\n\nWe will explore all of these ideas with some examples.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Two-Sample tests</span>"
    ]
  },
  {
    "objectID": "two_sample_tests.html#your-turn",
    "href": "two_sample_tests.html#your-turn",
    "title": "\n13¬† Two-Sample tests\n",
    "section": "\n13.2 üé¨ Your turn!",
    "text": "13.2 üé¨ Your turn!\nIf you want to code along you will need to start a new RStudio project, add a data-raw folder and open a new script. You will also need to load the tidyverse package (Wickham et al. 2019).",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Two-Sample tests</span>"
    ]
  },
  {
    "objectID": "two_sample_tests.html#two-independent-samples-parametric",
    "href": "two_sample_tests.html#two-independent-samples-parametric",
    "title": "\n13¬† Two-Sample tests\n",
    "section": "\n13.3 Two independent samples, parametric",
    "text": "13.3 Two independent samples, parametric\nA number of subspecies of the common chaffinch, Fringilla coelebs, have been described based principally on the differences in the pattern and colour of the adult male plumage (Su√°rez et al. 2009). Two of groups of these subspecies are:\n\nthe ‚Äúcoelebs group‚Äù (Figure¬†13.1 (a)) that occurs in Europe and Asia\nthe ‚Äúcanariensis group‚Äù (Figure¬†13.1 (b)) that occurs on the Canary Islands.\n\n\n\n\n\n\n\n\n\n\n(a) F. c. coelebs\n\n\n\n\n\n\n\n\n\n(b) F. c. palmae\n\n\n\n\n\n\nFigure¬†13.1: Adult male Fringilla coelebs of the coelebs group on the left (Andreas Trepte, CC BY-SA 2.5 https://creativecommons.org/licenses/by-sa/2.5, via Wikimedia Commons) and of the canariensis group on the right (H. Zell, CC BY-SA 3.0 https://creativecommons.org/licenses/by-sa/3.0, via Wikimedia Commons).\n\n\nThe data in chaff.txt give the masses of twenty individuals from each subspecies. We want to know if the subspecies differ in mass. These groups are independent - there is no link between values in one group and any value in the other group. In this scenario our null hypothesis, \\(H_0\\), is that there is no difference between the two subspecies in mass or that subspecies has no effect on mass. This is written as: \\(H_0: \\beta_1 = 0\\).\n\n13.3.1 Import and explore\nImport the data:\n\nchaff &lt;- read_table(\"data-raw/chaff.txt\")\n\n\n\n\n\n\nsubspecies\nmass\n\n\n\ncoelebs\n18.3\n\n\ncoelebs\n22.1\n\n\ncoelebs\n22.4\n\n\ncoelebs\n18.5\n\n\ncoelebs\n22.2\n\n\ncoelebs\n19.3\n\n\ncoelebs\n17.8\n\n\ncoelebs\n20.2\n\n\ncoelebs\n22.1\n\n\ncoelebs\n16.6\n\n\ncoelebs\n20.7\n\n\ncoelebs\n18.7\n\n\ncoelebs\n22.6\n\n\ncoelebs\n21.5\n\n\ncoelebs\n21.7\n\n\ncoelebs\n19.9\n\n\ncoelebs\n23.1\n\n\ncoelebs\n17.8\n\n\ncoelebs\n19.5\n\n\ncoelebs\n24.6\n\n\ncanariensis\n22.7\n\n\ncanariensis\n20.6\n\n\ncanariensis\n25.4\n\n\ncanariensis\n20.4\n\n\ncanariensis\n21.6\n\n\ncanariensis\n17.0\n\n\ncanariensis\n26.4\n\n\ncanariensis\n20.4\n\n\ncanariensis\n24.7\n\n\ncanariensis\n21.8\n\n\ncanariensis\n23.4\n\n\ncanariensis\n24.4\n\n\ncanariensis\n21.0\n\n\ncanariensis\n23.4\n\n\ncanariensis\n20.5\n\n\ncanariensis\n21.4\n\n\ncanariensis\n21.5\n\n\ncanariensis\n23.7\n\n\ncanariensis\n23.4\n\n\ncanariensis\n21.8\n\n\n\n\n\n\n\nThese data are in tidy format (Wickham 2014) - all the mass values are in one column with another column indicating the subspecies. This means they are well formatted for analysis and plotting.\nIn the first instance, it is always sensible to create a rough plot of our data. This is to give us an overview and help identify if there are any issues like missing or extreme values. It also gives us idea what we are expecting from the analysis which will make it easier for us to identify if we make some mistake in applying that analysis.\nViolin plots (geom_violin()), box plots (geom_boxplot(), see Figure¬†13.2) or scatter plots (geom_point()) all make good choices for exploratory plotting and it does not matter which of these you choose.\n\nggplot(data = chaff,\n       aes(x = subspecies, y = mass)) +\n  geom_boxplot()\n\n\n\n\n\n\nFigure¬†13.2: The mass of two subspecies of chaffinch. A boxplot is a useful way to get an overview of the data and helps us identify any issues such as missing or extreme values. It also tells us what to expect from the analysis.\n\n\n\n\nR will order the groups alphabetically by default.\nThe figure suggests that the canariensis group is heavier than the coelebs group.\nSummarising the data for each subspecies group is the next sensible step. The most useful summary statistics are the means, standard deviations, sample sizes and standard errors. I recommend the group_by() and summarise() approach:\n\nchaff_summary &lt;- chaff |&gt; \n  group_by(subspecies) |&gt; \n  summarise(mean = mean(mass),\n            std = sd(mass),\n            n = length(mass),\n            se = std/sqrt(n))\n\nWe have save the results to chaff_summary so that we can use the means and standard errors in our plot later.\n\nchaff_summary\n## # A tibble: 2 √ó 5\n##   subspecies   mean   std     n    se\n##   &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n## 1 canariensis  22.3  2.15    20 0.481\n## 2 coelebs      20.5  2.14    20 0.478\n\n\n13.3.2 Apply lm()\n\nWe can create a two-sample model like this:\n\nmod &lt;- lm(data = chaff, mass ~ subspecies)\n\nAnd examine the model with:\n\nsummary(mod)\n## \n## Call:\n## lm(formula = mass ~ subspecies, data = chaff)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.2750 -1.7000 -0.3775  1.6200  4.1250 \n## \n## Coefficients:\n##                   Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)        22.2750     0.4795  46.456   &lt;2e-16 ***\n## subspeciescoelebs  -1.7950     0.6781  -2.647   0.0118 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.144 on 38 degrees of freedom\n## Multiple R-squared:  0.1557, Adjusted R-squared:  0.1335 \n## F-statistic: 7.007 on 1 and 38 DF,  p-value: 0.01175\n\nThe Estimates in the Coefficients table give:\n\n(Intercept) known as \\(\\beta_0\\). The mean of the canariensis group (Figure¬†13.3). Just as the intercept is the value of the y (the response) when the value of x (the explanatory) is zero in a simple linear regression, this is the value of mass when the subspecies is at its first level. The order of the levels is alphabetical by default.\nsubspeciescoelebs, known as \\(\\beta_1\\), is what needs to be added to the mean of the canariensis group to get the mean of the coelebs group (Figure¬†13.3). Just as the slope is amount of y that needs to be added for each unit of x in a simple linear regression, this is the amount of mass that needs to be added when the subspecies goes from its first level to its second level (i.e., one unit). The subspeciescoelebs estimate is negative so the the coelebs group mean is lower than the canariensis group mean\n\nThe p-values on each line are tests of whether that coefficient is different from zero. Thus it is:\nsubspeciescoelebs  -1.7950     0.6781  -2.647   0.0118 *\nthat tells us the difference between the means is significant.\nThe F value and p-value in the last line are a test of whether the model as a whole explains a significant amount of variation in the response variable. For a two-sample test, just like a regression, this is exactly equivalent to the test of the slope against zero and the two p-values will be the same.\n\n\n\n\n\n\n\nFigure¬†13.3: In a two-sample linear model, the first estimate is the intercept which is the mean of the first group. The second estimate is the ‚Äòslope‚Äô which is what has to added to the intercept to get the second group mean. Note that y axis starts at 15 to create more space for the annotations.\n\n\n\n\n\n13.3.3 Check assumptions\nCheck the assumptions: All general linear models assume the ‚Äúresiduals‚Äù are normally distributed and have ‚Äúhomogeneity‚Äù of variance.\nOur first check of these assumptions is to use common sense: mass is a continuous variable and we would expect it to be normally distributed thus we would also expect the residuals to be normally distributed.\nWe then plot the residuals. The plot() function can be used to plot the residuals against the fitted values (See Figure¬†13.4). This is a good way to check for homogeneity of variance.\n\nplot(mod, which = 1)\n\n\n\n\n\n\nFigure¬†13.4: A plot of the residuals against the fitted values shows the points are distributed similarly in each group. This is a good sign for the assumption of homogeneity of variance.\n\n\n\n\nWe can also use a histogram to check for normality (See Figure¬†13.5).\n\nggplot(mapping = aes(x = mod$residuals)) + \n  geom_histogram(bins = 10)\n\n\n\n\n\n\nFigure¬†13.5: A histogram of residuals is symetrical and seems consistent with a normal distribution. This is a good sign for the assumption of normally distributed residuals.\n\n\n\n\nFinally, we can use the Shapiro-Wilk test to test for normality.\n\nshapiro.test(mod$residuals)\n## \n##  Shapiro-Wilk normality test\n## \n## data:  mod$residuals\n## W = 0.98046, p-value = 0.7067\n\nThe p-value is greater than 0.05 so this test of the normality assumption is not significant.\nTaken together, these results suggest that the assumptions of normality and homogeneity of variance are not violated.\n\n13.3.4 Report\nCanariensis chaffinches (\\(\\bar{x} \\pm s.e\\): 22.48 \\(\\pm\\) 0.48) were significantly heavier than Coelebs (20.28 \\(\\pm\\) 0.48 ) (t = 2.65; d.f. = 38; p = 0.012). See Figure¬†13.6.\nCodeggplot() +\n  geom_point(data = chaff, aes(x = subspecies, y = mass),\n             position = position_jitter(width = 0.1, height = 0),\n             colour = \"gray50\") +\n  geom_errorbar(data = chaff_summary, \n                aes(x = subspecies, ymin = mean - se, ymax = mean + se),\n                width = 0.3) +\n  geom_errorbar(data = chaff_summary, \n                aes(x = subspecies, ymin = mean, ymax = mean),\n                width = 0.2) +\n  scale_y_continuous(name = \"Mass (g)\", \n                     limits = c(0, 30), \n                     expand = c(0, 0)) +\n  scale_x_discrete(name = \"Subspecies\", \n                   labels = c(\"Canariensis\", \"Coelebs\")) +\n  annotate(\"segment\", x = 1, xend = 2, \n           y = 28, yend = 28,\n           colour = \"black\") +\n  annotate(\"text\", x = 1.5,  y = 29, \n           label = expression(italic(p)~\"= 0.012\")) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†13.6: Canariensis chaffinches are heavier than Coelebs chaffinches. The mean mass of 20 randomly sampled males from each subspecies was determined. Error bars are \\(\\pm\\) 1 standard error. Canariensis chaffinches were significantly heavier than Coelebs (t = 2.65; d.f. = 38; p = 0.012). Data analysis was conducted in R (R Core Team 2024) with tidyverse packages (Wickham et al. 2019).",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Two-Sample tests</span>"
    ]
  },
  {
    "objectID": "two_sample_tests.html#two-independent-samples-non-parametric",
    "href": "two_sample_tests.html#two-independent-samples-non-parametric",
    "title": "\n13¬† Two-Sample tests\n",
    "section": "\n13.4 Two independent samples, non-parametric",
    "text": "13.4 Two independent samples, non-parametric\nThe non-parametric equivalent of the linear model with two independent samples is the ‚ÄúWilcoxon rank sum test‚Äù (Wilcoxon 1945). It is commonly also known as the Mann-Whitney or Wilcoxon‚ÄìMann‚ÄìWhitney.\nThe general question you have about your data - are these two groups different - is the same, but one of more of the following is true:\n\nthe response variable is not continuous\nthe residuals are not normally distributed\nthe sample size is too small to tell if they are normally distributed.\nthe variance is not homogeneous\n\nThe test is a applied in R with the wilcox.test() function.\nThe data in arabidopsis.txt give the number of leaves on eight wildtype and eight mutant Arabidopsis thaliana plants. We want to know if the two types of plants have differing numbers of leaves. These are counts, so they are not continuous and the sample sizes are quite small. A non-parametric test is a safer option. In this scenario our null hypothesis, \\(H_0\\), is that there is no difference between the two types of plant in the number of leaves.\n\n13.4.1 Import and explore\n\narabidopsis &lt;- read_table(\"data-raw/arabidopsis.txt\")\n\nThese data are in tidy format (Wickham 2014) - the numbers of leaves are in one column with another column indicating whether the observation comes from a wildtype or mutant Arabidopsis. This means they are well formatted for analysis and plotting.\nCreate a quick plot of the data:\n\nggplot(data = arabidopsis, \n       aes(x = type, y = leaves)) +\n  geom_boxplot()\n\n\n\n\n\n\nFigure¬†13.7: The number of leaves on mutant and wildtype plants. A boxplot is a useful way to get an overview of the data and helps us identify any issues such as missing or extreme values. It also tells us what to expect from the analysis.\n\n\n\n\nOur rough plot shows that the mutant plants have fewer leaves than the wildtype plants.\nSummarising the data using the median and interquartile range is more aligned to the type of data and the type of analysis than using means and standard deviations:\n\narabidopsis_summary &lt;- arabidopsis |&gt; \n  group_by(type) |&gt; \n  summarise(median = median(leaves),\n            interquartile  = IQR(leaves),\n            n = length(leaves))\n\nView the results:\n\narabidopsis_summary\n## # A tibble: 2 √ó 4\n##   type   median interquartile     n\n##   &lt;chr&gt;   &lt;dbl&gt;         &lt;dbl&gt; &lt;int&gt;\n## 1 mutant    5             2.5     8\n## 2 wild      8.5           1.5     8\n\n\n13.4.2 Apply wilcox.test()\n\nWe pass the dataframe and variables to wilcox.test() in the same way as we did for lm(). We give the data argument and a ‚Äúformula‚Äù which says leaves ~ type meaning ‚Äúexplain leaves by type‚Äù.\n\nwilcox.test(data = arabidopsis, leaves ~ type)\n## \n##  Wilcoxon rank sum test with continuity correction\n## \n## data:  leaves by type\n## W = 5, p-value = 0.005051\n## alternative hypothesis: true location shift is not equal to 0\n\nThe warning message ‚ÄúWarning: cannot compute exact p-value with ties‚Äù is not something to worry about too much. It is a warning rather than an indication that your results are incorrect. It means the p -value is based on an approximation rather than being exact because there are ties (some values are the same).\nThe result of the test is given on this line: W = 5, p-value = 0.005051. W is the test statistic. The p-value is less than 0.05 meaning there is a significant difference in the number of leaves on wildtype and mutant plants.\n\n13.4.3 Report\nThere are significantly more leaves on wildtype (median = 8.5) than mutant (median = 5) plants (Wilcoxon rank sum test: W = 5, \\(n_1\\) = 8, \\(n_2\\) = 8, p = 0.005). See Figure¬†13.8.\nCodeggplot(data = arabidopsis, \n       aes(x = type, y = leaves)) +\n  geom_boxplot() +\n  scale_y_continuous(name = \"Number of leaves\", \n                     limits = c(0, 12), \n                     expand = c(0, 0)) +\n  scale_x_discrete(name = \"\", \n                   labels = c(\"Mutatnt\", \"Wildtype\")) +\n  annotate(\"segment\", x = 1, xend = 2, \n           y = 10.5, yend = 10.5,\n           colour = \"black\") +\n  annotate(\"text\", x = 1.5,  y = 11, \n           label = expression(italic(p)~\"= 0.005\")) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†13.8: Mutant Arabidopsis thaliana have fewer leaves. There are significantly more leaves on wildtype than mutant plants (Wilcoxon rank sum test: W = 5, \\(n_1\\) = 8, \\(n_2\\) = 8, p = 0.005). The heavy lines indicate the median of leaves, boxes indicate the interquartile range and whiskers the range. Data analysis was conducted in R (R Core Team 2024) with tidyverse packages (Wickham et al. 2019).",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Two-Sample tests</span>"
    ]
  },
  {
    "objectID": "two_sample_tests.html#two-paired-samples-parametric",
    "href": "two_sample_tests.html#two-paired-samples-parametric",
    "title": "\n13¬† Two-Sample tests\n",
    "section": "\n13.5 Two paired-samples, parametric",
    "text": "13.5 Two paired-samples, parametric\nThe data in marks.csv give the marks for ten students in two subjects: Data Analysis and Biology. These data are paired because we have two marks from one student so that a mark in one group has a closer relationship with one of the marks in the other group than with any of the other values. We want to know if students do equally well in both subjects. In this scenario our null hypothesis, \\(H_0\\), is that there is no difference between the Data Analysis and Biology marks for a student.\n\n13.5.1 Import and explore\nImport the data:\n\nmarks &lt;- read_csv(\"data-raw/marks.csv\")\n\nSince these data are paired, it makes sense to highlight how the marks differ for each student. One way of doing that is to draw a line linking their marks in each subject. This is known as a spaghetti plot. We can use two geoms: geom_point() and geom_line(). To join a student‚Äôs marks, we need to set the group aesthetic to student.1\n\nggplot(data = marks, aes(x = subject, y = mark)) +\n  geom_point() +\n  geom_line(aes(group = student))\n\n\n\n\n\n\n\nSummarise the data so that we can use the means in plots later:\n\nmarks_summary &lt;- marks |&gt;\n  group_by(subject) |&gt;\n  summarise(mean = mean(mark))\n\nA paired test requires us to into account the variation between students.\n\n13.5.2 Apply lm()\n\nWe can create a paired-sample model with the lm() function2 like this:\n\nmod &lt;- lm(mark ~ subject + factor(student), data = marks)\n\n\n\nmark is the dependent variable (response).\n\nsubject is the independent variable (explanatory factor).\n\nfactor(student) accounts for the pairing by treating student as another explanatory variable. We have used factor because the values in students are the numbers 1 to 10 and we want student to be treated as a category not a number\n\n\nsummary(mod)\n## \n## Call:\n## lm(formula = mark ~ subject + factor(student), data = marks)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n##   -6.5   -3.5    0.0    3.5    6.5 \n## \n## Coefficients:\n##                     Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)           89.500      4.697  19.055 1.39e-08 ***\n## subjectDataAnalysis    7.000      2.832   2.471 0.035486 *  \n## factor(student)2     -39.500      6.333  -6.237 0.000152 ***\n## factor(student)3     -26.500      6.333  -4.184 0.002361 ** \n## factor(student)4     -25.500      6.333  -4.026 0.002990 ** \n## factor(student)5     -16.000      6.333  -2.526 0.032431 *  \n## factor(student)6     -54.000      6.333  -8.526 1.33e-05 ***\n## factor(student)7      -9.000      6.333  -1.421 0.189010    \n## factor(student)8     -27.000      6.333  -4.263 0.002101 ** \n## factor(student)9     -44.000      6.333  -6.947 6.70e-05 ***\n## factor(student)10     -1.500      6.333  -0.237 0.818082    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6.333 on 9 degrees of freedom\n## Multiple R-squared:  0.9441, Adjusted R-squared:  0.8821 \n## F-statistic: 15.21 on 10 and 9 DF,  p-value: 0.0001814\n\nThe coefficient for (Intercept) gives the mean Biology mark and that for subjectDataAnalysis is amount that the Data Analysis mark are above Biology marks in general. The p-value tests whether this difference is significantly different from zero. The rest of the output considers how students differ. You can ignore this here.\nIf you find this a bit overwhelming to read you can use the anova() function on the model object to get a simpler output:\n\nanova(mod)\n## Analysis of Variance Table\n## \n## Response: mark\n##                 Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## subject          1  245.0  245.00   6.108 0.035486 *  \n## factor(student)  9 5856.2  650.69  16.222 0.000153 ***\n## Residuals        9  361.0   40.11                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nYou will notice that the p-value for subject is the same. The test statistic, \\(F\\) has a value of 6.11 and degrees of freedom of 1 and 9. This would be written as $F = $ 6.11 ; $d.f.= $ 1, 9; $p = $ 0.035 .\n\n13.5.3 Check assumptions\nWe might expect marks to be normally distributed. However, this is a very small sample, and choosing a non-parametric test instead would be reasonable. However, we will continue with this example to demonstrate how to interpret and report on the result of a parametric paired-samples test (paired-samples t-test).\nA plot the residuals against the fitted values (plot(mod, which = 1)) is not useful for a paired test. The normality of the residuals should be checked.\n\nggplot(mapping = aes(x = mod$residuals)) +\n  geom_histogram(bins = 3)\n\n\n\n\n\n\n\nWe only have 10 values, so the distribution is never going to look smooth. We can‚Äôt draw strong conclusions from this, but we do at least have a peak at 0. Similarly, a normality test is likely to be non-significant because of the small sample size, meaning the test is not very powerful. This means a non-significant result is not strong evidence of the residuals following a normal distribution:\n\nshapiro.test(mod$residuals)\n## \n##  Shapiro-Wilk normality test\n## \n## data:  mod$residuals\n## W = 0.92894, p-value = 0.1473\n\n\n13.5.4 Report\nIndividual students score significantly higher in Data Analysis than in Biology (t = 2.47; d.f. = 9; p = 0.0355) with an average difference of 7%. See Figure¬†13.9\nCodeggplot(data = marks, aes(x = subject, y = mark)) +\n  geom_point(pch = 1, size = 3) +\n  geom_line(aes(group = student), linetype = 3) +\n  geom_point(data = marks_summary,\n             aes(x = subject, y = mean),\n             size = 3) +\n  scale_x_discrete(name = \"\") +\n  scale_y_continuous(name = \"Mark\",\n                     expand = c(0, 0),\n                     limits = c(0, 110)) +\n  annotate(\"segment\", x = 1, xend = 2,\n           y = 105, yend = 105,\n           colour = \"black\") +\n  annotate(\"text\", x = 1.5,  y = 108,\n           label = expression(italic(p)~\"= 0.0355\")) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†13.9: Students score higher in Data Analysis than in Biology. Open circles indicate an individual student‚Äôs marks in each subject with dashed lines joining their marks in each subject. The filled circles indicate the mean mark for each subject. Individual students score significantly higher in Data Analysis than in Biology (t = 2.47; d.f. = 9; p = 0.0355) with an average difference of 7%. Data analysis was conducted in R (R Core Team 2024) with tidyverse packages (Wickham et al. 2019).",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Two-Sample tests</span>"
    ]
  },
  {
    "objectID": "two_sample_tests.html#two-paired-samples-non-parametric",
    "href": "two_sample_tests.html#two-paired-samples-non-parametric",
    "title": "\n13¬† Two-Sample tests\n",
    "section": "\n13.6 Two paired-samples, non-parametric",
    "text": "13.6 Two paired-samples, non-parametric\nWe have the marks for just 10 students. This sample is too small for us to judge whether the marks are normally distributed. We will use a non-parametric test instead. The ‚ÄúWilcoxon signed-rank‚Äù test is the non-parametric equivalent of the paired-samples t-test. This is often referred to as the paired-sample Wilcoxon test, or just the Wilcoxon test.\nThe test is also applied in R with the wilcox.test() function but we add the paired = TRUE argument. We also have to give the two datasets rather than using the ‚Äúformula method‚Äù of mark ~ subject. This means it is useful to pivot the data to ‚Äúwide‚Äù format.\n\n13.6.1 Pivot wider\nCreate a new dataframe marks_wide from marks:\n\nmarks_wide &lt;- marks |&gt; \n  pivot_wider(values_from = mark, \n              names_from = subject, \n              id_cols = student)\n\n\n\nvalues_from = mark: Uses the mark column as values in the wide format.\n\nnames_from = subject: Creates new columns based on unique values in subject\n\n\nid_cols = student: Keeps student as the identifier (i.e., each row represents a student).\n\n13.6.2 Apply wilcox.test()\n\nTo apply a paired test with wilcox.test() we need to use the wide format data (untidy) and add the paired = TRUE argument:\n\nwilcox.test(marks_wide$Biology, marks_wide$DataAnalysis, paired = TRUE)\n## \n##  Wilcoxon signed rank test with continuity correction\n## \n## data:  marks_wide$Biology and marks_wide$DataAnalysis\n## V = 6.5, p-value = 0.03641\n## alternative hypothesis: true location shift is not equal to 0\n\n\n13.6.3 Report\nIndividual students score significantly higher in Data Analysis than in Biology (Wilcoxon signed rank test: V = 6.5; \\(n\\) = 10; p = 0.036).",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Two-Sample tests</span>"
    ]
  },
  {
    "objectID": "two_sample_tests.html#summary",
    "href": "two_sample_tests.html#summary",
    "title": "\n13¬† Two-Sample tests\n",
    "section": "\n13.7 Summary",
    "text": "13.7 Summary\n\nA linear model with one explanatory variable with two groups and one continuous response is ‚Äúa two-sample test‚Äù.\nIf pairs of observations in the groups have something in common that make them more similar to each other, than to other observations, then those observations are not independent. A paired-samples test is used when the observations are not independent.\nA linear model with one explanatory variable with two groups and one continuous response is also known as a two-sample t-test when the samples are independent and as a paired-samples t-test when they are not\nWe can use lm() to do two-sample and paired sample tests. We can also use t.test() for these but using lm() helps us understand tests with more groups and/or more variables where we will have to use lm(). The output of lm() is also more typical of the output of statistical functions in R.\nWe estimate the coefficients (also called the parameters) of the model. For a two-sample test these are the mean of the first group, \\(\\beta_0\\) (which might also be called the intercept) and the difference between the means of the first and second groups, \\(\\beta_1\\) (which might also be called the slope). For a paired-sample test there is just one parameter, the mean difference between pairs of values, \\(\\beta_0\\) (which might also be called the intercept). We test whether the parameters differ significantly from zero\nWe can use lm() to a linear regression.\nIn the output of lm() the coefficients are listed in a table in the Estimates column. The p-value for each coefficient is in the test of whether it differs from zero. At the bottom of the output there is a test of the model overall. In this case, this is exactly the same as the test of the \\(\\beta_1\\) and the p-values are identical. The R-squared value is the proportion of the variance in the response variable that is explained by the model.\nThe assumptions of the general linear model are that the residuals are normally distributed and have homogeneity of variance. A residual is the difference between the predicted value and the observed value.\nWe examine a histogram of the residuals and use the Shapiro-Wilk normality test to check the normality assumption. We check the variance of the residuals is the same for all fitted values with a residuals vs fitted plot.\nIf the assumptions are not met, we can use alternatives known as non-parametric tests. These are applied with wilcox.test() in R.\nWhen reporting the results of a test we give the significance, direction and size of the effect. Our figures and the values we give should reflect the type of test we have used. We use means and standard errors for parametric tests and medians and interquartile ranges for non-parametric tests. We also give the test statistic, the degrees of freedom (parametric) or sample size (non-parametric) and the p-value. We annotate our figures with the p-value, making clear which comparison it applies to.\n\n\n\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nSu√°rez, Nicol√°s M., Eva Betancor, Tilman E. Klassert, Teresa Almeida, Mariano Hern√°ndez, and Jos√© J. Pestano. 2009. ‚ÄúPhylogeography and Genetic Structure of the Canarian Common Chaffinch (Fringilla Coelebs) Inferred with mtDNA and Microsatellite Loci.‚Äù Molecular Phylogenetics and Evolution 53 (2): 556‚Äì64. https://doi.org/10.1016/j.ympev.2009.07.018.\n\n\nWickham, Hadley. 2014. ‚ÄúTidy Data.‚Äù Journal of Statistical Software, Articles 59 (10): 1‚Äì23. https://vita.had.co.nz/papers/tidy-data.pdf.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D‚ÄôAgostino McGowan, Romain Fran√ßois, Garrett Grolemund, et al. 2019. ‚ÄúWelcome to the Tidyverse‚Äù 4: 1686. https://doi.org/10.21105/joss.01686.\n\n\nWilcoxon, Frank. 1945. ‚ÄúIndividual Comparisons by Ranking Methods.‚Äù Biometrics Bulletin 1 (6): 80‚Äì83. https://doi.org/10.2307/3001968.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Two-Sample tests</span>"
    ]
  },
  {
    "objectID": "two_sample_tests.html#footnotes",
    "href": "two_sample_tests.html#footnotes",
    "title": "\n13¬† Two-Sample tests\n",
    "section": "",
    "text": "You might like to try removing aes(group = student) to see what ggplot does when the lines are not grouped by student.‚Ü©Ô∏é\nThis is not the only way to apply a paired test. When there are only two groups and no other explanatory variables, we can use t.test(data = marks, mark ~ subject, paired = TRUE). A more general method that works when you have two or more non-independent values (e.g., more than two subjects) or additional explanatory variables is to create a ‚Äúlinear mixed model‚Äù with lmer().‚Ü©Ô∏é",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Two-Sample tests</span>"
    ]
  },
  {
    "objectID": "one_way_anova_and_kw.html",
    "href": "one_way_anova_and_kw.html",
    "title": "\n14¬† One-way ANOVA and Kruskal-Wallis\n",
    "section": "",
    "text": "14.1 Overview\nIn the last chapter, we learnt how to use and interpret the general linear model when the x variable was categorical with two groups. You will now extend that to situations when there are more than two groups. This is often known as the one-way ANOVA (analysis of variance). We will also learn about the Kruskal-Wallis test (Kruskal and Wallis 1952), a non-parametric test that can be used when the assumptions of the general linear model are not met.\nWe use lm() to carry out a one-way ANOVA. General linear models applied with lm() are based on the normal distribution and known as parametric tests because they use the parameters of the normal distribution (the mean and standard deviation) to determine if an effect is significant. Null hypotheses are about a mean or difference between means. The assumptions need to be met for the p-values generated to be accurate.\nIf the assumptions are not met, we can use the non-parametric equivalent known as the Kruskal-Wallis test. Like other non-parametric tests, the Kruskal-Wallis test :\nThe process of using lm() to conduct a one-way ANOVA is very similar to using lm() for a two-sample t-test, but with an important distinction. When we obtain a significant effect of our explanatory variable, it only indicates that at least two group means differ‚Äîit does not specify which ones. To determine where the differences lie, we need a post-hoc test.\nA post-hoc (‚Äúafter this‚Äù) test is performed after a significant ANOVA result. There are several options for post-hoc tests, and we will use Tukey‚Äôs HSD (honestly significant difference) test (Tukey 1949), implemented in the emmeans (Lenth 2023) package.\nPost-hoc tests adjust p-values to account for multiple comparisons. A Type I error occurs when we incorrectly reject a true null hypothesis, with a probability of 0.05. Conducting multiple comparisons increases the likelihood of obtaining a significant result by chance. The post-hoc test corrects for this increased risk, ensuring more reliable conclusions.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>One-way ANOVA and Kruskal-Wallis</span>"
    ]
  },
  {
    "objectID": "one_way_anova_and_kw.html#overview",
    "href": "one_way_anova_and_kw.html#overview",
    "title": "\n14¬† One-way ANOVA and Kruskal-Wallis\n",
    "section": "",
    "text": "is based on the ranks of values rather than the actual values themselves\nhas a null hypothesis about the mean rank rather than the mean\nhas fewer assumptions and can be used in more situations\ntends to be less powerful than a parametric test when the assumptions are met\n\n\n\n\n\n14.1.1 Model assumptions\nThe assumptions for a general linear model where the explanatory variable has two or more groups, are the same as for two groups: the residuals are normally distributed and have homogeneity of variance.\nIf we have a continuous response and a categorical explanatory variable with three or more groups, we usually apply the general linear model with lm() and then check the assumptions, however, we can sometimes tell when a non-parametric test would be more appropriate before that:\n\nUse common sense - the response should be continuous (or nearly continuous, see Ideas about data: Theory and practice). Consider whether you would expect the response to be continuous\nThere should decimal places and few repeated values.\n\nTo examine the assumptions after fitting the linear model, we plot the residuals and test them against the normal distribution in the same way as we did for single linear regression.\n\n14.1.2 Reporting\nIn reporting the result of one-way ANOVA or Kruskal-Wallis test, we include:\n\n\nthe significance of the effect\n\nparametric (GLM): The F-statistic and p-value\nnon-parametric (Kruskal-Wallis): The Chi-squared statistic and p-value\n\n\n\nthe direction of effect - which mean/median is greater in a the pairwise commparison\n\nPost-hoc test\n\n\n\nthe magnitude of effect - how big is the difference between the means/medians\n\nparametric: the means and standard errors for each group\nnon-parametric: the medians for each group\n\n\n\nFigures should reflect what you have said in the statements. Ideally they should show both the raw data and the statistical model:\n\nparametric: means and standard errors\nnon-parametric: boxplots with medians and interquartile range\n\nWe will explore all of these ideas with some examples.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>One-way ANOVA and Kruskal-Wallis</span>"
    ]
  },
  {
    "objectID": "one_way_anova_and_kw.html#your-turn",
    "href": "one_way_anova_and_kw.html#your-turn",
    "title": "\n14¬† One-way ANOVA and Kruskal-Wallis\n",
    "section": "\n14.2 üé¨ Your turn!",
    "text": "14.2 üé¨ Your turn!\nIf you want to code along you will need to start a new RStudio project, add a data-raw folder and open a new script. You will also need to load the tidyverse package (Wickham et al. 2019).",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>One-way ANOVA and Kruskal-Wallis</span>"
    ]
  },
  {
    "objectID": "one_way_anova_and_kw.html#one-way-anova",
    "href": "one_way_anova_and_kw.html#one-way-anova",
    "title": "\n14¬† One-way ANOVA and Kruskal-Wallis\n",
    "section": "\n14.3 One-way ANOVA",
    "text": "14.3 One-way ANOVA\nResearchers wanted the determine the best growth medium for growing bacterial cultures. They grew bacterial cultures on three different media formulations and measured the diameter of the colonies. The three formulations were:\n\nControl - a generic medium as formulated by the manufacturer\nsugar added - the generic medium with added sugar\nsugar and amino acids added - the generic medium with added sugar and amino acids\n\nThe data are in culture.csv.\nIn this scenario, our null hypothesis, \\(H_0\\), is that there is no difference in colony diameter between the three groups or that group membership has no effect on diameter. This is written as: \\(H_0: \\beta_1 = \\beta_2 = 0\\)\nThis means that none of the group means differ significantly from each other.\nAnother way of describing, \\(H_0\\), is to say the variation caused by media is no greater than the the random background variation. This is expressed with the F-statistic, a variance ratio that compares the variance explained by the model to the variance within groups. If \\(H_0\\) is true, the variance between groups is no greater than the variance within groups. That is, F, \\(\\frac{Var(between)}{Var(within)}\\) equals 1. This is written as: \\(H_0: F = 1\\)\nBoth versions of the null hypothesis describe the same idea: that here is no real difference between the groups.\n\nUsing \\(\\beta\\) coefficients: This version focuses on the linear model. It says that the group variable has no effect, meaning all groups have the same average value. If the coefficients (\\(\\beta_1\\), \\(\\beta_2\\), etc.) are zero, then the model isn‚Äôt capturing any meaningful differences between groups.\nUsing the F-statistic: This version looks at variance. It compares the variation between groups to the variation within groups. If the F-value is close to 1, it means the differences between groups are about the same as random variation within groups, so there‚Äôs no real effect.\n\nBoth approaches test the same hypothesis‚Äîif we reject the null, we conclude that at least one group is different from the others.\n\n14.3.1 Import and explore\nImport the data:\n\nculture &lt;- read_csv(\"data-raw/culture.csv\")\n\n\n\n\n\n\ndiameter\nmedium\n\n\n\n11.22\ncontrol\n\n\n9.35\ncontrol\n\n\n9.15\ncontrol\n\n\n10.35\ncontrol\n\n\n9.63\ncontrol\n\n\n10.96\ncontrol\n\n\n10.07\ncontrol\n\n\n10.40\ncontrol\n\n\n10.33\ncontrol\n\n\n9.24\ncontrol\n\n\n8.90\nsugar added\n\n\n10.75\nsugar added\n\n\n11.95\nsugar added\n\n\n9.85\nsugar added\n\n\n10.12\nsugar added\n\n\n10.05\nsugar added\n\n\n9.60\nsugar added\n\n\n10.10\nsugar added\n\n\n10.20\nsugar added\n\n\n10.88\nsugar added\n\n\n10.45\nsugar and amino acids added\n\n\n13.19\nsugar and amino acids added\n\n\n11.84\nsugar and amino acids added\n\n\n13.35\nsugar and amino acids added\n\n\n11.22\nsugar and amino acids added\n\n\n9.86\nsugar and amino acids added\n\n\n10.27\nsugar and amino acids added\n\n\n10.62\nsugar and amino acids added\n\n\n11.78\nsugar and amino acids added\n\n\n11.43\nsugar and amino acids added\n\n\n\n\n\n\n\nThe Response variable is colony diameters in millimetres and we would expect it to be continuous. The Explanatory variable is type of media and is categorical with 3 groups. It is known ‚Äúone-way ANOVA‚Äù or ‚Äúone-factor ANOVA‚Äù because there is only one explanatory variable. It would still be one-way ANOVA if we had 4, 20 or 100 media.\nThese data are in tidy format (Wickham 2014) - all the diameter values are in one column with another column indicating the media. This means they are well formatted for analysis and plotting.\nIn the first instance it is sensible to create a rough plot of our data. This is to give us an overview and help identify if there are any issues like missing or extreme values. It also gives us idea what we are expecting from the analysis which will make it easier for us to identify if we make some mistake in applying that analysis.\nViolin plots (geom_violin(), see Figure¬†14.1), box plots (geom_boxplot()) or scatter plots (geom_point()) all make good choices for exploratory plotting and it does not matter which of these you choose.\n\nggplot(data = culture,\n       aes(x = medium, y = diameter)) +\n  geom_violin()\n\n\n\n\n\n\nFigure¬†14.1: The diameters of bacterial colonies when grown in one of three media. A violin plot is a useful way to get an overview of the data and helps us identify any issues such as missing or extreme values. It also tells us what to expect from the analysis.\n\n\n\n\nR will order the groups alphabetically by default.\nThe figure suggests that adding sugar and amino acids to the medium increases the diameter of the colonies.\nSummarising the data for each medium is the next sensible step. The most useful summary statistics are the means, standard deviations, sample sizes and standard errors. I recommend the group_by() and summarise() approach:\n\nculture_summary &lt;- culture %&gt;%\n  group_by(medium) %&gt;%\n  summarise(mean = mean(diameter),\n            std = sd(diameter),\n            n = length(diameter),\n            se = std/sqrt(n))\n\nWe have save the results to culture_summary so that we can use the means and standard errors in our plot later.\n\nculture_summary\n## # A tibble: 3 √ó 5\n##   medium                       mean   std     n    se\n##   &lt;chr&gt;                       &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n## 1 control                      10.1 0.716    10 0.226\n## 2 sugar added                  10.2 0.818    10 0.259\n## 3 sugar and amino acids added  11.4 1.18     10 0.373\n\n\n14.3.2 Apply lm()\n\nWe can create a one-way ANOVA model like this:\n\nmod &lt;- lm(data = culture, diameter ~ medium)\n\nAnd examine the model with:\n\nsummary(mod)\n## \n## Call:\n## lm(formula = diameter ~ medium, data = culture)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -1.541 -0.700 -0.080  0.424  1.949 \n## \n## Coefficients:\n##                                   Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)                        10.0700     0.2930  34.370  &lt; 2e-16 ***\n## mediumsugar added                   0.1700     0.4143   0.410  0.68483    \n## mediumsugar and amino acids added   1.3310     0.4143   3.212  0.00339 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.9265 on 27 degrees of freedom\n## Multiple R-squared:  0.3117, Adjusted R-squared:  0.2607 \n## F-statistic: 6.113 on 2 and 27 DF,  p-value: 0.00646\n\nThe Estimates in the Coefficients table give:\n\n(Intercept) known as \\(\\beta_0\\). The mean of the control group (Figure¬†14.2). Just as the intercept is the value of the y (the response) when the value of x (the explanatory) is zero in a simple linear regression, this is the value of diameter when the medium is at its first level. The order of the levels is alphabetical by default.\nmediumsugar added known as \\(\\beta_1\\). This is what needs to be added to the mean of the control group to get the mean of the ‚Äòmedium sugar added‚Äô group (Figure¬†13.3). Just as the slope is amount of y that needs to be added for each unit of x in a simple linear regression, this is the amount of diameter that needs to be added when the medium goes from its first level to its second level (i.e., one unit). The mediumsugar added estimate is positive so the the ‚Äòmedium sugar added‚Äô group mean is higher than the control group mean\nmediumsugar and amino acids added known as \\(\\beta_2\\) is what needs to be added to the mean of the control group to get the mean of the ‚Äòmedium sugar and amino acids added‚Äô group (Figure¬†13.3). Note that it is the amount added to the intercept (the control in this case). The mediumsugar and amino acids added estimate is positive so the the ‚Äòmedium sugar and amino acids added‚Äô group mean is higher than the control group mean\n\nIf we had more groups, we would have more estimates and all would be compared to the control group mean.\nThe p-values on each line are tests of whether that coefficient is different from zero.\n\n\n(Intercept)                     10.0700     0.2930  34.370  &lt; 2e-16 *** tells us that the control group mean is significantly different from zero. This is not a very interesting, it just means the control colonies have a diameter.\n\nmediumsugar added                 0.1700     0.4143   0.410  0.68483 tells us that the ‚Äòmedium sugar added‚Äô group mean is not significantly different from the control group mean.\n\nmediumsugar and amino acids added   1.3310     0.4143   3.212  0.00339 ** tells us that the ‚Äòmedium sugar and amino acids added‚Äô group mean is significantly different from the control group mean.\n\nNote: none of this output tells us whether the medium sugar and amino acids added‚Äô group mean is significantly different from the ‚Äòmedium sugar added‚Äô group mean. We need to do a post-hoc test for that.\nThe F value and p-value in the last line are a test of whether the model as a whole explains a significant amount of variation in the response variable.\n\n\n\n\n\n\n\nFigure¬†14.2: In an one-way ANOVA model with three groups, the first estimate is the intercept which is the mean of the first group. The second estimate is the ‚Äòslope‚Äô which is what has to added to the intercept to get the second group mean. The third estimate is the ‚Äòslope‚Äô which is what has to added to the intercept to get the third group mean. Note that y axis starts at 15 to create more space for the annotations.\n\n\n\n\nThe ANOVA is significant but this only tells us that growth medium matters, meaning at least two of the means differ. To find out which means differ, we need a post-hoc test. A post-hoc (‚Äúafter this‚Äù) test is done after a significant ANOVA test. There are several possible post-hoc tests and we will be using Tukey‚Äôs HSD (honestly significant difference) test (Tukey 1949) implemented in the emmeans (Lenth 2023) package.\nWe need to load the package:\n\nlibrary(emmeans)\n\nThen carry out the post-hoc test:\n\nemmeans(mod, ~ medium) |&gt; pairs()\n##  contrast                                  estimate    SE df t.ratio p.value\n##  control - sugar added                        -0.17 0.414 27  -0.410  0.9117\n##  control - sugar and amino acids added        -1.33 0.414 27  -3.212  0.0092\n##  sugar added - sugar and amino acids added    -1.16 0.414 27  -2.802  0.0244\n## \n## P value adjustment: tukey method for comparing a family of 3 estimates\n\nEach row is a comparison between the two means in the ‚Äòcontrast‚Äô column. The ‚Äòestimate‚Äô column is the difference between those means and the ‚Äòp.value‚Äô indicates whether that difference is significant.\nA plot can be used to visualise the result of the post-hoc which can be especially useful when there are very many comparisons.\n\nemmeans(mod, ~ medium) |&gt; plot()\n\n\n\n\n\n\n\nWhere the purple bars overlap, there is no significant difference.\nWe have found that colony diameters are significantly greater when sugar and amino acids are added but that adding sugar alone does not significantly increase colony diameter.\n\n14.3.3 Check assumptions\nCheck the assumptions: All general linear models assume the ‚Äúresiduals‚Äù are normally distributed and have ‚Äúhomogeneity‚Äù of variance.\nOur first check of these assumptions is to use common sense: diameter is a continuous and we would expect it to be normally distributed thus we would expect the residuals to be normally distributed thus we would expect the residuals to be normally distributed\nWe then proceed by plotting residuals. The plot() function can be used to plot the residuals against the fitted values (See Figure¬†14.3). This is a good way to check for homogeneity of variance.\n\nplot(mod, which = 1)\n\n\n\n\n\n\nFigure¬†14.3: A plot of the residuals against the fitted values shows whether the points are distributed similarly in each group. Any difference seems small but perhaps the residuals are more variable for the highest mean.\n\n\n\n\nPerhaps the variance is higher for the highest mean?\nWe can also use a histogram to check for normality (See Figure¬†14.4).\n\nggplot(mapping = aes(x = mod$residuals)) + \n  geom_histogram(bins = 8)\n\n\n\n\n\n\nFigure¬†14.4: A histogram of residuals is symetrical and seems consistent with a normal distribution. This is a good sign for the assumption of normally distributed residuals.\n\n\n\n\nFinally, we can use the Shapiro-Wilk test to test for normality.\n\nshapiro.test(mod$residuals)\n## \n##  Shapiro-Wilk normality test\n## \n## data:  mod$residuals\n## W = 0.96423, p-value = 0.3953\n\nThe p-value is greater than 0.05 so this test of the normality assumption is not significant.\nTaken together, these results suggest that the assumptions of normality and homogeneity of variance are probably not violated.\n\n14.3.4 Report\nThere was a significant effect of media on the diameter of bacterial colonies (F = 6.11; d.f. = 2, 27; p = 0.006). Post-hoc testing with Tukey‚Äôs Honestly Significant Difference test (Tukey 1949) revealed the colony diameters were significantly larger when grown with both sugar and amino acids (\\(\\bar{x} \\pm s.e\\): 11.4 \\(\\pm\\) 0.37 mm) than with neither (10.2 \\(\\pm\\) 0.26 mm; p = 0.0092) or just sugar (10.1 \\(\\pm\\) 0.23 mm; p = 0.0244). See Figure¬†14.5.\nCodeggplot() +\n  geom_point(data = culture, aes(x = medium, y = diameter),\n             position = position_jitter(width = 0.1, height = 0),\n             colour = \"gray50\") +\n  geom_errorbar(data = culture_summary, \n                aes(x = medium, ymin = mean - se, ymax = mean + se),\n                width = 0.3) +\n  geom_errorbar(data = culture_summary, \n                aes(x = medium, ymin = mean, ymax = mean),\n                width = 0.2) +\n  scale_y_continuous(name = \"Diameter (mm)\", \n                     limits = c(0, 16.5), \n                     expand = c(0, 0)) +\n  scale_x_discrete(name = \"Medium\", \n                   labels = c(\"Control\", \n                              \"Sugar added\", \n                              \"Sugar and amino acids added\")) +\n  annotate(\"segment\", x = 2, xend = 3, \n           y = 14, yend = 14,\n           colour = \"black\") +\n  annotate(\"text\", x = 2.5,  y = 14.5, \n           label = expression(italic(p)~\"= 0.0244\")) +\n    annotate(\"segment\", x = 1, xend = 3, \n           y = 15.5, yend = 15.5,\n           colour = \"black\") +\n  annotate(\"text\", x = 2,  y = 16, \n           label = expression(italic(p)~\"= 0.0092\")) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†14.5: Medium affects bacterial colony diameter. Ten replicate colonies were grown on three types of media: control, with sugar added and with both sugar and amino acids added. Error bars are means \\(\\pm\\) 1 standard error. There was a significant effect of media on the diameter of bacterial colonies (F = 6.11; d.f. = 2, 27; p = 0.006). Post-hoc testing with Tukey‚Äôs Honestly Significant Difference test (Tukey 1949) revealed the colony diameters were significantly larger when grown with both sugar and amino acids than with neither or just sugar. Data analysis was conducted in R (R Core Team 2024) with tidyverse packages (Wickham et al. 2019).",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>One-way ANOVA and Kruskal-Wallis</span>"
    ]
  },
  {
    "objectID": "two_way_anova.html",
    "href": "two_way_anova.html",
    "title": "\n15¬† Two-way ANOVA\n",
    "section": "",
    "text": "15.1 Overview\nIn the last chapter, we learnt how to use and interpret the general linear model when the x variable was categorical with two or more groups. This procedure is known as one-way ANOVA. We will now incorporate a second categorical variable. This is often known as the two-way or two-factor ANOVA (analysis of variance). It might also be described as a ‚Äúfactorial design‚Äù.\nIn the last chapter we conducted a one-way ANOVA to test whether there was an of media on the diameter of bacterial colonies. Suppose we had run this experiment on more than one bacterial species. We would then have two explanatory variables: media and species. Perhaps we should do two one-way ANOVAs, one for each explanatory variable? This would tell us whether each explanatory variable had an effect on the response variable. However, it would not tell us whether there was an interaction between the two variables. An interaction is when the effect of one variable depends on the level of another and in some ways it is the most interesting result we could reveal. For example, we might find that the optimal media for one species was not the optimal media for the other species. A two-way ANOVA allows us to test for the effects of each explanatory variable and whether they interact. A two-way ANOVA has three null hypotheses:\nAs a General linear model, two-way ANOVA is a parametric test with assumptions based on the normal distribution which need to be met for the p-values generated to be accurate. We use lm() to fit the model, anova() to test the significance of the explanatory variables and emmeans() and pairs() for post-hoc testing.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Two-way ANOVA</span>"
    ]
  },
  {
    "objectID": "two_way_anova.html#overview",
    "href": "two_way_anova.html#overview",
    "title": "\n15¬† Two-way ANOVA\n",
    "section": "",
    "text": "There is no effect of media on diameter, or there is no difference between the mean diameter of colonies grown on different media.\nThere is no effect of species on diameter, or there is no difference between bacteria in colony diameter.\nThe effect of media is the same for all species, or the effect of media does not depend on species.\n\n\n\n15.1.1 Model assumptions\nWe examine the model assumptions in the same way as we did for single linear regression, two-sample tests and one-way ANOVA: we apply the general linear model with lm() and then check the assumptions using diagnostic plots and the Shapiro-Wilk normality test. We also use common sense:\n\nthe response should be continuous (or nearly continuous, see Ideas about data: Theory and practice) and we consider whether we would expect the response to be normally distributed.\nwe expect decimal places and few repeated values.\n\n15.1.2 Reporting\nIn reporting the result of two-way ANOVA, we include:\n\n\nthe significance of each of the explanatory variables and the interaction between them.\n\nThe F-statistic and p-value for each explanatory variable and the interaction term.\n\n\n\nthe direction of effect - which of the means is greater\n\nPost-hoc test\n\n\n\nthe magnitude of effect - how big is the difference between the means\n\nthe means and standard errors for each group\n\n\n\nFigures should reflect what you have said in the statements. Ideally they should show both the raw data and the statistical model: means and standard errors\n\n15.1.3 When the assumptions are not met\n\n15.1.3.1 When residuals are not normally distributed\nThere are a few options for dealing with non-normally distributed residuals when you wanted to do a two-way ANOVA. Transforming your data might help - but it depends on how the data are not normal - if there are lots of values the same for example, no transformation helps! If the data have positive skew (tail in the positive direction) then logging can help.\n\n15.1.3.2 When residuals are not normally distributed and/or variance is unequal\nThere are some rank-based tests, such as the Friedman test and the Scheirer-Ray-Hare test, that apply to specific situations. However, there is no non-parametric alternative that directly corresponds to two-way ANOVA in the same way that the Kruskal-Wallis test does for one-way ANOVA.\nIn these cases you have other good options.\n\nUse two-way ANOVA carefully. Using the two-way ANOVA test and interpreting the results with caution is one valid approach. ANOVA is actually quite robust to violating the normality assumption (it performs quite well anyway) depending on how much the residuals deviate.\nUsing non-parametric one-way tests. Divide the dataset according to the groups in one factor, then carry out a Wilcoxon rank-sum test (also known as the Mann-Whitney test) or a Kruskal-Wallis test on each subset for the other factor. For example, if the two factors are Fertilizer Type (Organic, Chemical, Control) and Soil Moisture (Low, Medium, High), split the dataset by Fertilizer Type. Within each subset, perform a Wilcoxon rank-sum test if the second factor has two levels or a Kruskal-Wallis test if it has more than two levels to determine whether the second factor affects the response variable. While there is no direct test for an interaction, you can infer one. If the results are consistent across all subsets, meaning the second factor has the same effect regardless of the first factor, there is no evidence of an interaction. However, if the effect of the second factor varies between subsets, this suggests an interaction, as the influence of one factor depends on the level of the other.\n\nWe will explore all of these ideas with some examples.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Two-way ANOVA</span>"
    ]
  },
  {
    "objectID": "two_way_anova.html#your-turn",
    "href": "two_way_anova.html#your-turn",
    "title": "\n15¬† Two-way ANOVA\n",
    "section": "\n15.2 üé¨ Your turn!",
    "text": "15.2 üé¨ Your turn!\nIf you want to code along you will need to start a new RStudio project, add a data-raw folder and open a new script. You will also need to load the tidyverse package (Wickham et al. 2019).",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Two-way ANOVA</span>"
    ]
  },
  {
    "objectID": "two_way_anova.html#two-way-anova",
    "href": "two_way_anova.html#two-way-anova",
    "title": "\n15¬† Two-way ANOVA\n",
    "section": "\n15.3 Two-way ANOVA",
    "text": "15.3 Two-way ANOVA\nResearchers have collected live specimens of two species of periwinkle (See Figure¬†15.1) from sites in northern England in the Spring and Summer. They took a measure of the gut parasite load by examining a slide of gut contents. The data are in periwinkle.txt. The data were collected to determine whether there was an effect of season or species on parasite load and whether these effects were independent.\n\n\n\n\n\nFigure¬†15.1: Periwinkles are marine gastropod molluscs (slugs and snails). A) Littorina brevicula (PD files - Public Domain, https://commons.wikimedia.org/w/index.php?curid=30577419) B) Littorina littorea. (photographed by Guttorm Flatab√∏ (user:dittaeva). - Photograph taken with an Olympus Camedia C-70 Zoom digital camera. Metainformation edited with Irfanview, possibly cropped with jpegcrop., CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=324769\n\n\n\n15.3.1 Import and explore\nImport the data:\n\nperiwinkle &lt;- read_delim(\"data-raw/periwinkle.txt\", delim = \"\\t\")\n\n\n\n\n\n\npara\nseason\nspecies\n\n\n\n58.86\nSpring\nLittorina brevicula\n\n\n51.73\nSpring\nLittorina brevicula\n\n\n54.99\nSpring\nLittorina brevicula\n\n\n39.84\nSpring\nLittorina brevicula\n\n\n65.05\nSpring\nLittorina brevicula\n\n\n67.46\nSpring\nLittorina brevicula\n\n\n60.38\nSpring\nLittorina brevicula\n\n\n54.42\nSpring\nLittorina brevicula\n\n\n47.71\nSpring\nLittorina brevicula\n\n\n66.87\nSpring\nLittorina brevicula\n\n\n51.02\nSpring\nLittorina brevicula\n\n\n43.95\nSpring\nLittorina brevicula\n\n\n62.40\nSpring\nLittorina brevicula\n\n\n55.67\nSpring\nLittorina brevicula\n\n\n58.45\nSpring\nLittorina brevicula\n\n\n43.52\nSpring\nLittorina brevicula\n\n\n69.29\nSpring\nLittorina brevicula\n\n\n71.22\nSpring\nLittorina brevicula\n\n\n64.60\nSpring\nLittorina brevicula\n\n\n58.53\nSpring\nLittorina brevicula\n\n\n51.15\nSpring\nLittorina brevicula\n\n\n70.62\nSpring\nLittorina brevicula\n\n\n55.10\nSpring\nLittorina brevicula\n\n\n47.00\nSpring\nLittorina brevicula\n\n\n54.48\nSpring\nLittorina brevicula\n\n\n43.52\nSpring\nLittorina littorea\n\n\n63.26\nSpring\nLittorina littorea\n\n\n58.76\nSpring\nLittorina littorea\n\n\n61.38\nSpring\nLittorina littorea\n\n\n63.70\nSpring\nLittorina littorea\n\n\n45.69\nSpring\nLittorina littorea\n\n\n45.20\nSpring\nLittorina littorea\n\n\n77.23\nSpring\nLittorina littorea\n\n\n57.12\nSpring\nLittorina littorea\n\n\n49.32\nSpring\nLittorina littorea\n\n\n72.92\nSpring\nLittorina littorea\n\n\n47.95\nSpring\nLittorina littorea\n\n\n78.70\nSpring\nLittorina littorea\n\n\n77.01\nSpring\nLittorina littorea\n\n\n71.70\nSpring\nLittorina littorea\n\n\n69.36\nSpring\nLittorina littorea\n\n\n66.46\nSpring\nLittorina littorea\n\n\n76.35\nSpring\nLittorina littorea\n\n\n75.55\nSpring\nLittorina littorea\n\n\n54.59\nSpring\nLittorina littorea\n\n\n62.02\nSpring\nLittorina littorea\n\n\n58.50\nSpring\nLittorina littorea\n\n\n76.09\nSpring\nLittorina littorea\n\n\n68.85\nSpring\nLittorina littorea\n\n\n84.55\nSpring\nLittorina littorea\n\n\n61.60\nSummer\nLittorina brevicula\n\n\n70.84\nSummer\nLittorina brevicula\n\n\n68.89\nSummer\nLittorina brevicula\n\n\n67.68\nSummer\nLittorina brevicula\n\n\n88.68\nSummer\nLittorina brevicula\n\n\n64.64\nSummer\nLittorina brevicula\n\n\n69.71\nSummer\nLittorina brevicula\n\n\n70.65\nSummer\nLittorina brevicula\n\n\n56.85\nSummer\nLittorina brevicula\n\n\n80.06\nSummer\nLittorina brevicula\n\n\n80.93\nSummer\nLittorina brevicula\n\n\n53.62\nSummer\nLittorina brevicula\n\n\n56.77\nSummer\nLittorina brevicula\n\n\n101.81\nSummer\nLittorina brevicula\n\n\n81.65\nSummer\nLittorina brevicula\n\n\n88.73\nSummer\nLittorina brevicula\n\n\n67.50\nSummer\nLittorina brevicula\n\n\n73.75\nSummer\nLittorina brevicula\n\n\n75.80\nSummer\nLittorina brevicula\n\n\n74.70\nSummer\nLittorina brevicula\n\n\n68.26\nSummer\nLittorina brevicula\n\n\n89.29\nSummer\nLittorina brevicula\n\n\n75.09\nSummer\nLittorina brevicula\n\n\n74.99\nSummer\nLittorina brevicula\n\n\n76.03\nSummer\nLittorina brevicula\n\n\n66.67\nSummer\nLittorina littorea\n\n\n61.31\nSummer\nLittorina littorea\n\n\n51.00\nSummer\nLittorina littorea\n\n\n67.43\nSummer\nLittorina littorea\n\n\n78.12\nSummer\nLittorina littorea\n\n\n79.26\nSummer\nLittorina littorea\n\n\n69.03\nSummer\nLittorina littorea\n\n\n76.14\nSummer\nLittorina littorea\n\n\n86.81\nSummer\nLittorina littorea\n\n\n97.56\nSummer\nLittorina littorea\n\n\n64.64\nSummer\nLittorina littorea\n\n\n68.64\nSummer\nLittorina littorea\n\n\n65.95\nSummer\nLittorina littorea\n\n\n49.50\nSummer\nLittorina littorea\n\n\n62.22\nSummer\nLittorina littorea\n\n\n57.34\nSummer\nLittorina littorea\n\n\n70.30\nSummer\nLittorina littorea\n\n\n62.75\nSummer\nLittorina littorea\n\n\n80.48\nSummer\nLittorina littorea\n\n\n81.74\nSummer\nLittorina littorea\n\n\n85.83\nSummer\nLittorina littorea\n\n\n77.51\nSummer\nLittorina littorea\n\n\n61.12\nSummer\nLittorina littorea\n\n\n59.83\nSummer\nLittorina littorea\n\n\n66.52\nSummer\nLittorina littorea\n\n\n\n\n\n\n\nThe Response variable is parasite load and it appears to be continuous. The Explanatory variables are species and season and each has two levels. It is known ‚Äútwo-way ANOVA‚Äù or ‚Äútwo-factor ANOVA‚Äù because there are two explanatory variables.\nThese data are in tidy format (Wickham 2014) - all the parasite load values are in one column (para) with the other columns indicating the species and the season. This means they are well formatted for analysis and plotting.\nIn the first instance it is sensible to create a rough plot of our data. This is to give us an overview and help identify if there are any issues like missing or extreme values. It also gives us idea what we are expecting from the analysis which will make it easier for us to identify if we make some mistake in applying that analysis.\nViolin plots (geom_violin(), see Figure¬†15.2), box plots (geom_boxplot()) or scatter plots (geom_point()) all make good choices for exploratory plotting and it does not matter which of these you choose.\n\nggplot(data = periwinkle, \n       aes(x = season, y = para, fill = species)) +\n  geom_violin()\n\n\n\n\n\n\nFigure¬†15.2: The parasite load for two species of Littorina indicated by the fill colour, in the Spring and Summer. Parasite load seems to be higher for both species in the summer and that effect looks bigger in L.brevicula - it has the lowest spring mean but the highest summer mean.\n\n\n\n\nR will order the groups alphabetically by default.\nThe figure suggests that parasite load is higher for both species in the summer and that effect looks bigger in L.brevicula - it has the lowest spring mean but the highest summer mean.\nSummarising the data for each species-season combination is the next sensible step. The most useful summary statistics are the means, standard deviations, sample sizes and standard errors. I recommend the group_by() and summarise() approach:\n\nperi_summary &lt;- periwinkle |&gt;  \n  group_by(season, species) |&gt;  \n  summarise(mean = mean(para),\n            sd = sd(para),\n            n = length(para),\n            se = sd / sqrt(n))\n\nWe have save the results to peri_summary so that we can use the means and standard errors in our plot later.\n\nperi_summary\n## # A tibble: 4 √ó 6\n## # Groups:   season [2]\n##   season species              mean    sd     n    se\n##   &lt;chr&gt;  &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n## 1 Spring Littorina brevicula  57.0  8.83    25  1.77\n## 2 Spring Littorina littorea   64.2 11.9     25  2.38\n## 3 Summer Littorina brevicula  73.5 11.2     25  2.24\n## 4 Summer Littorina littorea   69.9 11.5     25  2.30\n\nThe summary confirms both species have a higher mean in the summer and that the difference between the species is reversed - L.brevicula minus L.littorea is -7.2588 in the spring but 3.6328 in summer.\n\n15.3.2 Apply lm()\n\nWe can create a two-way ANOVA model like this:\n\nmod &lt;- lm(data = periwinkle, para ~ species * season)\n\nAnd examine the model with:\n\nsummary(mod)\n## \n## Call:\n## lm(formula = para ~ species * season, data = periwinkle)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -20.711  -6.308  -1.120   8.085  28.269 \n## \n## Coefficients:\n##                                        Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)                              56.972      2.185  26.073  &lt; 2e-16 ***\n## speciesLittorina littorea                 7.259      3.090   2.349   0.0209 *  \n## seasonSummer                             16.568      3.090   5.362 5.68e-07 ***\n## speciesLittorina littorea:seasonSummer  -10.892      4.370  -2.492   0.0144 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 10.93 on 96 degrees of freedom\n## Multiple R-squared:  0.2547, Adjusted R-squared:  0.2314 \n## F-statistic: 10.94 on 3 and 96 DF,  p-value: 3.043e-06\n\nThe Estimates in the Coefficients table give:\n\n(Intercept) known as \\(\\beta_0\\). This the mean of the group with the first level of both explanatory variables, i.e., the mean of L.brevicula group in Spring. Just as the intercept is the value of y (the response) when the value of x (the explanatory) is zero in a simple linear regression, this is the value of para when both season and species are at their first levels. The order of the levels is alphabetical by default.\nspeciesLittorina littorea known as \\(\\beta_1\\). This is what needs to be added to the L.brevicula Spring mean (the intercept) when the species goes from its first level to its second. That is, it is the difference between the L.brevicula and L.littorea means in Spring. The speciesLittorina littorea estimate is positive so the the L.littorea mean is higher than the L.brevicula mean in Spring. If we had more species we would have more estimates beginning species... and all would be comparisons to the intercept.\nseasonSummer known as \\(\\beta_2\\). This is what needs to be added to the L.brevicula Spring mean (the intercept) when the season goes from its first level to its second. That is, it is the difference between the Spring and Summer means for L.brevicula. The seasonSummer estimate is positive so the the Summer mean is higher than the Spring mean. If we had more seasons we would have more estimates beginning season... and all would be comparisons to the intercept.\nspeciesLittorina littorea:seasonSummer known as \\(\\beta_3\\). This is interaction effect. It is an additional effect. Going from L.brevicula to L.littorea adds \\(\\beta_1\\) to the intercept. Going from Spring to Summer adds \\(\\beta_2\\) to the intercept. Going from L.brevicula in Spring to L.littorea in Summer adds \\(\\beta_1 + \\beta_2 + \\beta_3\\) to the intercept. If \\(\\beta_3\\) is zero then the effect of species is the same in both seasons. If \\(\\beta_3\\) is not zero then the effect of species is different in the two seasons.\n\nThe p-values on each line are tests of whether that coefficient is different from zero.\nThe F value and p-value in the last line are a test of whether the model as a whole explains a significant amount of variation in the response variable. The model of season and species overall explains a significant amount of the variation in parasite load (p-value: 3.043e-06). To see which of the three effects are significant we can use the anova() function on our model.\nDetermine which effects are significant:\n\nanova(mod)\n## Analysis of Variance Table\n## \n## Response: para\n##                Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \n## species         1    82.2   82.17  0.6884   0.40876    \n## season          1  3092.8 3092.81 25.9108 1.778e-06 ***\n## species:season  1   741.4  741.42  6.2114   0.01441 *  \n## Residuals      96 11458.9  119.36                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe parasite load is significantly greater in Summer (F = 25.9; d.f. = 1, 96; p &lt; 0.0001) but this effect differs between species (F = 6.2; d.f. = 1,96; p = 0.014) with a greater increase in parasite load in L.brevicula than in L.littorea.\nWe need a post-hoc test to see which comparisons are significant and can again use then emmeans (Lenth 2023) package.\nLoad the package\n\nlibrary(emmeans)\n\nCarry out the post-hoc test\n\nemmeans(mod, ~ species * season) |&gt; pairs()\n##  contrast                                                estimate   SE df\n##  Littorina brevicula Spring - Littorina littorea Spring     -7.26 3.09 96\n##  Littorina brevicula Spring - Littorina brevicula Summer   -16.57 3.09 96\n##  Littorina brevicula Spring - Littorina littorea Summer    -12.94 3.09 96\n##  Littorina littorea Spring - Littorina brevicula Summer     -9.31 3.09 96\n##  Littorina littorea Spring - Littorina littorea Summer      -5.68 3.09 96\n##  Littorina brevicula Summer - Littorina littorea Summer      3.63 3.09 96\n##  t.ratio p.value\n##   -2.349  0.0943\n##   -5.362  &lt;.0001\n##   -4.186  0.0004\n##   -3.013  0.0172\n##   -1.837  0.2625\n##    1.176  0.6436\n## \n## P value adjustment: tukey method for comparing a family of 4 estimates\n\nEach row is a comparison between the two means in the ‚Äòcontrast‚Äô column. The ‚Äòestimate‚Äô column is the difference between those means and the ‚Äòp.value‚Äô indicates whether that difference is significant.\nA plot can be used to visualise the result of the post hoc which can be especially useful when there are very many comparisons.\nPlot the results of the post-hoc test:\n\nemmeans(mod, ~ species * season) |&gt; plot()\n\n\n\n\n\n\n\nWe have significant differences between:\n\n\nL.brevicula in the Spring and Summer p &lt;.0001\n\n\nL.brevicula in the Spring and L.littorea in the Summer p = 0.0004\n\n\nL.littorea in the Spring L.brevicula in the Summer p = 0.0172\n\n\n15.3.3 Check assumptions\nCheck the assumptions: All general linear models assume the ‚Äúresiduals‚Äù are normally distributed and have ‚Äúhomogeneity‚Äù of variance.\nOur first check of these assumptions is to use common sense: diameter is a continuous and we would expect it to be normally distributed thus we would expect the residuals to be normally distributed thus we would expect the residuals to be normally distributed\nWe then proceed by plotting residuals. The plot() function can be used to plot the residuals against the fitted values (See Figure¬†15.3). This is a good way to check for homogeneity of variance.\n\nplot(mod, which = 1)\n\n\n\n\n\n\nFigure¬†15.3: A plot of the residuals against the fitted values shows whether the points are distributed similarly in each group. Any difference seems small but perhaps the residuals are less variable for the lowest mean.\n\n\n\n\nWe can also use a histogram to check for normality (See Figure¬†15.4).\n\nggplot(mapping = aes(x = mod$residuals)) + \n  geom_histogram(bins = 10)\n\n\n\n\n\n\nFigure¬†15.4: A histogram of residuals is symetrical and seems consistent with a normal distribution. This is a good sign for the assumption of normally distributed residuals.\n\n\n\n\nFinally, we can use the Shapiro-Wilk test to test for normality.\n\nshapiro.test(mod$residuals)\n## \n##  Shapiro-Wilk normality test\n## \n## data:  mod$residuals\n## W = 0.98424, p-value = 0.2798\n\nThe p-value is greater than 0.05 so this test of the normality assumption is not significant.\nTaken together, these results suggest that the assumptions of normality and homogeneity of variance are probably not violated.\n\n15.3.4 Report\nWe might report this result as:\nThe parasite load was significantly greater in Summer (F = 25.9; d.f. = 1, 96; p &lt; 0.0001) but this effect differed between species (F = 6.2; d.f. = 1,96; p = 0.014) with a greater increase in parasite load in L.brevicula (from \\(\\bar{x} \\pm s.e\\): 57.0 \\(\\pm\\) 1.77 units to 73.5 \\(\\pm\\) 2.24 units) than in L.littorea (from 64.2 \\(\\pm\\) 2.38 units to 69.9 \\(\\pm\\) 2.30 units). See Figure¬†15.5.\nCodeggplot() +\n  geom_point(data = periwinkle, aes(x = season,\n                                    y = para,\n                                    shape = species),\n             position = position_jitterdodge(dodge.width = 1,\n                                             jitter.width = 0.4,\n                                             jitter.height = 0),\n             size = 3,\n             colour = \"gray50\") +\n  geom_errorbar(data = peri_summary, \n                aes(x = season, ymin = mean - se, ymax = mean + se, group = species),\n                linewidth = 0.4, size = 1,\n                position = position_dodge(width = 1)) +\n  geom_errorbar(data = peri_summary, \n                aes(x = season, ymin = mean, ymax = mean, group = species),\n                linewidth = 0.3, size = 1,\n                position = position_dodge(width = 1) ) +\n  scale_x_discrete(name = \"Season\") +\n  scale_y_continuous(name = \"Number of parasites\",\n                     expand = c(0, 0),\n                     limits = c(0, 140)) +\n  scale_shape_manual(values = c(19, 1),\n                     name = NULL,\n                     labels = c(bquote(italic(\"L.brevicula\")),\n                                bquote(italic(\"L.littorea\")))) +\n  # *L.brevicula* in the Spring and Summer `p &lt;0.0001`\n  annotate(\"segment\",\n           x = 0.75, xend = 1.75,\n           y = 115, yend = 115,\n           colour = \"black\") +\n  annotate(\"text\",\n           x = 1.25,  y = 119,\n           label = \"p &lt; 0.0001\") +\n  # # *L.brevicula* in the Spring and  *L.littorea* in the Summer `p = 0.0004`\n  annotate(\"segment\",\n           x = 0.75, xend = 2.25,\n           y = 125, yend = 125,\n           colour = \"black\") +\n  annotate(\"text\", x = 1.5,  y = 129,\n           label = \"p = 0.0004\") +\n  # *L.littorea* in the Spring *L.brevicula* in the Summer `p = 0.0172`\n  annotate(\"segment\",\n           x = 1.25, xend = 1.75,\n           y = 105, yend = 105,\n           colour = \"black\") +\n  annotate(\"text\", x = 1.5,  y = 109,\n           label = \"p = 0.0172\") +\n  theme_classic() +\n  theme(legend.title = element_blank(),\n        legend.position = c(0.85, 0.15)) \n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†15.5: Parasite load is greater in Summer especially for L.brevicula. Live specimens of two species of periwinkle, L.brevicula and L.littorea were collected from sites in northern England in the Spring and Summer and gut parasite load was determined. Error bars are means \\(\\pm\\) 1 standard error. Parasite load was significantly greater in Summer (F = 25.9; d.f. = 1, 96; p &lt; 0.0001) but this effect differed between species (F = 6.2; d.f. = 1,96; p = 0.014) with a greater increase in parasite load in L.brevicula than in L.littorea. Data analysis was conducted in R (R Core Team 2024) with tidyverse packages (Wickham et al. 2019). Post-hoc analysis was carried out with Tukey‚Äôs Honestly Significant Difference test (Tukey 1949).",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Two-way ANOVA</span>"
    ]
  },
  {
    "objectID": "two_way_anova.html#summary",
    "href": "two_way_anova.html#summary",
    "title": "\n15¬† Two-way ANOVA\n",
    "section": "\n15.4 Summary",
    "text": "15.4 Summary\n\nA linear model with two explanatory variable with two or more groups each is also known as a two-way ANOVA.\n\nWe estimate the coefficients (also called the parameters) of the model. For a two-way ANOVA with two groups in each explanatory variable these are\n\nthe mean of the group with the first level of both explanatory variables \\(\\beta_0\\),\nwhat needs to be added to \\(\\beta_0\\) when one of the explanatory variables goes from its first level to its second, \\(\\beta_1\\)\n\nwhat needs to be added to \\(\\beta_0\\) when the other of the explanatory variables goes from its first level to its second, \\(\\beta_2\\)\n\n\n\\(\\beta_3\\), the interaction effect: what additionally needs to be added to \\(\\beta_0\\) + \\(\\beta_1\\) + \\(\\beta_2\\) when both of the explanatory variables go from their first level to their second\n\n\nWe can use lm() to two-way ANOVA in R.\nIn the output of lm() the coefficients are listed in a table in the Estimates column. The p-value for each coefficient is in the test of whether it differs from zero. At the bottom of the output there is an \\(F\\) test of the model overall. The R-squared value is the proportion of the variance in the response variable that is explained by the model.\nTo see which of the three effects are significant we can use the anova() function on our model.\nTo find out which means differ, we need a post-hoc test. Here we use Tukey‚Äôs HSD applied with the emmeans() and pairs() functions from the emmeans package. Post-hoc tests make adjustments to the p-values to account for the fact that we are doing multiple tests.\nThe assumptions of the general linear model are that the residuals are normally distributed and have homogeneity of variance. A residual is the difference between the predicted value and the observed value.\nWe examine a histogram of the residuals and use the Shapiro-Wilk normality test to check the normality assumption. We check the variance of the residuals is the same for all fitted values with a residuals vs fitted plot.\nWhen reporting the results of a test we give the significance, direction and size of the effect. We use means and standard errors for parametric tests like two-way ANOVA. We also give the test statistic, the degrees of freedom (parametric) or sample size (non-parametric) and the p-value. We annotate our figures with the p-value, making clear which comparison it applies to.\n\n\n\n\n\nLenth, Russell V. 2023. ‚ÄúEmmeans: Estimated Marginal Means, Aka Least-Squares Means.‚Äù https://CRAN.R-project.org/package=emmeans.\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nTukey, John W. 1949. ‚ÄúComparing Individual Means in the Analysis of Variance.‚Äù Biometrics 5 (2): 99‚Äì114. https://doi.org/10.2307/3001913.\n\n\nWickham, Hadley. 2014. ‚ÄúTidy Data.‚Äù Journal of Statistical Software, Articles 59 (10): 1‚Äì23. https://vita.had.co.nz/papers/tidy-data.pdf.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D‚ÄôAgostino McGowan, Romain Fran√ßois, Garrett Grolemund, et al. 2019. ‚ÄúWelcome to the Tidyverse‚Äù 4: 1686. https://doi.org/10.21105/joss.01686.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Two-way ANOVA</span>"
    ]
  },
  {
    "objectID": "association.html",
    "href": "association.html",
    "title": "\n16¬† Association: Correlation and Contingency\n",
    "section": "",
    "text": "16.1 Overview\nUp to this point, we have focused on methods that examine how one variable affects another. Regression allowed us to model a relationship where one variable was explicitly treated as explanatory and the other as a response. Similarly, ANOVA and two-sample tests compared groups to see if a categorical explanatory variable influenced a numeric response.\nIn this chapter, we shift our focus to situations where no causal relationship is assumed. Instead of asking whether one variable predicts another, we ask whether two variables are associated - whether they tend to change together.\nWe will cover:\nUnlike regression, these methods do not imply that one variable influences the other - they simply measure the strength of an association. This allows us to explore patterns in data when no clear explanatory-response relationship exists.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Association: Correlation and Contingency</span>"
    ]
  },
  {
    "objectID": "association.html#overview",
    "href": "association.html#overview",
    "title": "\n16¬† Association: Correlation and Contingency\n",
    "section": "",
    "text": "Pearson‚Äôs correlation, which measures how strongly two continuous variables change together in a linear way.\n\nSpearman‚Äôs correlation, a non-parametric alternative\n\nChi-square tests for contingency tables, which assess whether two categorical variables are related.\n\n\n\n16.1.1 Correlation\nWhen working with two continuous variables, we may be interested in understanding how they are associated without assuming a cause-and-effect relationship. Unlike regression, where one variable is treated as explanatory and the other as a response, correlation simply measures the strength and direction of their association.\nThe correlation coefficient quantifies this relationship on a scale from -1 to 1. A coefficient of 1 indicates a perfect positive association, meaning that higher values in one variable are always linked to higher values in the other. Conversely, a coefficient of -1 represents a perfect negative association, where higher values in one variable are consistently associated with lower values in the other. Most real-world correlations fall somewhere between these extremes, with values closer to 0 suggesting little to no linear relationship (Figure¬†16.1).\nBy calculating the correlation coefficient, we can assess how strongly two variables move together, helping us uncover patterns in data without implying causation.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Association: Correlation and Contingency</span>"
    ]
  },
  {
    "objectID": "association.html#correlation-examples",
    "href": "association.html#correlation-examples",
    "title": "\n16¬† Association: Correlation and Contingency\n",
    "section": "\n16.2 Correlation Examples",
    "text": "16.2 Correlation Examples\n\n\n\n\n\n\n\nFigure¬†16.1: Scatter plots demonstrating different types of correlation. Left: A strong positive correlation (r ‚âà 0.8), where higher values in one variable are associated with higher values in the other. Middle: A strong negative correlation (r ‚âà -0.8), where higher values in one variable are associated with lower values in the other. Right: No correlation (r ‚âà 0), indicating no clear pattern between the two variables.\n\n\n\n\nWe estimate the population correlation coefficient, \\(\\rho\\) (pronounced rho), with the sample correlation coefficient, \\(r\\) and test whether it is significantly different from zero because a correlation coefficient of 0 means association.\nPearson‚Äôs correlation coefficient, is a parametric measure which assumes the variables are normally distributed and that any correlation is linear.\nSpearman‚Äôs rank correlation coefficient, is a non-parametric measure which does not assume the variables are normally distributed\nWe use cor.test() in R for both types of correlation.\n\n16.2.1 Contingency Chi-squared\n\ntwo categorical variables\nneither is an explanatory variable, i.e., there is not a causal relationship between the two variables\nwe count the number of observations in each caetory of each variable\nyou want to know if there is an association between the two variables\nanother way of describing this is that we test whether the proportion of observations falling in to each category of one variable is the same for each category of the other variable.\nwe use a chi-squared test to test whether the observed counts are significantly different from the expected counts if there was no association between the variables.\n\n16.2.2 Reporting\n\nthe significance of effect - whether the association is significant different from zero\n\nthe direction of effect\n\ncorrelation whether \\(r\\) is positive or negative\nwhich categories have higher than expected values\n\n\nthe magnitude of effect\n\nWe do not put a line of best fit on a scatter plot accompanying a correlation because such a line implies a causal relationship.\nWe will explore all of these ideas with an examples.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Association: Correlation and Contingency</span>"
    ]
  },
  {
    "objectID": "association.html#your-turn",
    "href": "association.html#your-turn",
    "title": "\n16¬† Association: Correlation and Contingency\n",
    "section": "\n16.3 üé¨ Your turn!",
    "text": "16.3 üé¨ Your turn!\nIf you want to code along you will need to start a new RStudio project, add a data-raw folder and open a new script. You will also need to load the tidyverse package (Wickham et al. 2019).",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Association: Correlation and Contingency</span>"
    ]
  },
  {
    "objectID": "association.html#correlation-1",
    "href": "association.html#correlation-1",
    "title": "\n16¬† Association: Correlation and Contingency\n",
    "section": "\n16.4 Correlation",
    "text": "16.4 Correlation\nHigh quality images of the internal structure of wheat seeds were taken with a soft X-ray technique. Seven measurements determined from the images:\n\nArea\nPerimeter\nCompactness\nKernel length\nKernel width\nAsymmetry coefficient\nLength of kernel groove\n\nResearch were interested in the correlation between compactness and kernel width\nThe data are in seeds-dataset.xlsx.\n\n16.4.1 Import and explore\nThe data are in an excel file so we will need the readxl (Wickham and Bryan 2023) package.\nLoad the package:\n\nlibrary(readxl)\n\nFind out the names of the sheets in the excel file:\n\nexcel_sheets(\"data-raw/seeds-dataset.xlsx\")\n## [1] \"seeds_dataset\"\n\nThere is only one sheet called seeds_dataset.\nImport the data:\n\nseeds &lt;- read_excel(\"data-raw/seeds-dataset.xlsx\",\n                    sheet = \"seeds_dataset\")\n\nNote that we could have omitted the sheet argument because there is only one sheet in the Excel file. However, it is good practice to be explicit.\n\n\n\n\n\narea\nperimeter\ncompactness\nkernal_length\nkernel_width\nasymmetry_coef\ngroove_length\n\n\n\n15.26\n14.84\n0.8710\n5.763\n3.312\n2.2210\n5.220\n\n\n14.88\n14.57\n0.8811\n5.554\n3.333\n1.0180\n4.956\n\n\n14.29\n14.09\n0.9050\n5.291\n3.337\n2.6990\n4.825\n\n\n13.84\n13.94\n0.8955\n5.324\n3.379\n2.2590\n4.805\n\n\n16.14\n14.99\n0.9034\n5.658\n3.562\n1.3550\n5.175\n\n\n14.38\n14.21\n0.8951\n5.386\n3.312\n2.4620\n4.956\n\n\n14.69\n14.49\n0.8799\n5.563\n3.259\n3.5860\n5.219\n\n\n14.11\n14.10\n0.8911\n5.420\n3.302\n2.7000\n5.000\n\n\n16.63\n15.46\n0.8747\n6.053\n3.465\n2.0400\n5.877\n\n\n16.44\n15.25\n0.8880\n5.884\n3.505\n1.9690\n5.533\n\n\n15.26\n14.85\n0.8696\n5.714\n3.242\n4.5430\n5.314\n\n\n14.03\n14.16\n0.8796\n5.438\n3.201\n1.7170\n5.001\n\n\n13.89\n14.02\n0.8880\n5.439\n3.199\n3.9860\n4.738\n\n\n13.78\n14.06\n0.8759\n5.479\n3.156\n3.1360\n4.872\n\n\n13.74\n14.05\n0.8744\n5.482\n3.114\n2.9320\n4.825\n\n\n14.59\n14.28\n0.8993\n5.351\n3.333\n4.1850\n4.781\n\n\n13.99\n13.83\n0.9183\n5.119\n3.383\n5.2340\n4.781\n\n\n15.69\n14.75\n0.9058\n5.527\n3.514\n1.5990\n5.046\n\n\n14.70\n14.21\n0.9153\n5.205\n3.466\n1.7670\n4.649\n\n\n12.72\n13.57\n0.8686\n5.226\n3.049\n4.1020\n4.914\n\n\n14.16\n14.40\n0.8584\n5.658\n3.129\n3.0720\n5.176\n\n\n14.11\n14.26\n0.8722\n5.520\n3.168\n2.6880\n5.219\n\n\n15.88\n14.90\n0.8988\n5.618\n3.507\n0.7651\n5.091\n\n\n12.08\n13.23\n0.8664\n5.099\n2.936\n1.4150\n4.961\n\n\n15.01\n14.76\n0.8657\n5.789\n3.245\n1.7910\n5.001\n\n\n16.19\n15.16\n0.8849\n5.833\n3.421\n0.9030\n5.307\n\n\n13.02\n13.76\n0.8641\n5.395\n3.026\n3.3730\n4.825\n\n\n12.74\n13.67\n0.8564\n5.395\n2.956\n2.5040\n4.869\n\n\n14.11\n14.18\n0.8820\n5.541\n3.221\n2.7540\n5.038\n\n\n13.45\n14.02\n0.8604\n5.516\n3.065\n3.5310\n5.097\n\n\n13.16\n13.82\n0.8662\n5.454\n2.975\n0.8551\n5.056\n\n\n15.49\n14.94\n0.8724\n5.757\n3.371\n3.4120\n5.228\n\n\n14.09\n14.41\n0.8529\n5.717\n3.186\n3.9200\n5.299\n\n\n13.94\n14.17\n0.8728\n5.585\n3.150\n2.1240\n5.012\n\n\n15.05\n14.68\n0.8779\n5.712\n3.328\n2.1290\n5.360\n\n\n16.12\n15.00\n0.9000\n5.709\n3.485\n2.2700\n5.443\n\n\n16.20\n15.27\n0.8734\n5.826\n3.464\n2.8230\n5.527\n\n\n17.08\n15.38\n0.9079\n5.832\n3.683\n2.9560\n5.484\n\n\n14.80\n14.52\n0.8823\n5.656\n3.288\n3.1120\n5.309\n\n\n14.28\n14.17\n0.8944\n5.397\n3.298\n6.6850\n5.001\n\n\n13.54\n13.85\n0.8871\n5.348\n3.156\n2.5870\n5.178\n\n\n13.50\n13.85\n0.8852\n5.351\n3.158\n2.2490\n5.176\n\n\n13.16\n13.55\n0.9009\n5.138\n3.201\n2.4610\n4.783\n\n\n15.50\n14.86\n0.8820\n5.877\n3.396\n4.7110\n5.528\n\n\n15.11\n14.54\n0.8986\n5.579\n3.462\n3.1280\n5.180\n\n\n13.80\n14.04\n0.8794\n5.376\n3.155\n1.5600\n4.961\n\n\n15.36\n14.76\n0.8861\n5.701\n3.393\n1.3670\n5.132\n\n\n14.99\n14.56\n0.8883\n5.570\n3.377\n2.9580\n5.175\n\n\n14.79\n14.52\n0.8819\n5.545\n3.291\n2.7040\n5.111\n\n\n14.86\n14.67\n0.8676\n5.678\n3.258\n2.1290\n5.351\n\n\n14.43\n14.40\n0.8751\n5.585\n3.272\n3.9750\n5.144\n\n\n15.78\n14.91\n0.8923\n5.674\n3.434\n5.5930\n5.136\n\n\n14.49\n14.61\n0.8538\n5.715\n3.113\n4.1160\n5.396\n\n\n14.33\n14.28\n0.8831\n5.504\n3.199\n3.3280\n5.224\n\n\n14.52\n14.60\n0.8557\n5.741\n3.113\n1.4810\n5.487\n\n\n15.03\n14.77\n0.8658\n5.702\n3.212\n1.9330\n5.439\n\n\n14.46\n14.35\n0.8818\n5.388\n3.377\n2.8020\n5.044\n\n\n14.92\n14.43\n0.9006\n5.384\n3.412\n1.1420\n5.088\n\n\n15.38\n14.77\n0.8857\n5.662\n3.419\n1.9990\n5.222\n\n\n12.11\n13.47\n0.8392\n5.159\n3.032\n1.5020\n4.519\n\n\n11.42\n12.86\n0.8683\n5.008\n2.850\n2.7000\n4.607\n\n\n11.23\n12.63\n0.8840\n4.902\n2.879\n2.2690\n4.703\n\n\n12.36\n13.19\n0.8923\n5.076\n3.042\n3.2200\n4.605\n\n\n13.22\n13.84\n0.8680\n5.395\n3.070\n4.1570\n5.088\n\n\n12.78\n13.57\n0.8716\n5.262\n3.026\n1.1760\n4.782\n\n\n12.88\n13.50\n0.8879\n5.139\n3.119\n2.3520\n4.607\n\n\n14.34\n14.37\n0.8726\n5.630\n3.190\n1.3130\n5.150\n\n\n14.01\n14.29\n0.8625\n5.609\n3.158\n2.2170\n5.132\n\n\n14.37\n14.39\n0.8726\n5.569\n3.153\n1.4640\n5.300\n\n\n12.73\n13.75\n0.8458\n5.412\n2.882\n3.5330\n5.067\n\n\n\n\n\n\n\nThese data are formatted usefully for analysis using correlation and plotting. Each row is a different seed and the columns are the variables we are interested. This means that the first values in each column are related - they are all from the same seed\nIn the first instance, it is sensible to create a rough plot of our data (See Figure¬†16.2). Plotting data early helps us in multiple ways:\n\nit helps identify whether there extreme values\nit allows us to see if the association is roughly linear\nit tells us whether any association positive or negative\n\nScatter plots (geom_point()) are a good choice for exploratory plotting with data like these.\n\nggplot(data = seeds,\n       aes(x = compactness, y = kernel_width)) +\n  geom_point()\n\n\n\n\n\n\nFigure¬†16.2: A default scatter plot of compactness and kernel width in seeds demonstrates an approximately linear positive association them the variables.\n\n\n\n\nThe figure suggests that there is a positive correlation between compactness and kernel_width and that correlation is roughly linear.\n\n16.4.2 Check assumptions\nWe can see from Figure¬†16.2 that the relationship between compactness and kernel_width is roughly linear. This is a good sign for using Pearson‚Äôs correlation coefficient.\nOur next check is to use common sense: both variables are continuous and we would expect them to be normally distributed. We can then plot histograms to examine the distributions (See Figure¬†16.3).\nggplot(data = seeds, aes(x = compactness)) +\n  geom_histogram(bins = 12)\n\nggplot(data = seeds, aes(x = kernel_width)) +\n  geom_histogram(bins = 12)\n\n\n\n\n\n\n\n\n\n(a) Compactness\n\n\n\n\n\n\n\n\n\n(b) Kernel width\n\n\n\n\n\n\nFigure¬†16.3: The distributions of the two variables are slightly skewed but do not seem too different from a normal distribution. We will continue with the Pearson‚Äôs correlation coefficient.\n\n\nThe distributions of the two variables are slightly skewed but do not seem too different from a normal distribution.\nFinally, we can use the Shapiro-Wilk test to test for normality.\n\nshapiro.test(seeds$compactness)\n## \n##  Shapiro-Wilk normality test\n## \n## data:  seeds$compactness\n## W = 0.99484, p-value = 0.9937\n\n\nshapiro.test(seeds$kernel_width)\n## \n##  Shapiro-Wilk normality test\n## \n## data:  seeds$kernel_width\n## W = 0.98989, p-value = 0.8486\n\nThe p-values are greater than 0.05 so these tests of the normality assumption are not significant. Note that ‚Äúnot significant‚Äù means not significantly different from a normal distribution. It does not mean definitely normally distributed.\nIt is not unreasonable to continue with the Pearson‚Äôs correlation coefficient. However, later we will also use the Spearman‚Äôs rank correlation coefficient, a non-parametric method which has fewer assumptions. Spearman‚Äôs rank correlation is a more conservative approach.\n\n16.4.3 Do a Pearson‚Äôs correlation test with cor.test()\n\nWe can carry out a Pearson‚Äôs correlation test with cor.test() like this:\n\ncor.test(data = seeds, ~ compactness + kernel_width)\n## \n##  Pearson's product-moment correlation\n## \n## data:  compactness and kernel_width\n## t = 7.3738, df = 68, p-value = 2.998e-10\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  0.5117537 0.7794620\n## sample estimates:\n##       cor \n## 0.6665731\n\nA variable is not being explained in this case so we do not need to include a response variable. Pearson‚Äôs is the default test so we do not need to specify it.\nWhat do all these results mean?\nThe last line gives the correlation coefficient, \\(r\\), that has been estimated from the data. Here, \\(r\\) = 0.67 which is a moderate positive correlation.\nThe fifth line tells you what test has been done: alternative hypothesis: true correlation is not equal to 0. That is \\(H_0\\) is that \\(\\rho = 0\\)\nThe forth line gives the test result: t = 7.3738, df = 68, p-value = 2.998e-10\nThe \\(p\\)-value is very much less than 0.05 so we can reject the null hypothesis and conclude there is a significant positive correlation between compactness and kernel_width.\n\n16.4.4 Report\nThere was a significant positive correlation (\\(r\\) = 0.67) between compactness and kernel width (Pearson‚Äôs: t = 7.374; df = 68; p-value &lt; 0.0001). See Figure¬†16.4.\nCodeggplot(data = seeds,\n       aes(x = compactness, y = kernel_width)) +\n  geom_point() +\n  scale_y_continuous(name = \"Diameter (mm)\") +\n    scale_x_continuous(name = \"Compactness\") +\n  annotate(\"text\", x = 0.85,  y = 3.6, \n           label = expression(italic(r)~\"= 0.67; \"~italic(p)~\"&lt; 0.001\")) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†16.4: Correlation between compactness and kernel width of wheat seeds. High quality images of the internal structure of wheat seeds were taken with a soft X-ray technique and the compactness and kernel width of the seeds were determined. There was a significant positive correlation (\\(r\\) = 0.67) between compactness and kernel width (Pearson‚Äôs: t = 7.374; df = 68; p-value &lt; 0.0001). Note: axes do not start at 0. Data analysis was conducted in R (R Core Team 2024) with tidyverse packages (Wickham et al. 2019).\n\n\n\n16.4.5 Spearman‚Äôs rank correlation coefficient\nTODO",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Association: Correlation and Contingency</span>"
    ]
  },
  {
    "objectID": "association.html#contingency-chi-squared-test",
    "href": "association.html#contingency-chi-squared-test",
    "title": "\n16¬† Association: Correlation and Contingency\n",
    "section": "\n16.5 Contingency Chi-squared test",
    "text": "16.5 Contingency Chi-squared test\nResearchers were interested in whether different pig breeds had the same food preferences. They offered individuals of three breads, Welsh, Tamworth and Essex a choice of three foods: cabbage, sugar beet and swede and recorded the number of individuals that chose each food. The data are shown in Table¬†16.1.\n\n\n\nTable¬†16.1: Food preferences of three pig breeds\n\n\n\n\n\nwelsh\ntamworth\nessex\n\n\n\ncabbage\n11\n19\n22\n\n\nsugarbeet\n21\n16\n8\n\n\nswede\n7\n12\n11\n\n\n\n\n\n\n\n\nWe don‚Äôt know what proportion of food are expected to be preferred but do expect it to be same for each breed if there is no association between breed and food preference. The null hypothesis is that the proportion of foods taken by each breed is the same.\nFor a contingency chi squared test, the inbuilt chi-squared test can be used but we need to to structure our data as a 3 x 3 table. The matrix() function is useful here and we can label the rows and columns to help us interpret the results.\nPut the data into a matrix:\n\n# create the data\nfood_pref &lt;- matrix(c(11, 19, 22,\n                      21, 16, 8,\n                      7, 12, 11),\n                    nrow = 3,\n                    byrow = TRUE)\nfood_pref\n##      [,1] [,2] [,3]\n## [1,]   11   19   22\n## [2,]   21   16    8\n## [3,]    7   12   11\n\nThe byrow and nrow arguments allow us to lay out the data in the matrix as we need. To name the rows and columns we can use the dimnames() function. We need to create a ‚Äúlist‚Äù object to hold the names of the rows and columns and then assign this to the matrix object. The names of rows are columns are called the ‚Äúdimension names‚Äù in a matrix.\nMake a list for the two vectors of names:\n\n# \n\nvars &lt;- list(food = c(\"cabbage\",\n                      \"sugarbeet\",\n                      \"swede\"),\n             breed = c(\"welsh\",\n                       \"tamworth\",\n                       \"essex\"))\n\nThe vectors can be of different lengths in a list which would be important if we had four breeds and only two foods, for example.\nNow assign the list to the dimension names in the matrix:\n\ndimnames(food_pref) &lt;- vars\n\nfood_pref\n##            breed\n## food        welsh tamworth essex\n##   cabbage      11       19    22\n##   sugarbeet    21       16     8\n##   swede         7       12    11\n\nThe data are now in a form that can be used in the chisq.test() function:\n\nchisq.test(food_pref)\n## \n##  Pearson's Chi-squared test\n## \n## data:  food_pref\n## X-squared = 10.64, df = 4, p-value = 0.03092\n\nThe test is significant since the p-value is less than 0.05. We have evidence of a preference for particular foods by different breeds. But in what way? We need to know the ‚Äúdirection of the effect‚Äù i.e., Who likes what?\nThe chisq.test() function has a residuals argument that can be used to calculate the residuals. These are the differences between the observed and expected values. The expected values are the values that would be expected if there was no association between the rows and columns. The residuals are standardised.\n\nchisq.test(food_pref)$residuals\n##            breed\n## food             welsh    tamworth      essex\n##   cabbage   -1.2433504 -0.05564283  1.2722209\n##   sugarbeet  1.9317656 -0.16014783 -1.7125943\n##   swede     -0.7289731  0.26939742  0.4225344\n\nWhere the residuals are positive, the observed value is greater than the expected value and where they are negative, the observed value is less than the expected value. Our results show the Welsh pigs much prefer sugarbeet and strongly dislike cabbage. The Essex pigs prefer cabbage and dislike sugarbeet and the Essex pigs slightly prefer swede but have less strong likes and dislikes.\nThe degrees of freedom are: (rows - 1)(cols - 1) = 2 * 2 = 4.\n\n16.5.1 Report\nDifferent pig breeds showed a significant preference for the different food types (\\(\\chi^2\\) = 10.64; df = 4; p = 0.031) with Essex much preferring cabbage and disliking sugarbeet, Welsh showing a strong preference for sugarbeet and a dislike of cabbage and Tamworth showing no clear preference.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Association: Correlation and Contingency</span>"
    ]
  },
  {
    "objectID": "association.html#summary",
    "href": "association.html#summary",
    "title": "\n16¬† Association: Correlation and Contingency\n",
    "section": "\n16.6 Summary",
    "text": "16.6 Summary\nTODO\n\n\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D‚ÄôAgostino McGowan, Romain Fran√ßois, Garrett Grolemund, et al. 2019. ‚ÄúWelcome to the Tidyverse‚Äù 4: 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, and Jennifer Bryan. 2023. ‚ÄúReadxl: Read Excel Files.‚Äù https://CRAN.R-project.org/package=readxl.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Association: Correlation and Contingency</span>"
    ]
  },
  {
    "objectID": "goodness_of_fit.html",
    "href": "goodness_of_fit.html",
    "title": "\n17¬† Goodness of fit\n",
    "section": "",
    "text": "Incomplete\n\n\n\nYou are reading a work in progress. This page is a dumping ground for ideas. Sections maybe missing, or contain a list of points to include.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Goodness of fit</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "\n18¬† Summary\n",
    "section": "",
    "text": "Incomplete\n\n\n\nYou are reading a work in progress. This page is a dumping ground for ideas. Sections maybe missing, or contain a list of points to include.",
    "crumbs": [
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "keyboard_shortcuts_tips.html",
    "href": "keyboard_shortcuts_tips.html",
    "title": "\n19¬† Keyboard short cuts and other tips\n",
    "section": "",
    "text": "Incomplete\n\n\n\nYou are reading a work in progress. This page is a dumping ground for ideas. Sections maybe missing, or contain a list of points to include.\n\n\n\n\n\n\n\n\n\n\nDescription\nItem\nWindows/ & Linux\nMac\n\n\n\nShow All the Keyboard Shortcuts\n\nAlt+Shift+K\nOption+Shift+K\n\n\nInsert the Assignment operator\n&lt;-\nAlt+-\nOption+-\n\n\nOpen a new script\n\nCtrl+Shift+N\nShift+Command+N\n\n\nRun current line/selection\n\nCtrl-Enter\nCommand-Return\n\n\nComment out current line/selection\n\nCtrl+Shift+C\nShift+Command+C\n\n\nOpen help on current function\n\nF1\nF1\n\n\nInsert the pipe operator\n|&gt;\nCtrl+Shift+M\nShift+Command+M\n\n\nInsert a new code section\n\nCtrl+Shift+R\nShift+Command+R",
    "crumbs": [
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Keyboard short cuts and other tips</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Allaire, J. J., Charles Teague, Carlos Scheidegger, Yihui Xie, and\nChristophe Dervieux. 2022. Quarto. https://doi.org/10.5281/zenodo.5960048.\n\n\nAustralian Curriculum, Assessment, and Reporting Authority. 2015.\n‚ÄúNational Assessment Program ICT\nLiteracy.‚Äù https://nap.edu.au/_resources/D15_8761__NAP-ICT_2014_Public_Report_Final.pdf.\n\n\nBryan, Jennifer, Jim Hester, Shannon Pileggi, and E. David Aja. n.d.\nWhat They Forgot to Teach You About\nR. Accessed September 26, 2019.\n\n\nDrossel, Kerstin, and Birgit Eickelmann. 2017. ‚ÄúThe Use of Tablets\nin Secondary Schools and Its Relationship with Computer\nLiteracy.‚Äù In, 114‚Äì24. Springer International Publishing. https://doi.org/10.1007/978-3-319-74310-3_14.\n\n\nDunn, Olive Jean. 1964. ‚ÄúMultiple Comparisons Using Rank\nSums.‚Äù Technometrics 6 (3): 241‚Äì52. https://doi.org/10.1080/00401706.1964.10490181.\n\n\nKruskal, William H., and W. Allen Wallis. 1952. ‚ÄúUse of Ranks in\nOne-Criterion Variance Analysis.‚Äù Journal of the American\nStatistical Association 47 (260): 583‚Äì621. https://doi.org/10.1080/01621459.1952.10483441.\n\n\nLenth, Russell V. 2023. ‚ÄúEmmeans: Estimated Marginal Means, Aka\nLeast-Squares Means.‚Äù https://CRAN.R-project.org/package=emmeans.\n\n\nLikert, R. 1932. ‚ÄúA Technique for the Measurement of\nAttitudes.‚Äù Archives of Psychology 22 140: 55‚Äì55.\n\n\nOgle, Derek H., Jason C. Doll, A. Powell Wheeler, and Alexis Dinno.\n2023. ‚ÄúFSA: Simple Fisheries Stock Assessment Methods.‚Äù https://CRAN.R-project.org/package=FSA.\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical\nComputing. Vienna, Austria: R Foundation for Statistical Computing.\nhttps://www.R-project.org/.\n\n\nRand, Emma, Chong, James, Buenabad-Chavez, Jorge, Cansdale, Annabel,\nForrester, Sarah, and Greeves, Evelyn. 2022.\n‚ÄúCloud-SPAN/00genomics: Cloud-SPAN Genomics Course\nOverview,‚Äù May. https://doi.org/10.5281/ZENODO.6564314.\n\n\nSu√°rez, Nicol√°s M., Eva Betancor, Tilman E. Klassert, Teresa Almeida,\nMariano Hern√°ndez, and Jos√© J. Pestano. 2009. ‚ÄúPhylogeography and\nGenetic Structure of the Canarian Common Chaffinch (Fringilla Coelebs)\nInferred with mtDNA and Microsatellite Loci.‚Äù Molecular\nPhylogenetics and Evolution 53 (2): 556‚Äì64. https://doi.org/10.1016/j.ympev.2009.07.018.\n\n\nTukey, John W. 1949. ‚ÄúComparing Individual Means in the Analysis\nof Variance.‚Äù Biometrics 5 (2): 99‚Äì114. https://doi.org/10.2307/3001913.\n\n\nWickham, Hadley. 2014. ‚ÄúTidy Data.‚Äù Journal of\nStatistical Software, Articles 59 (10): 1‚Äì23. https://vita.had.co.nz/papers/tidy-data.pdf.\n\n\n‚Äî‚Äî‚Äî. 2016. Ggplot2: Elegant Graphics for Data Analysis.\nSpringer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD‚ÄôAgostino McGowan, Romain Fran√ßois, Garrett Grolemund, et al. 2019.\n‚ÄúWelcome to the Tidyverse‚Äù 4:\n1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, and Jennifer Bryan. 2023. ‚ÄúReadxl: Read Excel\nFiles.‚Äù https://CRAN.R-project.org/package=readxl.\n\n\nWickham, Hadley, Romain Fran√ßois, Lionel Henry, Kirill M√ºller, and Davis\nVaughan. 2023. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2023. Readr: Read\nRectangular Text Data. https://CRAN.R-project.org/package=readr.\n\n\nWickham, Hadley, Davis Vaughan, and Maximilian Girlich. 2023. Tidyr:\nTidy Messy Data. https://CRAN.R-project.org/package=tidyr.\n\n\nWilcoxon, Frank. 1945. ‚ÄúIndividual Comparisons by Ranking\nMethods.‚Äù Biometrics Bulletin 1 (6): 80‚Äì83. https://doi.org/10.2307/3001968.\n\n\nXie, Yihui. 2022. ‚ÄúKnitr: A General-Purpose Package for Dynamic\nReport Generation in r.‚Äù https://yihui.org/knitr/.\n\n\nZhu, Hao. 2021. ‚ÄúkableExtra: Construct Complex Table with ‚ÄôKable‚Äô\nand Pipe Syntax.‚Äù https://CRAN.R-project.org/package=kableExtra.",
    "crumbs": [
      "References"
    ]
  }
]
[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Analysis for Bioscientists",
    "section": "",
    "text": "Welcome!\nfront page stuff",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "\n1¬† About this book\n",
    "section": "",
    "text": "1.1 Who is this book for?\nThis book is primarily being written to support Bioscience students at the University of York. The ultimate aim is to support the full spectrum of computational skills that a bioscience undergraduate or postgraduate at York - and elsewhere - might need. But it is a work in progress. The content included so far is described in the Overview of contents section below.\nIt is being written in the open so that it can be used by anyone who finds it useful. It is also being written in the open so that anyone can contribute to it.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>About this book</span>"
    ]
  },
  {
    "objectID": "intro.html#approach-of-this-book",
    "href": "intro.html#approach-of-this-book",
    "title": "\n1¬† About this book\n",
    "section": "\n1.2 Approach of this book",
    "text": "1.2 Approach of this book\n\nexplanations followed by worked examples",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>About this book</span>"
    ]
  },
  {
    "objectID": "intro.html#overview-of-contents",
    "href": "intro.html#overview-of-contents",
    "title": "\n1¬† About this book\n",
    "section": "\n1.3 Overview of contents",
    "text": "1.3 Overview of contents\nIt is in sections\nPart 1: What they forgot to teach you about computers\nThis chapter tries to teach the computer skills that you might have missed if you have used mainly the mobile devices. I focus on the knowledge gaps that often appear when people are learning computational data analysis. Primarily these are to do with finding and organising their files and folders in the file systems.\nPart 2 Getting started with data\nThe first steps into analysing data with R. The first chapter in this part covers important concepts about data: whether they are discrete and continuous and how we summarise them using descriptive statistics. The second chapter introduces you to R and RStudio for the first time. We start by exploring the layout and appearance then move on to coding. The third chapter describes some useful workflow patterns and tools for organising your work in RStudio. Using these will make learning R easier. Finally, we will go through a complete workflow from importing data from a file to saving a figure for reporting.\nPart 3 Statistical Analysis\nThis section is a first course in Statistical inference which is the process of inferring the characteristics of populations from samples using data analysis. In this first course we take what is called a frequentist - or classical - approach to statistical inference. This is the approach that is most commonly taught in introductory statistics courses. We will learn about the logic of hypothesis testing and confidence intervals. You will also get an introduction to statistical models, what is a statistical model and in particular a linear model.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>About this book</span>"
    ]
  },
  {
    "objectID": "intro.html#conventions-used-in-the-book",
    "href": "intro.html#conventions-used-in-the-book",
    "title": "\n1¬† About this book\n",
    "section": "\n1.4 Conventions used in the book",
    "text": "1.4 Conventions used in the book\nI use some conventions most of which I hope are intuitive. I have tried to articulate them here. If you recognise conventions I have used that are not listed here please let me know.\nCode and any output appears in blocks formatted like this:\n\n# import the chaff data\nchaff &lt;- read_table(\"data-raw/chaff.txt\")\nglimpse(chaff)\n## Rows: 40\n## Columns: 2\n## $ subspecies &lt;chr&gt; \"coelebs\", \"coelebs\", \"coelebs\", \"coelebs\", \"coelebs\", \"coe‚Ä¶\n## $ mass       &lt;dbl&gt; 18.3, 22.1, 22.4, 18.5, 22.2, 19.3, 17.8, 20.2, 22.1, 16.6,‚Ä¶\n\nLines of output start with a ## to distinguish from code comments which begin with a single #. You will learn more about comments in the Using Scripts section in First Steps in RStudio\nWithin the text: - packages are indicated in bold code font like this: ggplot2 - functions are indicated in code font with brackets after their name like this: ggplot() - R objects are indicated in code font like this: stag\nThe content of a code block can be copied using the icon in its top right corner.\nI use packages from the tidyverse (Wickham et al. 2019) including ggplot2 (Wickham 2016), dplyr (Wickham et al. 2023), tidyr (Wickham, Vaughan, and Girlich 2023) and readr (Wickham, Hester, and Bryan 2023) throughout the book. All the code assumes you have loaded the core tidyverse packages with:\n\nlibrary(tidyverse)\n\nIf you run examples and get an error like this:\n\n# Error in read_table(\"data-raw/stag.txt\") : \n#  could not find function \"read_table\"\n\nIt is likely you need to load the tidyverse as shown above.\nAll other packages will be loaded explicitly with library() statements where needed.\nWhen you see ‚Äúüé¨ Your turn!‚Äù indicates that you might want to code along with examples or that there is an opportunity to check your understanding by answering a question. Questions are answered in words or with a piece of code. The answers are given in collapsed sections so you can try to answer them before checking the answer. For example, a question answered in words looks like this:\nüé¨ Your turn! Use the file system above to answer these questions.\n\nWhat is the absolute path for the documentdoc4.txt on a Mac computer?\n\n\n\n\n\n\n\nüìñ\n\n\n\n\n\n\n/home/user1/docs/data/doc4.txt\n\n\n\n\nAnd a question answered with a piece of code looks like this:\nüé¨ Your turn! Assign the value of 4 to a variable called y:\n\nCodey &lt;- 4",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>About this book</span>"
    ]
  },
  {
    "objectID": "intro.html#annotating-this-book",
    "href": "intro.html#annotating-this-book",
    "title": "\n1¬† About this book\n",
    "section": "\n1.5 Annotating this book",
    "text": "1.5 Annotating this book\nThis page has annotating with Hypothesis enabled. Hypothesis allows you to annotate this book with your own private notes or make notes shared with friends. You need to create a free personal account. You can make annotations that are public, private only to you or shared with a private group. Please follow the code of conduct in your annotations.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>About this book</span>"
    ]
  },
  {
    "objectID": "intro.html#code-of-conduct",
    "href": "intro.html#code-of-conduct",
    "title": "\n1¬† About this book\n",
    "section": "\n1.6 Code of Conduct",
    "text": "1.6 Code of Conduct\nWe are dedicated to providing a welcoming and supportive learning environment for all readers, regardless of background or identity. As such, we do not tolerate comments that are disrespectful to fellow learners or that excludes, intimidates, or causes discomfort to others. The following bullet points set out explicitly what we hope you will consider to be appropriate community guidelines:\n\nBe respectful of different viewpoints and experiences. Do not use in homophobic, racist, transphobic, ageist, ableist, sexist, or otherwise exclusionary language.\nUse welcoming and inclusive language. Do not address others in an angry, intimidating, or demeaning manner. Be considerate of the ways the words you choose may impact others. Be patient and respectful of the fact that English is a second (or third or fourth!) language for many.\nRespect the privacy and safety of others. Do not share their information without their express permission.\nAs an overriding general rule, please be intentional in your actions and humble in your mistakes.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>About this book</span>"
    ]
  },
  {
    "objectID": "intro.html#contributing",
    "href": "intro.html#contributing",
    "title": "\n1¬† About this book\n",
    "section": "\n1.7 Contributing",
    "text": "1.7 Contributing\nThis book is being written in the open so that anyone can contribute to it. If you find a mistake, or have a suggestion for improvement you can create an issue.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>About this book</span>"
    ]
  },
  {
    "objectID": "intro.html#license",
    "href": "intro.html#license",
    "title": "\n1¬† About this book\n",
    "section": "\n1.8 License",
    "text": "1.8 License\n\nThis work is licensed under CC BY-NC 4.0 This license requires that reusers give credit to the creator. It allows reusers to distribute, remix, adapt, and build upon the material in any medium or format, for noncommercial purposes only.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>About this book</span>"
    ]
  },
  {
    "objectID": "intro.html#please-cite-as",
    "href": "intro.html#please-cite-as",
    "title": "\n1¬† About this book\n",
    "section": "\n1.9 Please cite as",
    "text": "1.9 Please cite as\nPlease cite this book as:\nRand, E. (2023). Computational Analysis for Bioscientists (Version 0.1) https://3mmarand.github.io/comp4biosci/",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>About this book</span>"
    ]
  },
  {
    "objectID": "intro.html#credits",
    "href": "intro.html#credits",
    "title": "\n1¬† About this book\n",
    "section": "\n1.10 Credits",
    "text": "1.10 Credits\nThis book is written with R (R Core Team 2023), Quarto (Allaire et al. 2022), knitr (Xie 2022), kableExtra (Zhu 2021). My R session information is shown below:\n\nsessionInfo()\n## R version 4.3.1 (2023-06-16 ucrt)\n## Platform: x86_64-w64-mingw32/x64 (64-bit)\n## Running under: Windows 10 x64 (build 19045)\n## \n## Matrix products: default\n## \n## \n## locale:\n## [1] LC_COLLATE=English_United Kingdom.utf8 \n## [2] LC_CTYPE=English_United Kingdom.utf8   \n## [3] LC_MONETARY=English_United Kingdom.utf8\n## [4] LC_NUMERIC=C                           \n## [5] LC_TIME=English_United Kingdom.utf8    \n## \n## time zone: Europe/London\n## tzcode source: internal\n## \n## attached base packages:\n## [1] stats     graphics  grDevices datasets  utils     methods   base     \n## \n## other attached packages:\n##  [1] patchwork_1.1.2 forcats_1.0.0   stringr_1.5.0   dplyr_1.1.0    \n##  [5] purrr_1.0.1     readr_2.1.4     tidyr_1.3.0     tibble_3.2.0   \n##  [9] ggplot2_3.4.1   tidyverse_1.3.2\n## \n## loaded via a namespace (and not attached):\n##  [1] utf8_1.2.3          generics_0.1.3      renv_0.17.0        \n##  [4] xml2_1.3.5          stringi_1.7.12      hms_1.1.2          \n##  [7] digest_0.6.34       magrittr_2.0.3      timechange_0.2.0   \n## [10] evaluate_0.23       grid_4.3.1          fastmap_1.1.0      \n## [13] cellranger_1.1.0    jsonlite_1.8.8      backports_1.4.1    \n## [16] DBI_1.1.3           googledrive_2.0.0   rvest_1.0.3        \n## [19] httr_1.4.4          fansi_1.0.4         scales_1.2.1       \n## [22] modelr_0.1.10       cli_3.4.1           crayon_1.5.2       \n## [25] rlang_1.1.1         dbplyr_2.3.0        reprex_2.0.2       \n## [28] ellipsis_0.3.2      munsell_0.5.0       withr_2.5.0        \n## [31] yaml_2.3.7          tools_4.3.1         tzdb_0.3.0         \n## [34] gargle_1.3.0        colorspace_2.1-0    broom_1.0.3        \n## [37] assertthat_0.2.1    vctrs_0.5.2         R6_2.5.1           \n## [40] lubridate_1.9.2     lifecycle_1.0.3     fs_1.6.1           \n## [43] pkgconfig_2.0.3     pillar_1.8.1        gtable_0.3.1       \n## [46] glue_1.6.2          haven_2.5.1         xfun_0.37          \n## [49] tidyselect_1.2.0    rstudioapi_0.14     knitr_1.42         \n## [52] htmltools_0.5.4     googlesheets4_1.0.1 rmarkdown_2.20     \n## [55] compiler_4.3.1      readxl_1.4.2\n\n\n\n\n\nAllaire, J. J., Charles Teague, Carlos Scheidegger, Yihui Xie, and Christophe Dervieux. 2022. Quarto. https://doi.org/10.5281/zenodo.5960048.\n\n\nR Core Team. 2023. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D‚ÄôAgostino McGowan, Romain Fran√ßois, Garrett Grolemund, et al. 2019. ‚ÄúWelcome to the Tidyverse‚Äù 4: 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Romain Fran√ßois, Lionel Henry, Kirill M√ºller, and Davis Vaughan. 2023. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2023. Readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr.\n\n\nWickham, Hadley, Davis Vaughan, and Maximilian Girlich. 2023. Tidyr: Tidy Messy Data. https://CRAN.R-project.org/package=tidyr.\n\n\nXie, Yihui. 2022. ‚ÄúKnitr: A General-Purpose Package for Dynamic Report Generation in r.‚Äù https://yihui.org/knitr/.\n\n\nZhu, Hao. 2021. ‚ÄúkableExtra: Construct Complex Table with ‚ÄôKable‚Äô and Pipe Syntax.‚Äù https://CRAN.R-project.org/package=kableExtra.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>About this book</span>"
    ]
  },
  {
    "objectID": "what_they_forgot.html",
    "href": "what_they_forgot.html",
    "title": "What they forgot to teach you about computers",
    "section": "",
    "text": "Why ‚ÄúWhat they forgot‚Äù?\nMost students grew up with the presence of digital technology and are thought to be comfortable with, and fluent in, technology. But for many, this fluency is with using tablets and smart phones and these are different to computers. Several studies have shown that the increased reliance on of tablets and smartphones makes people highly proficient at online communication but hampers the development of other computing skills Drossel and Eickelmann (2017); Australian Curriculum and Authority (2015).\nThis chapter tries to teach the computer skills that you might have missed if you have used mainly the mobile devices. I focus on the knowledge gaps that often appear when people are learning computational data analysis. Primarily these are to do with finding and organising their files and folders in the file systems.\nIf you often use a computer for tasks like organising files and writing documents and spreadsheets much of this chapter might be familiar. However, if you have mainly used your mobile phone and tablets to access the internet then reading all of it will be helpful. The first chapter briefly explains why a computer rather than a tablet is most useful for a science degree and what an operating system is. The second chapter some essential concept for scientific computers: file system organisation, file types, working directories and paths. Most people joining a bioscience degree programme are not familiar with the concept of working directories but will find learning data analysis much easier once they pick up this knowledge.",
    "crumbs": [
      "What they forgot to teach you about computers"
    ]
  },
  {
    "objectID": "what_they_forgot.html#why-what-they-forgot",
    "href": "what_they_forgot.html#why-what-they-forgot",
    "title": "What they forgot to teach you about computers",
    "section": "",
    "text": "Australian Curriculum, Assessment, and Reporting Authority. 2015. ‚ÄúNational Assessment Program ICT Literacy.‚Äù https://nap.edu.au/_resources/D15_8761__NAP-ICT_2014_Public_Report_Final.pdf.\n\n\nDrossel, Kerstin, and Birgit Eickelmann. 2017. ‚ÄúThe Use of Tablets in Secondary Schools and Its Relationship with Computer Literacy.‚Äù In, 114‚Äì24. Springer International Publishing. https://doi.org/10.1007/978-3-319-74310-3_14.",
    "crumbs": [
      "What they forgot to teach you about computers"
    ]
  },
  {
    "objectID": "machine_and_os.html",
    "href": "machine_and_os.html",
    "title": "\n2¬† Machines and operating systems\n",
    "section": "",
    "text": "2.1 Computer or tablet?\nComputers (laptop or desktop) and tablets can both be useful for doing a science degree but if you have to choose just one, I would recommend a computer. This is because\nTablets are more portable and have touch-screens and stylus support and this makes them easier to use for note taking in lectures and annotating documents. Some laptops also have touch-screens but they can be expensive. Overtime computers will adopt more of the advantages of tablets and tablets will adopt more of the advantages of computers.",
    "crumbs": [
      "What they forgot to teach you about computers",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Machines and operating systems</span>"
    ]
  },
  {
    "objectID": "machine_and_os.html#computer-or-tablet",
    "href": "machine_and_os.html#computer-or-tablet",
    "title": "\n2¬† Machines and operating systems\n",
    "section": "",
    "text": "Computers typically offer more processing power, storage capacity, and flexibility for running complex software all of which are important in science.\nScientific and academic software often cannot be installed on tablets - at least not easily.\nYour institution most likely has a networked computer system with computers that you use in taught workshop and which are available to you throughout your degree. Many people find it easier to use a similar computer at home so they can follow instructions from the workshop exactly to continue the work at home\nWe often need to multitask and use several programs at once. The greater power and screen size of a computer usually makes this easier.",
    "crumbs": [
      "What they forgot to teach you about computers",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Machines and operating systems</span>"
    ]
  },
  {
    "objectID": "machine_and_os.html#what-is-an-operating-system",
    "href": "machine_and_os.html#what-is-an-operating-system",
    "title": "\n2¬† Machines and operating systems\n",
    "section": "\n2.2 What is an operating system?",
    "text": "2.2 What is an operating system?\nEvery computer or mobile device has an operating system (OS) which controls how all the other software on a computer interacts with the computer hardware. You may already have a preference for a particular OS. They start the computer up, manage the memory and all the processes, and control all the devices connected to the computer. As a consequence, all the other software on a computer has to be able to work with the operating system installed.\nMost people do not install the operating system but buy their machine with it pre-installed.",
    "crumbs": [
      "What they forgot to teach you about computers",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Machines and operating systems</span>"
    ]
  },
  {
    "objectID": "machine_and_os.html#types-of-operating-system",
    "href": "machine_and_os.html#types-of-operating-system",
    "title": "\n2¬† Machines and operating systems\n",
    "section": "\n2.3 Types of operating system",
    "text": "2.3 Types of operating system\n\n2.3.1 Microsoft Windows and Apple macOS\nWindows and Mac are the two most widely used OS on personal computers. Apple Mac computers use only Apple macOS but Windows OS are used by many machine manufacturers. Both OS are proprietor meaning that they are designed and sold by the companies and are not meant to be altered by users. It also means the other applications they make are designed to work best with their OS.\nYou might already have had - or seen - a Mac versus Windows debate but in reality it is down to preference for most users. If you already have experience with one, you will probably prefer to keep using it but if you are buying for the first time, I recommend using the same as your institution uses. This makes it more likely that instructions for taught material will just work on your machine and more likely that people around you will be able to help.\nOne of the differences uses most notice between Windows and Mac machine is the keyboard keys names. Table Table¬†2.1 gives some of the most notable.\n\n\nTable¬†2.1: Keyboard equivalents for Windows and Mac\n\n\n\nWindows\nMac\n\n\n\nEnter\nReturn\n\n\nControl\nCommand\n\n\nAlt\nOption\n\n\n\n\n\n\nWhen using RStudio, the section on Keyboard Shortcuts and tips will help.\n\n2.3.2 Linux\nLinux is a family of open source OS. Open source means that anyone can modify and share the software.\n(TODO - expand)",
    "crumbs": [
      "What they forgot to teach you about computers",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Machines and operating systems</span>"
    ]
  },
  {
    "objectID": "file_systems.html",
    "href": "file_systems.html",
    "title": "\n3¬† Understanding file systems\n",
    "section": "",
    "text": "3.1 Files\nA file is a unit of storage on a computer with a name that uniquely identifies it. Files can be of different types depending on the sort of information held in them. The file name very often consists of two parts, separated by a dot:\nSome examples are report.docx, analysis.R, culture.csv and readme.txt. There is a relationship between the file extension and the file type",
    "crumbs": [
      "What they forgot to teach you about computers",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Understanding file systems</span>"
    ]
  },
  {
    "objectID": "file_systems.html#files",
    "href": "file_systems.html#files",
    "title": "\n3¬† Understanding file systems\n",
    "section": "",
    "text": "the name - the base name of the file\nan extension that should indicate the format or content of the file.",
    "crumbs": [
      "What they forgot to teach you about computers",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Understanding file systems</span>"
    ]
  },
  {
    "objectID": "file_systems.html#plain-text-files",
    "href": "file_systems.html#plain-text-files",
    "title": "\n3¬† Understanding file systems\n",
    "section": "\n3.2 Plain text files",
    "text": "3.2 Plain text files\nOne of the simplest types of file is a ‚Äútext file‚Äù, also known as ASCII files, which contains only text characters and no formatting, images or colours. Text files have many different extensions to indicate what the text content represents. A few of these are given in Table¬†3.1.\n\n\n\nTable¬†3.1: Some common text file extensions and the content the extension usually indicates.\n\n\n\n\nExtension\nContent usually inside\n\n\n\n.txt\nCould be any kind of text, including data\n\n\n.tab\nValues, often data, separated by tabs\n\n\n.csv\nValues, often data, separated by commas\n\n\n.R\nR commands and comments\n\n\n.py\nPython commands and comments\n\n\n.fasta\nNucleotide or amino acid sequences\n\n\n\n\n\n\n\n\nPlain text files are extremely portable which means they can be opened on any computer by a very large number of programs. Simple text editors which exist on any system like Windows Notepad or Mac‚Äôs TextEdit will open text files where as they will not open program-specific files like, for example, .xlsx (Excel) or .docx (Word) files (Figure¬†3.1). It is usually easy for application designers to make an application manage plain text files. This means\n\n\n\n\n\nFigure¬†3.1: Notepad will open an Excel file but the contents are a mess of strange characters\n\n\nData is commonly held in text files because of their portability.",
    "crumbs": [
      "What they forgot to teach you about computers",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Understanding file systems</span>"
    ]
  },
  {
    "objectID": "file_systems.html#plain-text-files-with-markup",
    "href": "file_systems.html#plain-text-files-with-markup",
    "title": "\n3¬† Understanding file systems\n",
    "section": "\n3.3 Plain text files with markup",
    "text": "3.3 Plain text files with markup\nText files can also include ‚Äúmarkup‚Äù which are text characters used to annotate text to control how it is displayed or processed. Such files retain their portability and are human readable. You will have used them often! (Table¬†3.2)\n\n\n\nTable¬†3.2: Some common markup file extensions and the content the extension usually indicates.\n\n\n\n\nExtension\nUsed for\n\n\n\n.html\nHypertext Markup Language - for documents designed to be displayed in a web browser.\n\n\n.md\nmarkdown - a lightweight markup language designed to be human readable\n\n\n.Rmd\nR markdown - like markdown but can include R code\n\n\n.JSON\nJavaScript Object Notation - commonly used for transmitting data in web applications using attribute-value pairs.\n\n\n.tex\nTeX - a typesetting langauge especially where the writer needs precise spacing and/or unusual fonts and characters such as in maths, linguistics and music.\n\n\n.qmd\nQuarto markdown - next-generation version of R Markdown designed to work with *any* programming langauge.",
    "crumbs": [
      "What they forgot to teach you about computers",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Understanding file systems</span>"
    ]
  },
  {
    "objectID": "file_systems.html#the-relationship-between-file-extensions-and-programs",
    "href": "file_systems.html#the-relationship-between-file-extensions-and-programs",
    "title": "\n3¬† Understanding file systems\n",
    "section": "\n3.4 The relationship between file extensions and programs",
    "text": "3.4 The relationship between file extensions and programs\nWhilst a file extension is intended to indicate the content and format, it is important to remember a few things. The extension is just part of the name and it is certainly possible to called a file myfile.csv without its contents being formatted as comma-separated values. Programs vary in their behaviour to file extensions. Most text editors will attempt to open any file regardless of the extension and make it possible to save a file with any extension. In program like MS Word you cannot save a Word format file with any extension other than .docx, you can only save it in another format to change the extension. Some programs will not open fies with a the wrong extension even if the contents are in the correct format.",
    "crumbs": [
      "What they forgot to teach you about computers",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Understanding file systems</span>"
    ]
  },
  {
    "objectID": "file_systems.html#file-systems",
    "href": "file_systems.html#file-systems",
    "title": "\n3¬† Understanding file systems\n",
    "section": "\n3.5 File systems",
    "text": "3.5 File systems\nIn a file system, the files are organised into directories. Directory is the old word for what many now call a folder but commands that act on folders in most programming languages and environments reflect this history\nFor example, all of these mean ‚Äútell me my working directory‚Äù:\n\n\ngetwd() get working directory in R\n\npwd print working directory on Linux\n\nos.getcwd() get current working directory in Python\n\nConsequently it is common to use the the word directory in scientific computing.\nFolders can contain sub-folders, which can contain their own sub-folders, and so on almost without limit.\nIt is easiest to picture a file system, or part of it, as a tree that starts at a directory and branches out from there. This is called a hierarchical structure. Figure¬†3.2 shows an example of a hierarchical file structure that starts at a directory called home:\n\n\n\n\n\nFigure¬†3.2: A file hierarchy containing 4 levels of folders and files. Figure adapted from (Rand, Emma et al. 2022).",
    "crumbs": [
      "What they forgot to teach you about computers",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Understanding file systems</span>"
    ]
  },
  {
    "objectID": "file_systems.html#using-a-file-manager",
    "href": "file_systems.html#using-a-file-manager",
    "title": "\n3¬† Understanding file systems\n",
    "section": "\n3.6 Using a file manager",
    "text": "3.6 Using a file manager\nFile managers are the basic way that you interact with the file system on your computer. They allows you to move, copy, and delete files. You can also launch applications from them and and connect to other networks. The windows file explorer is called File Explorer and on Mac it is called Finder (Figure¬†3.3).\n\n\n\n\n\n\n\n\n\n(a) File Explorer (Windows)\n\n\n\n\n\n\n\n\n\n(b) Finder (Mac)\n\n\n\n\n\n\nFigure¬†3.3: File manager programs in Windows and Mac\n\n\nWindows Explorer and Mac Finder do not show the file extensions on file names by default but I find it helpful to be able to see them. You can show the file extensions like this:\n\nunder View, in the Show/hide group, select the ‚ÄúFile name extensions‚Äù check box Figure¬†3.4 (a)\nchoose Finder | Preferences | Advanced then check ‚ÄúShow all filename extensions‚Äù box Figure¬†3.4 (a)\n\n\n\n\n\n\n\n\n\n\n(a) Windows\n\n\n\n\n\n\n\n\n\n\n\n(b) Mac\n\n\n\n\n\n\nFigure¬†3.4: Showing file extensions in Windows and Mac",
    "crumbs": [
      "What they forgot to teach you about computers",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Understanding file systems</span>"
    ]
  },
  {
    "objectID": "file_systems.html#root-and-home-directories",
    "href": "file_systems.html#root-and-home-directories",
    "title": "\n3¬† Understanding file systems\n",
    "section": "\n3.7 Root and home directories",
    "text": "3.7 Root and home directories\nThe top-level of directory on a computer system is known as the ‚Äúroot directory‚Äù. The root is represented as a / in Mac and Linux operating systems. In Windows the root directory is also known as a drive. In most cases, this will be the C:\\ drive.\nEven though the root directory is at the base of the file tree (or the top, depending on how you view it), it is not necessarily where our journey through the file system starts when we launch a new session on our computer. Instead our journey begins in the so called ‚Äúhome directory‚Äù. You home directory is not usually named home but with your username for that computer. Your personal files and directories can be found inside this folder. This is where your computer assumes you want to start when you open your file manager. On Windows and Mac your home directory is a directory inside the directory called Users immediately under the root and named with your username (Figure¬†3.5).\n\n\n\n\n\nFigure¬†3.5: The hierarchy from the root. The top level is C:\\ in Windows and / in Mac. Below that is the Users directory which has a folder for each user. Your home directory is named with your username inside the Users folder. Figure adapted from (Rand, Emma et al. 2022).\n\n\nThere will be other folders immediately below the root directory (on the same level of the hierarchy as Users). These contain system-level files and folders that you do not usually needed to open, edit or move. For example, Program Files is where programs are installed.",
    "crumbs": [
      "What they forgot to teach you about computers",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Understanding file systems</span>"
    ]
  },
  {
    "objectID": "file_systems.html#working-directories",
    "href": "file_systems.html#working-directories",
    "title": "\n3¬† Understanding file systems\n",
    "section": "\n3.8 Working directories",
    "text": "3.8 Working directories\nThe working directory of a program is the default location a program is using. It is where the program will read and write files by default. You have only one working directory at a time. The terms ‚Äòworking directory‚Äô, ‚Äòcurrent working directory‚Äô and ‚Äòcurrent directory‚Äô all mean the same thing.",
    "crumbs": [
      "What they forgot to teach you about computers",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Understanding file systems</span>"
    ]
  },
  {
    "objectID": "file_systems.html#file-paths",
    "href": "file_systems.html#file-paths",
    "title": "\n3¬† Understanding file systems\n",
    "section": "\n3.9 File Paths",
    "text": "3.9 File Paths\nA path gives the address - or location - of a filesystem object, such as a file or directory. Paths appear in the address bar of your browser or file manager. We need to know a file path whenever we want to read, write or refer to a file using code rather than interactively pointing and clicking to navigate. In a file path, each directory is represented as a separate component separated by a backslash\\ ¬†or a forward slash /. Most systems use forward slashes but Windows uses backslashes1 to separate path components and that is how the path will appear in the address bar of Windows Explorer. However, in R you can use paths with forward slashes even on windows.\nA path can be absolute or relative depending on the starting point.\n\n3.9.1 Absolute paths\nAn Absolute path contains the complete list of directories needed to locate a file on your computer from the root. For example, the absolute path for the file called doc3.txt in the file system above would be /Users/user1/docs/data/doc3.txt on Mac and C:\\Users\\user1\\docs\\data\\doc3.txt on Windows. In R, even on Windows, it can be given as C:/Users/user1/docs/data/doc3.txt",
    "crumbs": [
      "What they forgot to teach you about computers",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Understanding file systems</span>"
    ]
  },
  {
    "objectID": "file_systems.html#relative-paths",
    "href": "file_systems.html#relative-paths",
    "title": "\n3¬† Understanding file systems\n",
    "section": "\n3.10 Relative paths",
    "text": "3.10 Relative paths\nA relative path gives the location of a filesystem object relative to the working directory. Whenever the file you want to reference is in the working directory you can use just its name but if it is in a different folder you need to give the relative path. Some examples:\n\nif your working directory was docs, the relative path for doc3.txt would be data/doc3.txt.\nif your working directory was docs the relative path for abe.exe files would be ../programs/abe.exe.\n\n../ allows you to look in the directory above the working directory and ../.. allows you to look in the directory two levels above the working directory and so on.\nüé¨ Your turn! Use the file system above to answer these questions.\n\nWhat is the absolute path for the documentdoc4.txt on a Mac computer?\nWhat is the absolute path for the document doc4.txt on a Windows computer?\nAssuming your working directory is docs, what is the relative path for the document doc2.txt?\nAssuming your working directory is data, what is the relative path for the document doc2.txt?\n\n\n\n\n\n\n\nüìñ\n\n\n\n\n\n\n/Users/user1/docs/data/doc4.txt\nC:/Users/user1/docs/data/doc4.txt\ndoc2.txt\n../doc2.txt?\n\n\n\n\nMost of the time you should use relative paths because that makes your work portable. You only need to use absolute paths when you are referring to filesystem outside the one you are using.",
    "crumbs": [
      "What they forgot to teach you about computers",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Understanding file systems</span>"
    ]
  },
  {
    "objectID": "file_systems.html#save-files-from-the-internet",
    "href": "file_systems.html#save-files-from-the-internet",
    "title": "\n3¬† Understanding file systems\n",
    "section": "\n3.11 Save files from the internet",
    "text": "3.11 Save files from the internet\nFiles downloaded from the internet go to a folder called Downloads by default on many browsers. This is annoying when you often want to place a file in a particular folder. I recommend you change this behaviour.\n\nChrome Go to chrome://settings/downloads and turn on ‚ÄúAsk where to save each file before downloading‚Äù\nSafari Go into Preferences and under General you can change ‚ÄúDownloads‚Äù to ‚ÄúAsk for each download‚Äù",
    "crumbs": [
      "What they forgot to teach you about computers",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Understanding file systems</span>"
    ]
  },
  {
    "objectID": "file_systems.html#summary",
    "href": "file_systems.html#summary",
    "title": "\n3¬† Understanding file systems\n",
    "section": "\n3.12 Summary",
    "text": "3.12 Summary\n\nA file system consists of files and folders organized hierarchically. It is crucial to comprehend different file types, navigate the computer‚Äôs file system, and effectively organize and manipulate files.\nPlain text files, also known as ASCII files, contain only text characters without formatting, images, or colors. They have various extensions, such as .txt, .tab, .csv, .R, .py, .fasta, and others.\nWhile file extensions are intended to indicate content and format, it‚Äôs important to remember that programs may behave differently with file extensions. Most text editors will attempt to open any file regardless of the extension, but some programs may only open files with the correct extension.\nA directory is a folder\nIn a file system, files are organized into directories. Directories can contain sub-directories, creating a hierarchical structure. The top-level directory is known as the root directory.\nFile managers, such as File Explorer in Windows and Finder in Mac, are used to interact with the file system.\nThe working directory of a program is the default location where files are read from or written to. It is important to understand working directories when referencing files in code.\nFile paths provide the address or location of a file or directory in the file system. Paths can be absolute or relative. Absolute paths contain the complete list of directories from the root, while relative paths are based on the working directory.\nUsing relative paths is recommended for portability, as it allows files to be referenced relative to the working directory.\n\n\n\n\n\nRand, Emma, Chong, James, Buenabad-Chavez, Jorge, Cansdale, Annabel, Forrester, Sarah, and Greeves, Evelyn. 2022. ‚ÄúCloud-SPAN/00genomics: Cloud-SPAN Genomics Course Overview,‚Äù May. https://doi.org/10.5281/ZENODO.6564314.",
    "crumbs": [
      "What they forgot to teach you about computers",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Understanding file systems</span>"
    ]
  },
  {
    "objectID": "file_systems.html#footnotes",
    "href": "file_systems.html#footnotes",
    "title": "\n3¬† Understanding file systems\n",
    "section": "",
    "text": "Windows uses backslashes because it did not have directories in 1981 when it‚Äôs predecessor, MS DOS, was released. At the time it used the / character for ‚Äòswitches‚Äô (instead of the existing convention - ) so when it did start using directories it couldn‚Äôt use /‚Ü©Ô∏é",
    "crumbs": [
      "What they forgot to teach you about computers",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Understanding file systems</span>"
    ]
  },
  {
    "objectID": "organising_work.html",
    "href": "organising_work.html",
    "title": "\n4¬† Organising your work\n",
    "section": "",
    "text": "4.1 Use folders",
    "crumbs": [
      "What they forgot to teach you about computers",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Organising your work</span>"
    ]
  },
  {
    "objectID": "organising_work.html#be-consistency",
    "href": "organising_work.html#be-consistency",
    "title": "\n4¬† Organising your work\n",
    "section": "\n4.2 Be consistency",
    "text": "4.2 Be consistency",
    "crumbs": [
      "What they forgot to teach you about computers",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Organising your work</span>"
    ]
  },
  {
    "objectID": "organising_work.html#naming-things",
    "href": "organising_work.html#naming-things",
    "title": "\n4¬† Organising your work\n",
    "section": "\n4.3 Naming things",
    "text": "4.3 Naming things",
    "crumbs": [
      "What they forgot to teach you about computers",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Organising your work</span>"
    ]
  },
  {
    "objectID": "getting_started_with_data.html",
    "href": "getting_started_with_data.html",
    "title": "Getting started with data",
    "section": "",
    "text": "First draft\n\n\n\nYou are reading a work in progress. This page is a first draft but should be readable.\n\n\nIn the first part of this book you learnt some foundational ideas about working with computers. Now you are ready to take your first steps into analysing data with R. The first chapter in this part covers important concepts about data: whether they are discrete and continuous and how we summarise them. The second chapter introduces you to R and RStudio for the first time. We start by exploring the layout and appearance then move on to coding. The third chapter describes some useful workflow patterns and tools for organising your work in RStudio. Using these will make learning R easier. Finally, we will go through a complete workflow from importing data from a file to saving a figure for reporting.",
    "crumbs": [
      "Getting started with data"
    ]
  },
  {
    "objectID": "ideas_about_data.html",
    "href": "ideas_about_data.html",
    "title": "\n5¬† Ideas about data\n",
    "section": "",
    "text": "5.1 Roles of variables in analysis\nWhen we do research, we typically have variables that we choose or set and variables that we measure. The variables we choose or set are called independent or explanatory variables. The variables we measure are called dependent or response variables. For example, we might measure the concentration of enzymes and hormones in blood samples of individuals with different genotypes. The genotype acts as the explanatory variable and the blood measurements are response. We would be interested in whether the blood measures differ between genotypes.",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Ideas about data</span>"
    ]
  },
  {
    "objectID": "ideas_about_data.html#distributions",
    "href": "ideas_about_data.html#distributions",
    "title": "\n5¬† Ideas about data\n",
    "section": "\n5.2 Distributions",
    "text": "5.2 Distributions\nAny variable has a distribution which describes the values the variable can take and the chance of them occurring. The distribution is a function (relationship) which captures how the values are mapped to the probability of them occurring. A distribution has a general type, given by the function and is further tuned by the parameters in the function. For example variables like height, length, concentration, mass and intensity follow a normal distribution also know as the bell-shaped curve so that they look similar. However, they have different means and standard deviations. The mean and the standard deviation are the parameters of the normal distribution.\nAn important distinction is between discrete and continuous types of data. Continuous variables are measurements that can take any value in their range (Figure¬†5.1). Discrete variables can take only specific values (Figure¬†5.2).\n\n\n\n\n\n\n\nFigure¬†5.1: A continuous variable has a smooth distribution and the probability of getting a particular value or less is given by the area under the curve. The example is a normal distribution.\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†5.2: A discrete variable has a stepped distribution and the probability of getting exactly a particular is given by the area of the bar at that value. The example is a Poisson distribtion.",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Ideas about data</span>"
    ]
  },
  {
    "objectID": "ideas_about_data.html#summarising-data",
    "href": "ideas_about_data.html#summarising-data",
    "title": "\n5¬† Ideas about data\n",
    "section": "\n5.3 Summarising data",
    "text": "5.3 Summarising data\nWe summarise data by giving a measure of central tendency and a measure of dispersion. A measure of central tendency is a single value that represents the middle or centre of a variable‚Äôs distribution. The three most common measures are:\n\nthe mean, or more accurately, the arithmetic mean, \\(\\bar{x} = \\frac{\\sum{x}}{n}\\)\nthe median: the middle value for a variable with values arranged in order of magnitude\nthe mode: the most frequent value in the variable.\n\nMeasures of dispersion describe the spread of values around the centre and indicate how much variability there is in the variable. The most common measures of dispersion are:\n\nthe range: the difference between the maximum value and the minimum value in a variable\nthe interquartile range: two values, the first quartile and the thrid quartile. The first quartile is half way between the median value and the lowest value when the values are arranged in order and the third quartile is halfway between the median value and the highest value\nthe variance: the average of the squared differences between each value and the variable‚Äôs mean, \\(\\bar{x} = \\frac{(\\sum{x - \\bar{x})^2}}{n - 1}\\)\nthe standard deviation: the square root of the variance.",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Ideas about data</span>"
    ]
  },
  {
    "objectID": "ideas_about_data.html#discrete-data",
    "href": "ideas_about_data.html#discrete-data",
    "title": "\n5¬† Ideas about data\n",
    "section": "\n5.4 Discrete data",
    "text": "5.4 Discrete data\nDiscrete variables can take only specific values and can be categories, like genotype, or counts, like the number of petals.\n\n5.4.1 Categories: Nominal and Ordinal\nCategorical data can be nominal and ordinal depending on whether they are ordered. Nominal variable have no particular order, for example, the eye colour of Drosophila or the species of bird. When summarising data on bird species, it wouldn‚Äôt matter in what order the information was given or plotted. Ordinal variables have an order. The Likert scale Likert (1932) used in questionnaires is one example. The possible responses are Strongly agree, Agree, Disagree and Strongly disagree; these have an order that you would use when plotting them (Figure¬†5.3).\n\n\n\n\n\n\n\nFigure¬†5.3: The categories of bird species are unordered - or nominal - but those in a likert variable are ordinal.\n\n\n\n\nThe most appropriate way to summarise nominal or ordinal data is to report the mode (most frequent value) or tabulate the number of each value (Table¬†5.1).\n\n\n\nTable¬†5.1: Frequency of reposnes.\n\n\n\n\nResponse\nFrequency\n\n\n\nStrongly agree\n15\n\n\nAgree\n18\n\n\nDisagree\n10\n\n\nStrongly disagree\n2\n\n\n\n\n\n\n\n\n\n5.4.2 Counts\nCounts are one of the most common data types. They are quantitative but discrete because they can take only specific values. Counts tend to follow a distribution called the Poisson distribution meaning that there is an expected number of zeros, ones and twos etc that appear. This is true no matter what you count. The distribution of counts is not symmetrical and has a tail to the right - especially for lower count ranges.\nA mean and standard deviation are not usually the best way to summarise a count variable because their distribution is not symmetrical. Figure¬†5.4 shows the number of groundsel plants in a hundred 1 metre square quadrats1.\n\n\n\n\n\n\n\nFigure¬†5.4: Counts are discrete. They often follow a Poisson distribution.\n\n\n\n\nThe mean of 0.73 groundsel plants per quadrat does not really reflect that the majority of quadrats contained no plants. The average is being dragged up by the few quadrats containing 3 plants.\nFor count variables - and other variables that are not symmetrical - it is often better to give the median and interquartile range.\nThe median is the middle value when the values are arranged in order. The interquartile range (IQR) indicates the spread in the data. The lower quartile is half way between the lowest value and the middle value and the upper quartile is half way between the middle value and the highest value.",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Ideas about data</span>"
    ]
  },
  {
    "objectID": "ideas_about_data.html#continuous-data",
    "href": "ideas_about_data.html#continuous-data",
    "title": "\n5¬† Ideas about data\n",
    "section": "\n5.5 Continuous data",
    "text": "5.5 Continuous data\nContinuous variables are measurements that can take any value in their range so there are an infinite number of possible values. The values have decimal places. Variables like the length and mass of an organism, the volume and optical density of a solution, or the colour intensity of an image are continuous. Many response variables are continuous but continuous variables can also be explanatory. A large number of continuous variables follow the normal distribution.\n\n5.5.1 The normal distribution\nFor example, a variable like human height has values with decimal places which follow a normal distribution also known as the Gaussian distribution or the bell-shaped curve. Values of 1.65 metres occur more often than values of 2 metres and values of 3 metres never occur (Figure¬†5.5).\n\n\n\n\n\n\n\nFigure¬†5.5: Human height follows a normal distribution.\n\n\n\n\nThe mean and standard deviation are the best way to summarise a normally distributed variable.",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Ideas about data</span>"
    ]
  },
  {
    "objectID": "ideas_about_data.html#theory-and-practice",
    "href": "ideas_about_data.html#theory-and-practice",
    "title": "\n5¬† Ideas about data\n",
    "section": "\n5.6 Theory and practice",
    "text": "5.6 Theory and practice\nThe distinction between continuous and discrete values is clear in theory but in practice, the actual values you have may differ from the expected distribution for a particular variable. For example, we would expect the mass of cats to be continuous because any value in the range is possible. However, if our scales only measure to the nearest kilogram, then the values become discrete because the only the values possible are 0, 1, 2, 3, 4, 5, 6 and 7. The gap between values, 1kg, is big compared to the range of values we expect (Figure¬†5.6)\n\n\n\n\n\n\n\nFigure¬†5.6: The mass of cats in kilograms is theoretically continuous (left). If we measure only to the nearest kilogram, mass becomes discrete because 1kg covers a large chunk of the range (right).\n\n\n\n\nIn contrast, the number of hairs on a human head ranges from about 80,000 to 150,000 so the difference between having 100,203 hairs and 100,204 hairs is so tiny the variable is practically continuous (Figure¬†5.7).\n\n\n\n\n\n\n\nFigure¬†5.7: The number of hairs on a human head discrete. But one increment is a tiny proportion of the range of values so the number of hairs is practically continuous.\n\n\n\n\nA rule of thumb is that if the mean count is above about 100, then the distribution of counts closely matches the normal distribution.",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Ideas about data</span>"
    ]
  },
  {
    "objectID": "ideas_about_data.html#summary",
    "href": "ideas_about_data.html#summary",
    "title": "\n5¬† Ideas about data\n",
    "section": "\n5.7 Summary",
    "text": "5.7 Summary\n\nData: Data consists of variables (properties measured or recorded) and observations (individual things with those properties). Data is commonly organised with variables in columns and observations in rows.\nRoles of Variables in Analysis: Variables in research can be independent or explanatory variables (chosen or set by researchers) and dependent or response variables (measured by researchers).\nDistributions: A variable‚Äôs distribution describes the types of values it can take and the likelihood of each value occurring. Variables can be classified as discrete (specific values) or continuous (any value within a range). The shape of a distribution is determined by parameters.\nDiscrete Data: Discrete variables can be categories or counts. Categorical data can be nominal (no order) or ordinal (ordered) and are best summarised with the mode or a table. Counts are often best summarised by the median and interquartile range.\nContinuous Data: Continuous variables can take any value within a range. The normal distribution is the most common continuous distribution and is best summarised with the mean and standard deviation.\nTheory and Practice: While the theoretical distinction between continuous and discrete values is clear, the actual values obtained in practice may deviate from the expected distribution. Measurement precision and range affect whether a variable is considered continuous or discrete. A rule of thumb suggests that if the mean count is above approximately 100, the distribution of counts approximates a normal distribution.\n\n\n\n\n\nLikert, R. 1932. ‚ÄúA Technique for the Measurement of Attitudes.‚Äù Archives of Psychology 22 140: 55‚Äì55.",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Ideas about data</span>"
    ]
  },
  {
    "objectID": "ideas_about_data.html#footnotes",
    "href": "ideas_about_data.html#footnotes",
    "title": "\n5¬† Ideas about data\n",
    "section": "",
    "text": "A¬†quadrat¬†is a frame used in¬†ecology¬†to outline a standard unit of area for study of the distribution of plant species over a wider are. Typically, quadrats are placed randomly over the area and the number of individuals in each quadrat is recorded.‚Ü©Ô∏é",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Ideas about data</span>"
    ]
  },
  {
    "objectID": "first_steps_rstudio.html",
    "href": "first_steps_rstudio.html",
    "title": "\n6¬† First Steps in RStudio\n",
    "section": "",
    "text": "6.1 What are R and Rstudio?",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>First Steps in RStudio</span>"
    ]
  },
  {
    "objectID": "first_steps_rstudio.html#what-are-r-and-rstudio",
    "href": "first_steps_rstudio.html#what-are-r-and-rstudio",
    "title": "\n6¬† First Steps in RStudio\n",
    "section": "",
    "text": "6.1.1 What is R?\nR is a programming language and environment for statistical computing and graphics which is free and open source. It is widely used in industry and academia. It is what is known as a ‚Äúdomain-specific‚Äù language meaning that it is designed especially for doing data analysis and visualisation rather than a ‚Äúgeneral-purpose‚Äù programming language like Python and C++. It makes doing the sorts of things that bioscientists do a bit easier than in a general purpose-language.\n\n6.1.2 What is RStudio?\nRStudio is what is known as an ‚Äúintegrated development environment‚Äù (IDE) for R made by Posit. IDEs have features that make it easier to do coding like syntax highlighting, code completion and viewers for files, code objects, packages and plots. You don‚Äôt have to use RStudio to use R but it is very helpful.\n\n6.1.3 Why is it better to use R than Excel, googlesheets or some other spreadsheet program?\nSpreadsheet programs are not statistical packages so although you can carry out some analysis tasks in them they are limited, get things wrong (known about since 1994) and teach you bad data habits. Spreadsheets encourage you to do things that are going to make analysis difficult.\n\n6.1.4 Why is it better to use R than SPSS, Minitab or some other menu-driven statistics program?\n\nR is free and open source which it will always be available to you .\nCarrying out data analysis using coding makes everything you do reproducible\nThe skills and expertise you gain through learning R are highly transferable ‚Äì much more so than those acquired using SPSS.\nSee Thomas Mock‚Äôs demonstration of doing some data analysis in R including ‚ÄúThe Kick Ass Curve‚Äù.\n\nThere are other good options such as Julia and Python and you are encouraged to explore these. We chose R in part because of the R community which is one of R‚Äôs greatest assets, being vibrant, inclusive and supportive of users at all levels. https://ropensci.org/blog/2017/06/23/community/",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>First Steps in RStudio</span>"
    ]
  },
  {
    "objectID": "first_steps_rstudio.html#installing-r-and-rstudio",
    "href": "first_steps_rstudio.html#installing-r-and-rstudio",
    "title": "\n6¬† First Steps in RStudio\n",
    "section": "\n6.2 Installing R and Rstudio",
    "text": "6.2 Installing R and Rstudio\nYou will need to install both R and RStudio to use them on your own machine. Installation is normally straightforward but you can follow a tutorial\n\n6.2.1 Installing R\nGo to https://cloud.r-project.org/ and download the ‚ÄúPrecompiled binary distributions of the base system and contributed packages‚Äù appropriate for your machine.\n\n6.2.1.1 For Windows\nClick ‚ÄúDownload R for Windows‚Äù, then ‚Äúbase‚Äù, then ‚ÄúDownload R 4.#.# for Windows‚Äù. This will download an .exe file. Once downloaded, open (double click) that file to start the installation.\n\n6.2.1.2 For Mac\nClick ‚ÄúDownload R for (Mac) OS X‚Äù, then ‚ÄúR-4.#.#.pkg‚Äù to download the installer. Run the installer to complete installation.\n\n6.2.1.3 For Linux\nClick ‚ÄúDownload R for Linux‚Äù. Instructions on installing are given for Debian, Redhat, Suse and Ubuntu distributions. Where there is a choice, install both r-base and r-base-dev.\n\n6.2.2 Installing R Studio\nGo to https://posit.co/download/rstudio-desktop/",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>First Steps in RStudio</span>"
    ]
  },
  {
    "objectID": "first_steps_rstudio.html#packages",
    "href": "first_steps_rstudio.html#packages",
    "title": "\n6¬† First Steps in RStudio\n",
    "section": "\n6.3 Packages",
    "text": "6.3 Packages\nTODO some text\nInstall tidyverse:\n\ninstall.packages(\"tidyverse\")\ninstall.packages(\"readxl\")",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>First Steps in RStudio</span>"
    ]
  },
  {
    "objectID": "first_steps_rstudio.html#introduction-to-rstudio",
    "href": "first_steps_rstudio.html#introduction-to-rstudio",
    "title": "\n6¬† First Steps in RStudio\n",
    "section": "\n6.4 Introduction to RStudio",
    "text": "6.4 Introduction to RStudio\nIn this section we will introduce you to working in RStudio. We will explain the windows that you see when you first open RStudio and how to change its appearance to suit you. Then we will see how we use R as a calculator and how assign values to R objects.\n\n6.4.1 Changing the appearance\nWhen you first open RStudio it will display three panes and have a white background Figure¬†6.1\n\n\n\n\n\nFigure¬†6.1: When you first open RStudio it will be white with three panes\n\n\nWe will talk more about these three panes soon but first, let‚Äôs get into character - the character of a programmer! You might have noticed that people comfortable around computers are often using dark backgrounds. A dark background reduces eye strain and often makes ‚Äúcode syntax‚Äù more obvious making it faster to learn and understand at a glance. Code syntax is the set of rules that define what the various combinations of symbols mean. It takes time to learn these rules and you will learn best by repeated exposure to writing, reading and copying code. You have done this before when you learned your first spoken language. All languages have syntax rules governing the order of words and we rarely think about these consciously, instead relying on what sounds and looks right. And what sounds and looks right grows out repeated exposure. For example, 35% of languages, including English, Chinese, Yoruba and Polish use the Subject-Verb-Object syntax rule:\n\nEnglish: Emma likes R\nChinese: ËâæÁéõÂñúÊ¨¢R Emma x«êhuƒÅn R\nYoruba: Emma f·∫πran R\nPolish: Emma lubi R\n\nand 40% use Subject-Object-Verb including Turkish and Korean\n\nTurkish: Emma R‚Äôyi seviyor\nKorean: Ïó†ÎßàÎäî RÏùÑ Ï¢ãÏïÑÌïúÎã§ emmaneun Reul joh-ahanda\n\nYou learned this rule in your language very early, long before you were conscious of it, just by being exposed to it frequently. In this book I try to tell you the syntax rules, but you will learn most from looking at, and copying code. Because of this, it is well worth tinkering with the appearance of RStudio to see what Editor theme makes code elements most obvious to you.\nThere is a tool bar at the top of RStudio. Choose the Tools option and then Global options. This will open a window where many options can be changed Figure¬†6.2.\n\n\n\n\n\nFigure¬†6.2: Tools | Global Options opens a window. One of the options is Appearance\n\n\nGo to the Appearance Options and choose and Editor theme you like, followed by OK.\nThe default theme is Textmate. You will notice that all the Editor themes have syntax highlighting so that keywords, variable names, operators, etc are coloured but some themes have stronger contrasts than others. For beginners, I recommend Vibrant Ink, Chaos or Merbivore rather than Dreamweaver or Gob which have little contrast between some elements. However, individuals differ so experiment for yourself. I tend to vary between Solarised light and dark.\nYou can also turn one Screen Reader Support in the Accessibility Options in Tools | Global Options.\nBack to the Panes. You should be looking at three windows: One on the left and two on the right1.\nThe window on the left, labelled Console, is where R commands are executed. In a moment we will start by typing commands in this window. Over on the right hand side, at the top, have several tabs, with the Environment tab showing. This is where all the objects and data that you create will be listed. Behind the Environment tab is the History and later you will be able to view this to see a history of all your commands.\nOn the bottom right hand side, we have a tab called Plots which is where your plots will go, a tab called Files which is a file explorer just like Windows Explorer or Mac Finder, and a Packages tab where you can see all the packages that are installed. The Packages tab also provides a way to install additional packages. The Help tab has access to all the manual pages.\nRight, let‚Äôs start coding!\n\n6.4.2 Your first piece of code\nWe can use R just like a calculator. Put your cursor after the &gt; in the Console, type 3 + 4 and ‚Üµ Enter to send that command:\n\n3+4\n## [1] 7\n\nThe &gt; is called the ‚Äúprompt‚Äù. You do not have to type it, it tells you that R is ready for input.\nWhere I‚Äôve written 3+4, I have no spaces. However, you can have spaces, and in fact, it‚Äôs good practice to use spaces around your operators because it makes your code easier to read. So a better way of writing this would be:\n\n3 + 4\n## [1] 7\n\nIn the output we have the number 7, which, obviously, is the answer. From now on, you should assume commands typed at the console should be followed by ‚Üµ Enter to send them.\nThe one in parentheses, [1], is an index. It is telling you that the 7 is the first element of the output. We can see this more clear if we create something with more output. For example, 50:100 will print the numbers from 50 to 100.\n\n50:100\n##  [1]  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68\n## [20]  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87\n## [39]  88  89  90  91  92  93  94  95  96  97  98  99 100\n\nThe numbers in the square parentheses at the beginning of the line give you the index of the first element in the line. R is telling you where you are in the output.\n\n6.4.3 Assigning variables\nVery often we want to keep input values or output for future use. We do this with ‚Äòassignment‚Äô An assignment is a statement in programming that is used to set a value to a variable name. In R, the operator used to do assignment is &lt;-. It assigns the value on the right-hand to the value on the left-hand side.\nTo assign the value 3 to x we do:\n\nx &lt;- 3\n\nand ‚Üµ Enter to send that command.\nThe assignment operator is made of two characters, the &lt; and the - and there is a keyboard short cut: Alt+- (windows) or Option+- (Mac). Using the shortcut means you‚Äôll automatically get spaces. You won‚Äôt see any output when the command has been executed because there is no output. However, you will see x listed under Values in the Environment tab (top right).\nYour turn! Assign the value of 4 to a variable called y:\n\nCodey &lt;- 4\n\n\nCheck you can see y listed in the Environment tab.\nType x and ‚Üµ Enter to print the contents of x in the console:\n\nx\n## [1] 3\n\nWe can use these values in calculations just like we could in in maths and algebra.\n\nx + y\n## [1] 7\n\nWe get the output of 7 just as we expect. Suppose we make a mistake when typing, for example, accidentally pressing the u button instead of the y button:\n\nx + u\n## Error in eval(expr, envir, enclos): object 'u' not found\n\nWe get an error. We will probably see this error quite often - it means we have tried to use a variable that is not in our Environment. So when you get that error, have a quick look up at your environment2.\nWe made a typo and will want to try again. We usefully have access to all the commands that previously entered when we use the ‚Üë Up Arrow. This is known as command recall. Pressing the ‚Üë Up Arrow once recalls the last command; pressing it twice recalls the command before the last one and so on.\nRecall the x + u command (you may need to use the ‚Üì Down Arrow to get back to get it) and use the Back space key to remove the u and then add a y.\nA lot of what we type is going to be wrong - that is not because you are a beginner, it is same for everybody! On the whole, you type it wrong until you get it right and then you move to the next part. This means you are going to have to access your previous commands often. The History - behind the Environment tab - contains everything you can see with the ‚Üë Up Arrow. You can imagine that as you increase the number of commands you run in a session, having access the this record of everything you did is useful. However, the thing about the history is, that it has everything you typed, including all the wrong things!\nWhat we really want is a record of everything we did that was right! This is why we use scripts instead of typing directly into the console.\n\n6.4.4 Using Scripts\nAn R script is a plain text file with a .R extension and containing R code. Instead of typing into the console, we normally type into a script and then send the commands to the console when we are ready to run them. Then if we‚Äôve made any mistakes, we just correct our script and by the end of the session, it contains a record of everything we typed that worked.\nYou have several options open a new script:\n\nbutton: Green circle with a white cross, top left and choose R Script\nmenu: File | New File | R Script\nkeyboard shortcut: Ctrl+Shift+N (Windows) / Shift+Command+N (Mac)\n\nOpen a script and add the two assignments to it:\n\nx &lt;- 3\ny &lt;- 4\n\nTo send the first line to the console, we place our cursor on the line (anywhere) and press Ctrl-Enter (Windows) / Command-Return. That line will be executed in the console and in the script, our cursor will jump to the next line. Now, send the second command to the console in the same way.\nFrom this point forward, you should assume commands should be typed into the script and sent to the console.\nAdd the incorrect command attempting to sum the two variables:\n\nx + u\n## Error in eval(expr, envir, enclos): object 'u' not found\n\nTo correct this, we do not add another line to the script but instead edit the existing command:\n\nx + y\n## [1] 7\n\nIn addition to making it easy to keep a record of your work, scripts have another big advantage, you can include ‚Äòcomments‚Äô - pieces of text that describe what the code is doing. Comments are indicated with a # in front of them. You can write anything you like after a # and R will recognise that it is not code and doesn‚Äôt need to be run.\n\n# This script performs the sum of two values\n\nx &lt;- 3    # can be altered\ny &lt;- 4    # can be altered\n\n# perform sum\nx + y\n## [1] 7\n\nThe comments should be highlighted in a different colour than the code. They will be italic in some Editor themes.\nYou have several options save a script:\n\nbutton: use the floppy disc icon\nmenu: File | Save\nkeyboard shortcut: Ctrl+S (Windows) / Command+S (Mac)\n\nYou could use a name like test1.R - note the .R extension wil be added automatically.\n\n6.4.5 Other types of file in RStudio\n\n\n.R script code but not the objects. You always want to save this\n\n.Rdata also known as the workspace or session, the objects, but not the code. You usually do not want to save this. Some exceptions e.g., if it takes quite a long time to run the commands.\ntext files\n\n6.4.6 Changing some defaults to make life easier\nI recommend changing some of the default settings to make your life a little easier. Go back into the Global Options window with Tools | Global Options. The top tab is General Figure¬†6.3.\n\n\n\n\n\nFigure¬†6.3: Tools | Global Options opens a window. One of the options is General. This where you can change the default behaviour of RStudio. Highlighted is the default (start up) directory and the option to Save and Restore the workspace.\n\n\nFirst, we will set our default working directory. Under ‚ÄòDefault working directory (when not in a project3):‚Äô click Browse and navigate to a through your file system to a folder where you want to work. You may want to create a folder specifically for studying this book.\nSecond, we will change the Workspace options. Turn off ‚ÄòRestore .RData into workspace at startup‚Äô and change ‚ÄòSave workspace to .RData on exit‚Äô to ‚ÄòNever‚Äô. These options mean R will start up clean each time.",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>First Steps in RStudio</span>"
    ]
  },
  {
    "objectID": "first_steps_rstudio.html#recap-of-rstudio-anatomy",
    "href": "first_steps_rstudio.html#recap-of-rstudio-anatomy",
    "title": "\n6¬† First Steps in RStudio\n",
    "section": "\n6.5 Recap of RStudio anatomy",
    "text": "6.5 Recap of RStudio anatomy\nThis figure (see Figure¬†6.44) summarises shows what each of the four RStudio panes and what they are used for to summarise much of what we have covered so far.\n\n\n\n\n\n\n\nFigure¬†6.4: A screenshot of RStudio‚Äôs four panes annotated with what each pane is for.",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>First Steps in RStudio</span>"
    ]
  },
  {
    "objectID": "first_steps_rstudio.html#data-types-and-structures-in-r",
    "href": "first_steps_rstudio.html#data-types-and-structures-in-r",
    "title": "\n6¬† First Steps in RStudio\n",
    "section": "\n6.6 Data types and structures in R",
    "text": "6.6 Data types and structures in R\nIn this section, we are going to introduce you to some of R‚Äôs data types and structures. We won‚Äôt be covering all of them now, just those you are going to use often in this part of the book. These are numerics (numbers), characters, ‚Äòlogicals‚Äô, vectors and dataframes. We can do a lot with just these. We will also cover using functions.\nWe are going to consider\n\ntypes of value also known as data types\nfunctions\ndata structures\n\n\n6.6.1 Data types\nThis refers to the type of value that something is. They might be numerics or characters or ‚Äòlogical‚Äô (either true of false). We assign a number, like the value of 23 to a variable called x like this:\n\nx &lt;- 23\nx\n## [1] 23\n\nWe do not need to use quotes for numbers but we do need to use them for characters and can assign the word banana to the variable a like this:\n\na &lt;- \"banana\"\na\n## [1] \"banana\"\n\nQuotes are needed because otherwise R wouldn‚Äôt know whether you were referring to a value or a existing object called banana. This is also why you can‚Äôt have variable name like 14 - R would not be able to tell the difference between the number 14 and an object named 14 since numbers and objects don‚Äôt need quotes.\nAnything composed of non-numeric characters, including single characters, need to have quotes around it. You can even force a number to be a character by putting quotes around it:\n\nb &lt;- \"23\"\nb\n## [1] \"23\"\n\nNotice that things inside quotes appear in a different colour (the colour will depend on the Editor theme you choose). This will help you identify when you have forgotten some closing quotes5:\n\na &lt;- \"banana\nx &lt;- 23\n\nAlthough the data type is ‚Äòcharacter‚Äô we often use the term ‚Äòstring‚Äô for collections - strings - of characters\nWe also have special values called ‚Äòlogicals‚Äô which take a value of either TRUE or FALSE.\n\nc &lt;- TRUE\nc\n## [1] TRUE\n\nAlthough TRUE is a word, R recognises it as special word. It appears in a different colour and no quotes are needed. This is the same for FALSE.\n\nc &lt;- FALSE\nc\n## [1] FALSE\n\nAs you type FALSE, the colour changes as it recognises the special word FALSE. Try to pay attention to the editor theme‚Äôs colouring - it is trying to help you!\n\n6.6.2 Functions\nThe aim of this section is to help you understand the logic of using a function. Functions have a name and then a set of parentheses. The function name minimally explains what the function does. Inside the parentheses are ‚Äòarguments‚Äô - the pieces of information you give to the function. When coding, we often talk about passing arguments to functions and calling functions. A simple function call looks like this:\nfunction_name(argument)\nA function can take zero to many arguments. Where you give several arguments, they are separated by commas:\nfunction_name(argument1, argument2, argument3, ...)\nThe first function you are going to use is str() which gives the structure of an object:\n\nstr(x)\n##  num 23\n\nIt‚Äôs telling me that x is a number and contains 23.\nYour turn! Use str() on b:\n\nCodestr(b)\n##  chr \"23\"\n\n\nstr() is a function I use a lot to check what sort of R object I have.\nWe must give str() at least one argument, the object we want the structure of, but additional arguments are also possible. Later we will discover how to find out and use a function‚Äôs arguments.\nSo far we our objects have consisted of a single thing but usually we have several bits of data that we want to collect together into a data structure.\n\n6.6.3 Data structures: vectors\nImagine we have the ages of six people. Since all the numbers are ages, we would want to keep them together. The minimal data structure is called a vector. We can create a vector, which collects together several numbers using a function, c(). This is one of only a few functions in R with a single-letter name. Because it has a single letter, sometimes people get confused about it but we can tell it is a function because it has a set of parentheses.\nTo create a vector called ages of several numbers we use\n\nages &lt;- c(23, 42, 7, 9, 54, 12)\n\nType ages and run if you want to print the contents to the console:\n\nages\n## [1] 23 42  7  9 54 12\n\nUsing str() on ages\n\nstr(ages)\n##  num [1:6] 23 42 7 9 54 12\n\ntells us we have numbers with the indices of 1 to 6.\nWe can also create a vector of strings. Suppose we have names to go with the ages.\n\nnames &lt;- c(\"Rowan\", \"Aang\", \"Zain\", \"Charlie\", \"Jules\", \"Efe\")\n\nRStudio has a super useful feature for putting items in quotes, brackets or parentheses if you initially forget them: If you select something and type the opening element, that thing will be surrounded rather than over written. For example, if you had written:\n\nnames &lt;- Rowan, Aang, Zain, Charlie, Jules, Efe\n\nSelect one of the names and then type an opening quote - you should see the name is then surrounded by quotes rather than overwritten. You can repeat this process for all the names (note that double clicking on the name will select it). Selecting the whole list and typing an opening parenthesis will put the whole list in parentheses. This is a feature you get to love so much and use so often that other programs will annoy you when you overwrite something you meant to quote!\nSo we can also have vectors of logical values. For example, we might have a vector that indicates that Rowan, Aang and Charlie like chocolate, but Zain, Jules and Efe do not:\n\nchocolate &lt;- c(TRUE, TRUE, FALSE, TRUE, FALSE, FALSE)\n\nRemember, because TRUE and FALSE are special words so we do not need quote them.\n\n6.6.4 Indexing vectors\nYou might be wondering how to get a single element out of a vector if typing the vector‚Äôs name prints the entire vector. This is by ‚Äòindexing‚Äô. An index is a number from 16 to the length of a vector which gives the position in the vector and is denoted by square brackets. For example, to pull out the second element of ages:\n\nages[2]\n## [1] 42\n\nYour turn! Print the last element of names:\n\nCodenames[6]\n## [1] \"Efe\"\n\n\nWe an extract more than one element by giving more than one index. If the indices are adjacent like 3rd, 4th and 5th, we have use the colon:\n\nnames[3:5]\n## [1] \"Zain\"    \"Charlie\" \"Jules\"\n\nIf the indices are not adjacent, like 2 and 6 with need to combine them with c():\n\nnames[c(2, 6)]\n## [1] \"Aang\" \"Efe\"\n\nWe can also use a logical vector to extract elements. Suppose you want to extract the names and ages of the people who like chocolate:\n\nnames[chocolate]\n## [1] \"Rowan\"   \"Aang\"    \"Charlie\"\nages[chocolate]\n## [1] 23 42  9\n\nAt each of the indices in chocolate that contain TRUE, the name and age are returned.\n\n6.6.5 Changing the defaults for a function\nThe functions we have used so far, c() and str() have worked without us having to change default behaviour. For example, if we want to calculate the mean age of our people we can use the mean() function in default form:\n\nmean(ages)\n## [1] 24.5\n\nImagine Charlie would like their age removed from the dataset or that we never knew their age. We would not want a vector containing just five elements because the ages would not match the people in the same position in the vector. Instead, we would have a missing value at that position. Missing values are NA (not applicable) in R and NA is another special word, like TRUE and FALSE, that doesn‚Äôt need quotes. We can set Charlie‚Äôs age to NA using indexing:\n\nages[names == \"Charlie\"] &lt;- NA\nages\n## [1] 23 42  7 NA 54 12\n\nThe == means ‚Äúis equal to‚Äù and the result of names == \"Charlie\" is a vector of logicals: FALSE FALSE FALSE  TRUE FALSE FALSE. This means ages[names == \"Charlie\"] references the age in ages at the index of Charlie in names\nIf we now try to calculate the mean age:\n\nmean(ages)\n## [1] NA\n\nWe get an NA! What we really want is an average of the ages we do have. The mean() function has an argument that allows you to cope with that situation called na.rm. By default, na.rm is set to FALSE but we can set it to TRUE using\n\nmean(ages, na.rm = TRUE)\n## [1] 27.6\n\n\n6.6.6 Data structures: dataframes\nWe have three vectors, names, ages and chocolate which are all part of the same dataset. By far the most common way of organising data in R is within a ‚Äúdataframe‚Äù. A dataframe, has rows and columns: each column represents a variable and each row represents a case. To make a dataframe using our three vectors we use:\n\npeople &lt;- data.frame(names, ages, chocolate)\n\nYou will see people listed in the Global environment under Data. To open a spreadsheet-like view of the dataframe click its name in the Global Environment Figure¬†6.5\n\n\n\n\n\nFigure¬†6.5: To open a spreadsheet-like view of the dataframe click its name in the Global Environment",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>First Steps in RStudio</span>"
    ]
  },
  {
    "objectID": "first_steps_rstudio.html#summary",
    "href": "first_steps_rstudio.html#summary",
    "title": "\n6¬† First Steps in RStudio\n",
    "section": "\n6.7 Summary",
    "text": "6.7 Summary\nTODO",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>First Steps in RStudio</span>"
    ]
  },
  {
    "objectID": "first_steps_rstudio.html#footnotes",
    "href": "first_steps_rstudio.html#footnotes",
    "title": "\n6¬† First Steps in RStudio\n",
    "section": "",
    "text": "If this is not a fresh install of RStudio, you might be looking at fours windows, two on the left and two on the right. That‚Äôs fine - we will al be using four shortly. For the time being, you might want to close the ‚ÄúScript‚Äù window using the small cross next to ‚ÄúUntitled1‚Äù.‚Ü©Ô∏é\nWhen we are using scripts, it is very easy to write code but forget to run it. Very often when you see this error it will because you have written the code to create an object but forgotten to execute it.‚Ü©Ô∏é\nWe will find out what an RStudio Project is very soon. You will want to use a project for most of your work - they make everything a little easier.‚Ü©Ô∏é\nYou can zoom into this at the Direct link‚Ü©Ô∏é\nRStudio makes it hard for you to forget closing quotes and parentheses because when you type an opening quote or parenthesis, it automatically adds its closing partner. When people are learning they are sometimes tempted to delete these so they can type what goes inside the quotes/parentheses and then manually add the closing partner. I strongly recommend you don‚Äôt not delete them. RStudio adds the closing character but it leaves your cursor in the right position to complete the contents.‚Ü©Ô∏é\nWe start counting from 1 in R. Most programming languages count from zero.‚Ü©Ô∏é",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>First Steps in RStudio</span>"
    ]
  },
  {
    "objectID": "workflow_rstudio.html",
    "href": "workflow_rstudio.html",
    "title": "\n7¬† Workflow in RStudio\n",
    "section": "",
    "text": "7.1 RStudio Projects",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Workflow in RStudio</span>"
    ]
  },
  {
    "objectID": "workflow_rstudio.html#rstudio-projects",
    "href": "workflow_rstudio.html#rstudio-projects",
    "title": "\n7¬† Workflow in RStudio\n",
    "section": "",
    "text": "7.1.1 What is an RStudio Project?\nUsing an RStudio Project will help you organise your analysis work, make it much easier to manage working directories and paths and also to collaborate with others including yourself on another computer! An RStudio Project is a folder that contains a file with the extension .RProj and all the code, data, and other files associated with a particular piece of work.\nFor example, if you were analysing some data on stem cells you might have an RStudio Project called ‚Äústem-cells‚Äù. This would be a folder called stem-cells, known as the project folder, which contains the stem-cells.Rproj file - both of these are created automatically. Then you might create folders for the data and for figures from the analysis along with the script that contains code for importing the data, doing the analysis, creating the figures and writing the figures to file.\n-- stem-cells\n   |__stem-cells.Rproj\n   |__analysis.R\n   |__data-raw\n      |__2019-03-21_donor_1.csv\n      |__2019-03-21_donor_2.csv\n      |__2019-03-21_donor_3.csv\n   |__figures\n      |__01_volcano_donor_1_vs_donor_2.png\n      |__02_volcano_donor_1_vs_donor_3.png\n\nWhen you open an RStudio Project, it automatically sets the project directory as the working directory. This means when you write your code with paths relative to the project directory your code will work the same on any computer you send that RStudio Project to.\n\n7.1.2 Creating an RStudio Project\n\nClick on the drop-down menu on top right where it says ‚ÄúProject: (None)‚Äù and choose New Project\nA dialogue box will appear. Choose ‚ÄúNew Directory‚Äù, then ‚ÄúNew Project‚Äù\nClick the Browse button next to ‚ÄúCreate project as a subdirectory of:‚Äù to navigate to a place in your file system where you want to create the project folder.\nType a name the ‚ÄúDirectory name‚Äù. This should be something that helps you identify the contents. Follow the advice in Naming things",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Workflow in RStudio</span>"
    ]
  },
  {
    "objectID": "workflow_rstudio.html#some-useful-settings",
    "href": "workflow_rstudio.html#some-useful-settings",
    "title": "\n7¬† Workflow in RStudio\n",
    "section": "\n7.2 Some useful settings",
    "text": "7.2 Some useful settings\nYou can adjust some of the default settings in RStudio you suit your own needs better. The settings are accessed through the Tools Menu under Global options. I like the following settings:\n\nWhen using RStudio Projects the working directory is the Project directory but when you start RStudio up and want to make a new project you might find the default location doesn‚Äôt suit you. You can change the default directory when not in a project.\nTo ensure you have a fresh session with no R objects in the workspace you can change Workspace options.\n\nIn the Code options\n\nDisplay - check Use rainbow brackets which makes it easier to see which bracket are pairs\nDisplay - check Show margin which will add a line at 80 characters to help you use new lines more often and not create very long lines of code that are difficult to read\nDiagnostics - check Show diagnostics for R which will put a marker on the line that includes a syntax error and make a suggestion what the error is\nDiagnostics - check Provide R style diagnostics (e.g.¬†whitespace) which will help you layout your code",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Workflow in RStudio</span>"
    ]
  },
  {
    "objectID": "workflow_rstudio.html#handy-housekeeping-command",
    "href": "workflow_rstudio.html#handy-housekeeping-command",
    "title": "\n7¬† Workflow in RStudio\n",
    "section": "\n7.3 Handy housekeeping command",
    "text": "7.3 Handy housekeeping command\n\n7.3.1 Where am I?\nThere are several ways you can find out what your working directory is.\n\nCode. The getwd() (get working directory)\n\n\ngetwd()\n\n\nAlong the top of the Console window. There is also a little arrow you can click to show your working directory in the Files pane.\nIn the Files pane, provided to have not navigated around in there. If you have, you can view your working directory using blue wheel and choosing ‚ÄúGo To Working Directory‚Äù or by using the arrow on the top of the Console window\n\n7.3.2 What files can I see?\nThere are several ways you can see the files and folders in your working directory.\n\n\nCode. The dir() (directory)\n\ndir()\n\n\n\ndir() list the contents of the working directory\n\ndir(\"..\") list the contents of the directory above the working directory\n\ndir(../..) list the contents of the directory two directories above the working directory\n\ndir(\"data-raw\") list the contents of a folder call data-raw which is in the working directory.\n\n\nLook in the the Files pane, provided to have not navigated around in there. If you have, you can view your working directory using blue wheel and choosing ‚ÄúGo To Working Directory‚Äù or by using the arrow on the top of the Console window\n\n7.3.3 What R objects can I see?\n\n\nCode. The ls() (list)\n\nls()\n\n\nLook in the the Environment pane",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Workflow in RStudio</span>"
    ]
  },
  {
    "objectID": "workflow_rstudio.html#understanding-the-pipe",
    "href": "workflow_rstudio.html#understanding-the-pipe",
    "title": "\n7¬† Workflow in RStudio\n",
    "section": "\n7.4 Understanding the pipe |>\n",
    "text": "7.4 Understanding the pipe |&gt;\n\nThe pipe operator improves code readability by:\n\nstructuring sequences of data operations left-to-right and top to bottom rather than from inside and out),\nminimizing the need for intermediates,\nmaking it easy to add steps anywhere in the sequence of operations.\n\nFor example, suppose we want to apply a log-square root transformation which is sometimes applied to make a flat distribution more normal. There are two approaches we could use without the pipe: nesting the functions and creating an intermediate. We will consider both of these. First, let us generate a few numbers of work with:\n\n# generate some numbers\n# this will give me ten random numbers between 1 and 100\nnums &lt;- sample(1:100, size = 10)\n\nTo apply the transformation we can nest the functions so the output put of sqrt(nums) becomes the input of log():\n\n# apply a log-square root transformation\ntnums &lt;- log(sqrt(nums))\ntnums\n##  [1] 2.131340 1.880600 2.221326 1.763180 1.892095 2.292484 2.255430 1.629048\n##  [9] 2.152033 1.956012\n\nThe first function to be applied is innermost. When we are using just two functions, the level of nesting does not cause too much difficulty in reading the code. However, you can image this gets more unreadable as the number of functions applied increases. It also makes it harder to debug and find out where an error might be. One solution is to create intermediate variables so the commands a given in order:\n\n# apply a log-square root transformation\nsqrtnums &lt;- sqrt(nums)\ntnums &lt;- log(sqrtnums)\n\nUsing intermediates make your code easier to follow at first but clutters up your environment and code with variables you don‚Äôt care about. You also start of run out names!\nThe pipe is a more elegant and readable solution. It allows you to send the output of one operation as input to the next function. The pipe has long been used by Unix operating systems (where the pipe operator is |). The R pipe operator is |&gt;, a short cut for which is Ctrl+Shift+M.\nUsing the pipe, we can apply out transformation with:\n\ntnums &lt;- nums |&gt; \n  sqrt() |&gt; \n  log()\n\nThe command are in the order applied, there are no intermediates and the code is easier to debug and to build-up step-by-step..\nNote that |&gt; is the pipe that comes with base R which was only added in the last couple of years. Before it existed, **tidyverse** had a pipe operator provided by the *magrittr** package. The magrittr pipe is %&gt;%. In your googling, you may well see code written using the %&gt;%. In most cases, the pipes are interchangeable.\nWhat They Forgot to Teach You About R Bryan et al. (n.d.)\n\n\n\n\nBryan, Jennifer, Jim Hester, Shannon Pileggi, and E. David Aja. n.d. What They Forgot to Teach You About R. Accessed September 26, 2019.",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Workflow in RStudio</span>"
    ]
  },
  {
    "objectID": "import_to_report.html",
    "href": "import_to_report.html",
    "title": "\n8¬† From importing to reporting\n",
    "section": "",
    "text": "8.1 Importing data from files\nThere are two things you need to know before you can import data from a file.\nüé¨ Your turn! If you want to code along you will need to start a new RStudio project then a new script.\nThis chapter covers reading .txt files and .csv files using tidyverse (Wickham et al. 2019) functions and excel files using the readxl (Wickham and Bryan 2023) package. We will demonstrate what needs to be done differently if the file is not in your working directory.\nlibrary(tidyverse)\nlibrary(readxl)",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>From importing to reporting</span>"
    ]
  },
  {
    "objectID": "import_to_report.html#importing-data-from-files",
    "href": "import_to_report.html#importing-data-from-files",
    "title": "\n8¬† From importing to reporting\n",
    "section": "",
    "text": "What format the data are in\nThe format of the data determines what function you will use to import it. Often the file extension indicates format.\n\n\n.txt a plain text file1, where the columns are often separated by a space but might also be separated by a tab, a backslash or forward slash, or some other character\n\n.csv a plain text file where the columns are separated by commas\n\n.xlsx an Excel file More detail on file types was covered in Understanding file systems\n\n\nHowever, you should always check the file to make sure it is in the format you expect because there is little to force a match between a file‚Äôs contents and the extension in its name. You can check by opening the file in a text editor (e.g., Notepad on Windows, TextEdit on Mac) or in RStudio (see below).\n\n\nWhere the file is relative to your working directory\nR can only read in a file if you say where it is, i.e., you give its relative path. More detail on relative file paths and working directories was covered in Understanding file systems\n\n\n\n\n\n\n8.1.1 Importing data from .txt file\nThe data in adipocytes.txt give the concentration of a hormone called adiponectin in some cells. There are two columns: the first gives the adiponectin concentration and the second, treatment, indicates whether the cells were treated with nicotinic acid or not. Save this file to the project folder.\nA .txt extension suggests this is plain text file with columns separated by spaces. However, before we attempt to read it in, when should take a look at it. We can do this from RStudio by clicking on the file in the Files pane. Any plain text file will open in the top left pane.\n\n\n\n\n\nFigure¬†8.1: The adipocytes.txt data file open. We can see the columns are separated by spaces\n\n\nThe files are separated by spaces as we suspected. We use the read_table() command to read in plain text files of single columns, or where the columns are separated by spaces:\n\nadipo &lt;- read_table(\"adipocytes.txt\")\n\nThe data from the file has been read into a dataframe called adipo. You will and you will be able to see it in the Environment window. Clicking on it in the Environment window will open a spreadsheet-like view of the dataframe.\n\n8.1.2 Importing a from a.csv file\nThe data seal.csv give the myoglobin concentration of skeletal muscle for three species of seal. There are two columns: the first gives the myoglobin concentration and the second indicates species.\nThe .csv extension suggests this is plain text file with columns separated by commas. We will again check this before we attempt to read it in. Click on the file in the Files pane - a pop-up will appear for files ending .csv or .xlsx. Choose View File2.\n\n\nRstudio Files pane showing the data files and the View File option that appears when you click on the a particular file\n\nCSV files will open in the top left pane (Excel files will launch Excel). We can see that the file does contain comma separated values. There is aread_csv() function which works very like read_table()[^working_with_data_rstudio-3]:\n\nseal &lt;- read_csv(\"seals.csv\")\n\n\n8.1.3 Importing a from a.xlsx file\nThe data in blood.xlsx are measurements of several blood parameters from fifty people with Crohn‚Äôs disease, a lifelong condition where parts of the digestive system become inflamed. Twenty-five of people are in the early stages of diagnosis and 25 have started treatment.\n\nblood &lt;- read_excel(\"blood.xlsx\")\n\n\n8.1.4 Importing data from a file not in your working directory\nWhen using an RStudio project, your working directory is the project folder (the folder containing the .Rproj file. Suppose our file adipocytes.txt is in a folder, data-raw, in our working directory.\n-- myproject\n   |__myproject.Rproj\n   |__import.R\n   |__data-raw\n      |__adipocytes.txt\n      |__blood.xlsx\n      |__seal.csv\nWe need to adjust the code to give the relative path to the datafile:\n\nadipo &lt;- read_table(\"data-raw/adipocytes.txt\")\n\nThe relative path is the path from the working directory to the file. In this case, the working directory is myproject and the relative path is data-raw/adipocytes.txt.\nüé¨ Your turn! Create a folder called data-raw inside the project folder and move the data files to it. Now modify the data import code to import seal.csv from data-raw.\n\nCodeseal &lt;- read_csv(\"data-raw/seal.csv\")",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>From importing to reporting</span>"
    ]
  },
  {
    "objectID": "import_to_report.html#summarising-data",
    "href": "import_to_report.html#summarising-data",
    "title": "\n8¬† From importing to reporting\n",
    "section": "\n8.2 Summarising data",
    "text": "8.2 Summarising data\nWe summarise data using the the dplyr (Wickham et al. 2023) package, which provides a set of functions designed for efficient data manipulation. This is a tidyverse (Wickham et al. 2019) package which you already loaded. The approach replies on the data being in a tidy format, meaning each column represents a variable, each row represents an observation, and each cell contains a single value. The pipeline is:\n\nGroup the data: If you want to summarize your data based on certain groups, you can use the group_by()\nSummarise: Once your data grouped (if necessary), you use summarise() with functions likemean(), median(), sd(), min()and max() within it.\n\nWe will demonstrate summarising using the adipo dataframe. adiponectin is the response and is continuous and treatment is an explanatory with categorical with two levels (groups).\nThe most useful summary statistics for a continuous variable like adiponectin are the means, standard deviations, sample sizes and standard errors. We use the group_by() and summarise() functions along with the functions that do the calculations.\nTo create a data frame called adipo_summary that contains the means, standard deviations, sample sizes and standard errors for the control and nicotinic acid treated samples:\n\nadipo_summary &lt;- adipo  |&gt; \n  group_by(treatment) |&gt;\n  summarise(mean = mean(adiponectin),\n            std = sd(adiponectin),\n            n = length(adiponectin),\n            se = std/sqrt(n))\n\nYou can type:\n\nadipo_summary\n## # A tibble: 2 √ó 5\n##   treatment  mean   std     n    se\n##   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n## 1 control    5.55  1.48    15 0.381\n## 2 nicotinic  7.51  1.79    15 0.463\n\nor click on environment to open a spreadsheet-like view of the dataframe.\n\n8.2.1 Visualise data\nMost commonly, we put the explanatory variable on the x axis and the response variable on the y axis. A continuous response, particularly one that follows the normal distribution, is best summarised with the mean and the standard error. In my opinion, you should also show all the raw data points if possible.\nWe are going to create a figure like this:\n\n\n\n\n\n\n\n\n\n8.2.1.1 ggplot2\n\nggplot2 (Wickham 2016) is a powerful data visualisation package in R that is part of the tidyverse. It provides a flexible and layered approach to creating high-quality and customizable graphics.\nThe core concept of ggplot2 is to build a plot layer by layer. The basic structure consists of three main components:\n\nData: the data frame to be used for plotting.\nAesthetic mappings (aes): how variables in the data map to visual elements such as x and y positions, colours, shapes, etc.\nGeometric objects (geoms): the actual graphical elements used to visualize the data, such as points, lines, bars, etc.\n\nTo create a basic plot, you start with the ggplot() function and provide the data and aesthetic mappings.\nggplot(data = adipo, aes(x = treatment, y = adiponectin))\n\nYou can add geometric layers to the plot using specific functions such as geom_point(), geom_line(), geom_bar(), etc. These functions define the type of plot you want to create:\nggplot(data = adipo, aes(x = treatment, y = adiponectin)) +\n  geom_point()\n\nIn the figure we are aiming for, we are plotting two dataframes: the adipo dataframe which contains the data points themselves; and the adipo_summary dataframe containing and the means and standard errors.\nThe dataframes and aesthetics for ggplot can be specified within a geom_xxxx (rather than in the ggplot()). This is very useful if the geom only applies to some of the data you want to plot.\n\n\n\n\n\n\nTip: ggplot()\n\n\n\nYou put the data argument and aes() inside ggplot() if you want all the geoms to use that dataframe and variables. If you want a different dataframe for a geom, put the data argument and aes() inside the geom_xxxx()\n\n\nI will build the plot up in small steps you should edit your existing ggplot() command as we go.\nWe will plot the data points first. Notice that we have given the data argument and the aesthetics inside the geom_point(). The variables treatment and adiponectin are in the adipo dataframe\n\nggplot() +\n  geom_point(data = adipo, \n             aes(x = treatment, y = adiponectin))\n\n\n\n\n\n\n\nSo the data points don‚Äôt overlap, we can add some random jitter in the x direction (edit your existing code):\n\nggplot() +\n  geom_point(data = adipo, \n             aes(x = treatment, y = adiponectin),\n             position = position_jitter(width = 0.1, height = 0))\n\n\n\n\n\n\n\nNote that position = position_jitter(width = 0.1, height = 0) is inside the geom_point() parentheses, after the aes() and a comma.\nWe‚Äôve set the vertical jitter to 0 because, in contrast to the categorical x-axis, movement on the y-axis has meaning (the adiponectin levels).\nLet‚Äôs make the points a light grey (edit your existing code):\n\nggplot() +\n  geom_point(data = adipo, \n             aes(x = treatment, y = adiponectin),\n             position = position_jitter(width = 0.1, height = 0),\n             colour = \"grey50\")\n\n\n\n\n\n\n\nNow to add the errorbars. These go from one standard error below the mean to one standard error above the mean.\nAdd a geom_errorbar() for errorbars (edit your existing code):\n\nggplot() +\n  geom_point(data = adipo, aes(x = treatment, y = adiponectin),\n             position = position_jitter(width = 0.1, height = 0),\n             colour = \"grey50\") +\n  geom_errorbar(data = adipo_summary, \n                aes(x = treatment, ymin = mean - se, ymax = mean + se),\n                width = 0.3) \n\n\n\n\n\n\n\nWe have specified the adipo_summary dataframe and the variables treatment, mean and se are in that.\nThere are several ways you could add the mean. You could use geom_point() but I like to use geom_errorbar() again with the ymin and ymax both set to the mean.\nAdd a geom_errorbar() for the mean (edit your existing code):\n\nggplot() +\n  geom_point(data = adipo, aes(x = treatment, y = adiponectin),\n             position = position_jitter(width = 0.1, height = 0),\n             colour = \"grey50\") +\n  geom_errorbar(data = adipo_summary, \n                aes(x = treatment, ymin = mean - se, ymax = mean + se),\n                width = 0.3) +\n  geom_errorbar(data = adipo_summary, \n                aes(x = treatment, ymin = mean, ymax = mean),\n                width = 0.2)\n\n\n\n\n\n\n\nAlter the axis labels and limits using scale_y_continuous() and scale_x_discrete() (edit your existing code):\n\nggplot() +\n  geom_point(data = adipo, aes(x = treatment, y = adiponectin),\n             position = position_jitter(width = 0.1, height = 0),\n             colour = \"grey50\") +\n  geom_errorbar(data = adipo_summary, \n                aes(x = treatment, ymin = mean - se, ymax = mean + se),\n                width = 0.3) +\n  geom_errorbar(data = adipo_summary, \n                aes(x = treatment, ymin = mean, ymax = mean),\n                width = 0.2) +\n  scale_y_continuous(name = \"Adiponectin (pg/mL)\", \n                     limits = c(0, 12), \n                     expand = c(0, 0)) +\n  scale_x_discrete(name = \"Treatment\", \n                   labels = c(\"Control\", \"Nicotinic acid\"))\n\n\n\n\n\n\n\nYou only need to use scale_y_continuous() and scale_x_discrete() to use labels that are different from those in the dataset. Often this is to use proper terminology and captialisation.\nFormat the figure in a way that is more suitable for including in a report using theme_classic() (edit your existing code):\n\nggplot() +\n  geom_point(data = adipo, aes(x = treatment, y = adiponectin),\n             position = position_jitter(width = 0.1, height = 0),\n             colour = \"gray50\") +\n  geom_errorbar(data = adipo_summary, \n                aes(x = treatment, ymin = mean - se, ymax = mean + se),\n                width = 0.3) +\n  geom_errorbar(data = adipo_summary, \n                aes(x = treatment, ymin = mean, ymax = mean),\n                width = 0.2) +\n  scale_y_continuous(name = \"Adiponectin (pg/mL)\", \n                     limits = c(0, 12), \n                     expand = c(0, 0)) +\n  scale_x_discrete(name = \"Treatment\", \n                   labels = c(\"Control\", \"Nicotinic acid\")) +\n  theme_classic()\n\n\n\n\n\n\n\nThe ggsave() function is used to save a ggplot object as an image file. It provides a convenient way to export your plots to various file formats, such as PNG, PDF, SVG, or JPEG.\nThe basic syntax of ggsave() is as follows:\nggsave(filename,\n       plot,\n       device,\n       width,\n       height,\n       units,\n       dpi)\nYou must give a file name for the output file but all the other options have defaults.\n\nplot: The ggplot object you want to save. Defaults to the last created plot.\ndevice: one of ‚Äúpng‚Äù, ‚Äúeps‚Äù, ‚Äúps‚Äù, ‚Äútex‚Äù, ‚Äúpdf‚Äù, ‚Äújpeg‚Äù, ‚Äútiff‚Äù, ‚Äúpng‚Äù, ‚Äúbmp‚Äù, ‚Äúsvg‚Äù or ‚Äúwmf‚Äù (windows only). Defaults to the format given by the file extension in filename\nwidth, height units: Plot size in units (‚Äúin‚Äù, ‚Äúcm‚Äù, ‚Äúmm‚Äù, or ‚Äúpx‚Äù). Defaults to the size of the plot in the Plots window.\ndpi: Plot resolution.\n\nWe can save the figure we just created as a 3 inch x 3 inch png file as follows:\n\nggsave(\"adipocytes.png\",\n       device = \"png\",\n       width = 3,\n       height = 3,\n       units = \"in\",\n       dpi = 300)\n\nIt is often a good idea to explicitly assign the ggplot object to a variable and use that in the ggsave()\n\nfig1 &lt;- ggplot() +\n  geom_point(data = adipo, aes(x = treatment, y = adiponectin),\n             position = position_jitter(width = 0.1, height = 0),\n             colour = \"gray50\") +\n  geom_errorbar(data = adipo_summary, \n                aes(x = treatment, ymin = mean - se, ymax = mean + se),\n                width = 0.3) +\n  geom_errorbar(data = adipo_summary, \n                aes(x = treatment, ymin = mean, ymax = mean),\n                width = 0.2) +\n  scale_y_continuous(name = \"Adiponectin (pg/mL)\", \n                     limits = c(0, 12), \n                     expand = c(0, 0)) +\n  scale_x_discrete(name = \"Treatment\", \n                   labels = c(\"Control\", \"Nicotinic acid\")) +\n  theme_classic()\n\nggsave(\"adipocytes.png\",\n       plot = fig1,\n       device = \"png\",\n       width = 3,\n       height = 3,\n       units = \"in\",\n       dpi = 300)",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>From importing to reporting</span>"
    ]
  },
  {
    "objectID": "import_to_report.html#tidy-data",
    "href": "import_to_report.html#tidy-data",
    "title": "\n8¬† From importing to reporting\n",
    "section": "\n8.3 Tidy data",
    "text": "8.3 Tidy data\nData that is in a format that is easy to work with is often referred to as ‚Äútidy data‚Äù. Tidy data is a concept that was introduced by Hadley Wickham in his paper Tidy Data (Wickham 2014). The paper is available online at http://vita.had.co.nz/papers/tidy-data.pdf.\nTidy data:\n\nEach variable should be in one column.\nEach different observation of that variable should be in a different row.\nThere should be one table for each ‚Äúkind‚Äù of data.\nIf you have multiple tables, they should include a column in the table that allows them to be linked.\n\nThese concepts have been around for a long time and underlie formats enforced in several statistical packages such as SPSS, minitab and SAS.\nRecognising the structure of your data and organising it in tidy format is one of the most important step in data analysis.\n\n\n\n\nWickham, Hadley. 2014. ‚ÄúTidy Data.‚Äù Journal of Statistical Software, Articles 59 (10): 1‚Äì23. https://vita.had.co.nz/papers/tidy-data.pdf.\n\n\n‚Äî‚Äî‚Äî. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D‚ÄôAgostino McGowan, Romain Fran√ßois, Garrett Grolemund, et al. 2019. ‚ÄúWelcome to the Tidyverse‚Äù 4: 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, and Jennifer Bryan. 2023. ‚ÄúReadxl: Read Excel Files.‚Äù https://CRAN.R-project.org/package=readxl.\n\n\nWickham, Hadley, Romain Fran√ßois, Lionel Henry, Kirill M√ºller, and Davis Vaughan. 2023. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>From importing to reporting</span>"
    ]
  },
  {
    "objectID": "import_to_report.html#footnotes",
    "href": "import_to_report.html#footnotes",
    "title": "\n8¬† From importing to reporting\n",
    "section": "",
    "text": "Plain text files can be opened in notepad or other similar editor and still be readable.‚Ü©Ô∏é\nThere is also and option to import the dataset. Do not¬†be tempted to import data this way! Unless you are careful and know what you are doing, your data import will not be scripted or will not be scripted correctly.‚Ü©Ô∏é",
    "crumbs": [
      "Getting started with data",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>From importing to reporting</span>"
    ]
  },
  {
    "objectID": "statistical_analysis_1.html",
    "href": "statistical_analysis_1.html",
    "title": "Statistical Analysis - Part 1",
    "section": "",
    "text": "What this part is about\nThis section is a first course in Statistical inference which is the process of inferring the characteristics of populations from samples using data analysis. In this first course we take what is called a frequentist - or classical - approach to statistical inference. This is the approach that is most commonly taught in introductory statistics courses. The first chapter in the section explains the logic of hypothesis testing in making statistical inferences. We then cover confidence intervals and explain what is a statistical model, and specifically, what is a linear model.\nThis remaining chapters cover regression, t-tests and ANOVA which are special cases of a much more widely applicable statistical model known as the ‚Äúgeneral linear model.‚Äù It is common for t-tests and ANOVA to be taught using the t.test() and aov() functions in R respectively. Here we teach them using the lm() function. This is to emphasise that regression, t-tests and ANOVA are fundamentally the same model known as the General Linear Model. This approach allows you to became familiar with the language and terminology of statistics and to more easily build on your knowledge. The output of lm() is typical of statistical modelling functions in R general and you will find it easier to build on your understanding if you are familiar with the output of lm()",
    "crumbs": [
      "Statistical Analysis - Part 1"
    ]
  },
  {
    "objectID": "logic_hyopthesis_testing.html",
    "href": "logic_hyopthesis_testing.html",
    "title": "\n9¬† The logic of hyothesis testing\n",
    "section": "",
    "text": "9.1 What is Hypothesis testing?\nHypothesis testing is a statistical technique which allows us to make inferences about the characteristics of a populations based on a sample. We almost always have to use samples because we are very rarely able to measure every observation in a population. For example, if we are interested in knowing if maternal poverty influences birth weight we would not be able to measure the birth weight of every baby in our population. Instead we would take a sample of babies born to mothers in poverty and determine if their weight, on average, was different to the national average. A statistical procedure is needed because even if maternal poverty has no effect, our sample average will differ from the national average just by chance.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>The logic of hyothesis testing</span>"
    ]
  },
  {
    "objectID": "logic_hyopthesis_testing.html#samples-and-populations",
    "href": "logic_hyopthesis_testing.html#samples-and-populations",
    "title": "\n9¬† The logic of hyothesis testing\n",
    "section": "\n9.2 Samples and populations",
    "text": "9.2 Samples and populations",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>The logic of hyothesis testing</span>"
    ]
  },
  {
    "objectID": "logic_hyopthesis_testing.html#logic-of-hypothesis-testing",
    "href": "logic_hyopthesis_testing.html#logic-of-hypothesis-testing",
    "title": "\n9¬† The logic of hyothesis testing\n",
    "section": "\n9.3 Logic of hypothesis testing",
    "text": "9.3 Logic of hypothesis testing\nThe logic behind hypothesis testing follows these general steps:\n\nFormulating a ‚ÄúNull Hypothesis‚Äù denoted \\(H_0\\). The null hypothesis is what we expect to happen if nothing interesting is happening. It states that there is no difference between groups or no relationship between variables. In contrast, the ‚ÄúAlternative Hypothesis‚Äù (\\(H_1\\)) states that there is a significant difference between groups or a relationship between variables.\nDesigning an experiment that generates data to test the null hypothesis.\nFinding the probability (the p-value) of getting our experimental data, or data more extreme, if \\(H_0\\) is true.\nDeciding whether to reject or not reject the \\(H_0\\) based on that probability:\n\nIf p ‚â§ 0.05 we reject \\(H_0\\)\n\nIf p &gt; 0.05 do not reject \\(H_0\\)\n\n\n\n\nIf the null hypothesis is rejected it means we have evidence that \\(H_0\\) is untrue and support for \\(H_1\\). If the null hypothesis is not rejected, it means there is insufficient evidence to support the alternative hypothesis. Note that it does not mean that \\(H_0\\) is true, just that it cannot be discounted. Since there is a real state to \\(H_0\\), that is, \\(H_0\\) is either true or not true, and we make a decision to reject or not reject it, we can be wrong.\n\n9.3.1 Type I and type II errors\nType I and type II errors describe the cases when we make the wrong decision about the null hypothesis. These errors are inherent in the approach rather than mistakes you can prevent.\n\nA type I error occurs when we reject a null hypothesis that is true. This can be thought of as a false positive. It is a real error in that we have a real difference or effect. Since we use a probability of 0.05 to reject the null hypothesis, we will make a type I error 5% of the time.\nA type II error occurs when we do not reject a null hypothesis that is false. This is a false negative. It is not a real error in the sense that we only conclude we do not have enough evidence to reject the null hypothesis.\nIf we reject a null hypothesis that is false we have not made an error.\nIf we do not reject a null hypothesis that is true we have not made an error.\n\n\n\nType I and Type II errors.\n\nWe can decrease our chance of making a type I error by reducing the the p-value required to reject the null hypothesis. However, this will increase our chance of making a type II error. We can decrease our chance of making a type II error by collecting enough data. The amount of data needed will depend on the the size of the effect relative to the random variation in the data.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>The logic of hyothesis testing</span>"
    ]
  },
  {
    "objectID": "logic_hyopthesis_testing.html#sampling-distribution-of-the-mean",
    "href": "logic_hyopthesis_testing.html#sampling-distribution-of-the-mean",
    "title": "\n9¬† The logic of hyothesis testing\n",
    "section": "\n9.4 Sampling distribution of the mean",
    "text": "9.4 Sampling distribution of the mean\nThe sampling distribution of the mean is a fundamental concept in hypothesis testing and constructing confidence intervals. Parametric tests such as regression, two-sample tests and ANOVA (all applied with lm()) are based on the sampling distribution of the mean. It is a theoretical distribution that describes the distribution of the sample means if an infinite number of samples were taken.\nThe key characteristics of the sampling distribution of the mean are:\n\nThe mean of the sampling distribution of the mean is equal to the population mean\nThe standard deviation of the sampling distribution of the mean is known the standard error of the mean and is always smaller than the standard deviation of the values. There is a fixed relationship between the standard deviation of a sample or population and the standard error of the mean: \\(s.e. = \\frac{s.d.}{\\sqrt{n}}\\)\n\nWhen we are determining the probability of getting our experimental data, or data more extreme, if \\(H_0\\) is true, it is the sampling distribution of the mean that matters. We ask what is the probability of getting a sample mean like this.\n\n\n\n\n\n\n\nFigure¬†9.1: The sampling distribution of the means is a theoretical distribution that describes the distribution of the sample means if an infinite number of samples of size n were taken. The sampling distribution of the mean has a standard devation which is smaller than the population and is called the standard error.\n\n\n\n\n\n9.4.1 Example\nLet‚Äôs work through this logic using an example.\nQuestion: National average birth weight is 3300 grams with an s.d. = 900 grams. Does maternal poverty influence birth weight?\n\nSet up the null hypothesis There is no effect of maternal poverty on birth weight so the mean of a sample of babies born into poverty is equal to the national average (Figure¬†9.2). This is written as \\(H_0: \\bar{x} = 3300\\). The alternative hypothesis is that the sample mean is not equal to the national average. This is written as \\(H_1: \\bar{x} \\neq 3300\\).\n\n\n\n\n\n\n\n\nFigure¬†9.2: Distribution of the population of birth weights has a mean of 3300 g and a standard deviation of 900. The null hypothesis is that the mean birth weight of babies born to women in poverty is the same as the national average of 3300 g.\n\n\n\n\n\nWe take a sample of \\(n = 12\\) women who live in poverty and determine the mean birth weight of their babies. We calculate \\(\\bar{x} = 3000 g\\). This is lower than the national average but might we get a sample like that even if the null hypothesis is true?\nDetermine the probability (the p-value) of getting our experimental data, or more extreme data, if \\(H_0\\) is true. (Figure¬†9.3)\n\n\n\n\n\n\n\n\nFigure¬†9.3: The probability of getting a sample mean of 3000 or less is the area under the sampling distribution of the mean to the left of 3000.\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†9.4: The probability of getting a sample mean of 3000 or more extreme is the area under the sampling distribution of the mean to the left of 3000 plus that to the right of 3600. This is because 3600 is as extreme (as far away from the mean) as 3000.\n\n\n\n\n\nDeciding whether to reject or not reject the \\(H_0\\) based on that probability. If the shaded area is less than 0.05 we reject the null hypothesis and conclude maternal poverty does influence birth weight. If the shaded area is more than 0.05 we do not reject the null hypothesis.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>The logic of hyothesis testing</span>"
    ]
  },
  {
    "objectID": "confidence_intervals.html",
    "href": "confidence_intervals.html",
    "title": "\n10¬† Confidence Intervals\n",
    "section": "",
    "text": "10.1 What is a confidence interval?\nWhen we calculate a mean from a sample, we are using it to estimate the mean of the population. Confidence intervals are a range of values and are a way to quantify the uncertainty in our estimate. When we report a mean with its 95% confidence interval we give the mean plus and minus some variation. We are saying that 95% of the time, that range will contain the population mean.\nThe confidence interval is calculated from the sample mean and the standard error of the mean. The standard error of the mean is the standard deviation of the sampling distribution of the mean.\nTo understand confidence intervals we need to understand some properties of the normal distribution.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "confidence_intervals.html#the-normal-distribution",
    "href": "confidence_intervals.html#the-normal-distribution",
    "title": "\n10¬† Confidence Intervals\n",
    "section": "\n10.2 The normal distribution",
    "text": "10.2 The normal distribution\nA distribution describes the values the variable can take and the chance of them occurring. A distribution has a general type, given by the function, and is further tuned by the parameters in the function. For the normal distribution these parameters are the mean and the standard deviation. Every variable that follows the normal distribution has the same bell shaped curve and the distributions differ only in their means and/or standard deviations. The mean determines where the centre of the distribution is, the standard deviation determines the spread (Figure¬†10.1).\n\n\n\n\n\n\n\nFigure¬†10.1: The mean determines where the centre of the distribution is, the standard deviation determines the spread. The distributions on the left have the same mean but different standard deviations. The distributions on the right have the same standard deviation but different means.\n\n\n\n\nWhilst normal distributions vary in the location on the horizontal axis and their width, they all share some properties and it is these shared properties that allow the calculation of confidence intervals with some standard formulae. The properties are that a fix percentage of values lie between a given number of standard deviations. For example, 68.2% values lie between plus and minus one standard deviation from the mean and 95% values lie between +/-1.96 standard deviations. Another way of saying this is that there is a 95% chance that a randomly selected value will lie between +/-1.96 standard deviations from the mean. This is illustrated in Figure¬†10.2.\n\n\n\n\n\n\n\nFigure¬†10.2: Normal distributions share some properties regardless of the mean and standard deviation. 68% of the values are within 1 standard deviation of the mean and 95% are within 1.96 standard deviations.\n\n\n\n\nR has some useful functions associated with distributions, including the normal distribution.\n\n10.2.1 Distributions: the R functions\nFor any distribution, R has four functions:\n\nthe density function, which gives the height of the function at a given value.\nthe distribution function, which gives the probability that a variable takes a particular value or less.\nthe quantile function which is the inverse of the Distribution function, i.e., it returns the value (‚Äòquantile‚Äô) for a given probability.\nthe random number generating function\n\nThe functions are named with a letter d, p, q or r preceding the distribution name. Table¬†10.1 shows these four functions for the normal, binomial, Poisson and t distributions.\n\n\nTable¬†10.1: R functions that provide values for some example distributions\n\n\n\n\n\n\n\n\n\n\nDistribution\nDensity\nDistribution\nQuantile\nRandom number generating\n\n\n\nNormal\ndnorm()\npnorm()\nqnorm()\nrnorm()\n\n\nBinomial\ndbinom()\npbinom()\nqbinom()\nrbinom()\n\n\nPoisson\ndpois()\nppois()\nqpois()\nrpois()\n\n\nt\ndt()\npt()\nqt()\nrt()\n\n\n\n\n\n\nSearching for the manual with ?normal or any one of the functions (?pnorm) will bring up a single help page for all four associated functions.\n\n\n\n\n\n\nFigure¬†10.3: The R Manual Page for the normal distribution which shows the four functions associated functions.\n\n\nThe functions which are of most use to us are pnorm() and qnorm() and these are illustrated in Figure¬†10.4.\n\n\n\n\n\n\n\nFigure¬†10.4: The pnorm() function calculates the probability that a value is less than or equal to a given value.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "confidence_intervals.html#confidence-intervals-on-large-samples",
    "href": "confidence_intervals.html#confidence-intervals-on-large-samples",
    "title": "\n10¬† Confidence Intervals\n",
    "section": "\n10.3 Confidence intervals on large samples",
    "text": "10.3 Confidence intervals on large samples\n\\[\n\\bar{x} \\pm 1.96 \\times s.e.\n\\tag{10.1}\\]\n95% of confidence intervals calculated in this way will contain the true population mean.\nDo you have to remember the value of 1.96? Not if you have R!\n\nqnorm(0.975)\n## [1] 1.959964\n\nNotice that it is qnorm(0.975) and not qnorm(0.95) for a 95% confidence interval. This is because the functions are defined as giving the area under to the curve to the left of the value given. If we gave 0.95, we would get the value that put 0.05 in one tail. We want 0.025 in each tail, so we need to use 0.975 in qnorm().\nTO-DO pic\n\n10.3.1 Example in R\nTO-DO",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "confidence_intervals.html#confidence-intervals-on-small-samples",
    "href": "confidence_intervals.html#confidence-intervals-on-small-samples",
    "title": "\n10¬† Confidence Intervals\n",
    "section": "\n10.4 Confidence intervals on small samples",
    "text": "10.4 Confidence intervals on small samples\nThe calculation of confidence intervals on small samples is very similar but we use the t-distribution rather than the normal distribution. The formula is:\n\\[\n\\bar{x} \\pm t_{[d.f.]} \\times s.e.\n\\tag{10.2}\\]\nThe t-distibution is a modified version of the normal distribution and we use it because the sampling distribution of the mean is not quite normal when the sample size is small. The t-distribution has an additional parameter called the degrees of freedom which is the sample size minus one (\\(n -1\\)). Like the normal distribution, the t-distribution has a mean of zero and is symmetrical. However, The t-distribution has fatter tails than the normal distribution and this means that the probability of getting a value in the tails is higher than for the normal distribution. The degrees of freedom determine how much fatter the tails are. The smaller the sample size, the fatter the tails. As the sample size increases, the t-distribution becomes more and more like the normal distribution.\n\n10.4.1 Example in R\nTO-DO",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Confidence Intervals</span>"
    ]
  },
  {
    "objectID": "what_statistical_model.html",
    "href": "what_statistical_model.html",
    "title": "\n11¬† What is a statistical model?\n",
    "section": "",
    "text": "11.1 Overview\nThis section discusses statistical models which are equations representing relationships between variables. Statistical models help us test hypotheses and make predictions. The process involves estimating model ‚Äúparameters‚Äù from data and assessing ‚Äúmodel fit‚Äù. Linear models include regression, t-tests, and ANOVA, known collectively as the General Linear Model. The assumptions of the general linear model are that the ‚Äúresiduals‚Äù are normally distribution and variance is homogeneous. If the assumptions are violated we can use non-parametric tests.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>What is a statistical model?</span>"
    ]
  },
  {
    "objectID": "what_statistical_model.html#what-is-a-statistical-model",
    "href": "what_statistical_model.html#what-is-a-statistical-model",
    "title": "\n11¬† What is a statistical model?\n",
    "section": "\n11.2 What is a statistical model?",
    "text": "11.2 What is a statistical model?\nA statistical model is a mathematical equation that helps us understand the relationships between variables. We evaluate how well our data fit a particular model so we can infer something about how the values arose or make predictions about future values.\nThe equation states what has to be done to the explanatory values to get the response value. For example, a simple model of plant growth might be that a plant grows by 2 cm a day week after it is two weeks old. This model would be written as:\n\\[\nh_{(t)} = h_{(0) }+ 2t\n\\tag{11.1}\\]\nWhere:\n\n\n\\(t\\) is the time in days after the plant is two weeks old\n\n\\(h_{(t)}\\) is the height of the plant at time \\(t\\)\n\n\n\\(h_{(0)}\\) is the height of the plant at \\(t=0\\), i.e., at two weeks\n\nThis model is a linear model because the relationship between the response variable, height, and the explanatory variable, time, is linear (See Figure¬†11.1 (a)). In a linear model, the gradient of the line is the same no matter what the value of \\(t\\). In this case, it is fixed at 2 cm per day.\nOne alternative is a simple exponential model. In an exponential model, the height might increase by 12% each day and the gradient of the line would increase over time. This model is written as:\n\\[\nh_{(t)} = h_{(0)}1.2^t\n\\tag{11.2}\\]\nWhere:\n\n\n\\(t\\) is the time in days after the plant is two weeks old\n\n\\(h_{(t)}\\) is the height of the plant at time \\(t\\)\n\n\n\\(h_{(0)}\\) is the height of the plant at \\(t=0\\), i.e., at two weeks\n\nThis model is not a straight line (See Figure¬†11.1 (b)). The gradient of the line increase as time goes on.\n\n\n\n\n\n\n\n\n\n(a) Linear\n\n\n\n\n\n\n\n\n\n(b) Expontential\n\n\n\n\n\n\nFigure¬†11.1: Two possible models of plant growth.\n\n\nWhen we do statistics we using a statistical model. This means we are making about the relationship between the explanatory and response variables. For the model we choose, we estimate the parameters of the model from the data. For the linear model of plant growth the parameters are the intercept and gradient of the line.\nStatistical testing determines whether the parameters differ from zero and the fit of the data to the model. Determining whether a parameter differs from zero relies on calculating the probability of getting the estimate we calculate if its true value is zero.\nThis means as well as making assumptions about the type of relationship we also make assumptions about the distribution of the data. The assumption we make is that parameter is drawn from a normal distribution. This is a reasonable assumption, because many things are. However, if this assumption is not met, then our probability will not be accurate.\nFor this reason, we check the assumptions of our model and test before drawing conclusions. If the assumptions are not met we take some action to transform the data or use a different model with fewer assumptions. Non-parametric tests make fewer assumptions about the distribution of the data. They are called non-parametric because they do not estimate parameters like an intercept and gradient.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>What is a statistical model?</span>"
    ]
  },
  {
    "objectID": "what_statistical_model.html#using-a-linear-model-in-practice.",
    "href": "what_statistical_model.html#using-a-linear-model-in-practice.",
    "title": "\n11¬† What is a statistical model?\n",
    "section": "\n11.3 Using a linear model in practice.",
    "text": "11.3 Using a linear model in practice.\nImagine we are studying a population of bacteria and want understand how nutrient availability influences its growth. We could grow the bacteria with different levels of nutrients and measure the diameter of bacterial colonies on agar plates in a controlled environment so that everything except the nutrient availability was identical. We could then plot the diameters against the nutrient levels.\nWe might expect the relationship between nutrient level and growth to be linear and add a line of best fit. See Figure¬†11.2.\n\n\n\n\n\n\n\n\n\n(a) Data without a model.\n\n\n\n\n\n\n\n\n\n(b) Data with a line of best fit\n\n\n\n\n\n\nFigure¬†11.2: The effect of nutrient level on bacterial colony diamters without (1) and with (2) a linear model.\n\n\nThe equation of this line is a statistical model that allows us to make predictions about colony diameter from nutrient levels. A line - or linear model - has the form:\n\\[\ny = \\beta_{0} + \\beta_{1}x\n\\tag{11.3}\\]\nWhere:\n\n\n\\(y\\) is the response variable and \\(x\\) is the explanatory variable.\n\n\\(\\beta_{0}\\) is the value of \\(y\\) when \\(x = 0\\) usually known as the intercept\n\n\\(\\beta_{1}\\) is the amount added to \\(y\\) for each unit increase in \\(x\\) usually known as the slope\n\n\\(\\beta_{0}\\) and \\(\\beta_{1}\\) are called the coefficients - or parameters - of the model.\nIn this case \\[\nDiameter = \\beta_{0} + \\beta_{1}Nutrient\n\\tag{11.4}\\]\nLinear models are amongst the most commonly used statistics. Regression, t-tests and ANOVA are all linear models collectively known as the General Linear Model.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>What is a statistical model?</span>"
    ]
  },
  {
    "objectID": "what_statistical_model.html#model-fitting",
    "href": "what_statistical_model.html#model-fitting",
    "title": "\n11¬† What is a statistical model?\n",
    "section": "\n11.4 Model fitting",
    "text": "11.4 Model fitting\nThe process of estimating the parameters \\(\\beta_{0}\\) and \\(\\beta_{1}\\) from data is known as fitting a linear model. The line gives the predicted values of \\(y\\). The actual measured value of \\(y\\) will differ from the predicted value and this difference is called a residual or an error. The line is a best fit in the sense that \\(\\beta_{0}\\) and \\(\\beta_{1}\\) minimise the sum of the squared residuals, \\(SSE\\).\n\\[\nSSE = \\sum(y_{i}-\\hat{y})^2\n\\tag{11.5}\\]\nWhere:\n\n\n\\(y_{i}\\) represents each of the measured \\(y\\) values from the 1st to the ith\n\n\\(\\hat{y}\\) is predicted value\n\nSince \\(\\beta_{0}\\) and \\(\\beta_{1}\\) are those that minimise the \\(SSE\\), they are described as least squares estimates. You do not need to worry about this too much but it is a useful piece of statistical jargon to have heard of because it pops up often. The mean of a sample is also a least squares estimate - the sum of the squared differences between each value and the mean is smaller than the sum of the squared differences between each value and any other value.\nThe role played by \\(SSE\\) in estimating our parameters means that it is also used in determining how well our model fits our data. Our model can be considered useful if the difference between the actual measured value of \\(y\\) and the predicted value is small but \\(SSE\\) will also depend on the size of \\(y\\) and the sample size. This means we express \\(SSE\\) as a proportion of the total variation in \\(y\\). The total variation in \\(y\\) is denoted \\(SST\\):\n\\[\n\\frac{SSE}{SST}\n\\tag{11.6}\\]\n\\(\\frac{SSE}{SST}\\) is called the residual variation. It is the proportion of variance remaining after the model fitting. In contrast, the proportion of the total variance that is explained by the model is called R-squared, \\(R^2\\). It is:\n\\[\nR^2=1-\\frac{SSE}{SST}\n\\tag{11.7}\\]\nIf there were no explanatory variables, the value we would predict for the response variable is its mean. In other words, if you did not know the nutrient level for a randomly chosen bacterial colony the best guess you could make for its eventual diameter is the mean diameter. Thus, a good model should fit the response better than the mean - that is, a good model should fit the response better than a best guess. The output of lm() includes the \\(R^2\\). It represents the proportional improvement in the predictions from the regression model relative to the mean model. It ranges from zero, the model is no better than the mean, to 1, the predictions are perfect. See Figure¬†11.3\n\n\n\n\n\nFigure¬†11.3: A linear model with different fits. A) the model is a poor fit - the explanatory variable is no better than the response mean for predicting the response. B) the model is good fit - the explanatory variable explains a high proportion of the variance in the response. C) the model is a perfect fit - the response can be predicted perfectly from the explanatory variable. Measured response values are in pink, the predictions are in green and the dashed blue line gives the mean of the response.\n\n\nSince the distribution of the responses for a given \\(x\\) is assumed to be normal and the variances of those distributions are assumed to be homogeneous, both are also true of the residuals. It is our examination of the residuals which allows us to evaluate whether the assumptions are met.\nSee Figure¬†11.4 for a graphical representation of linear modelling terms introduced so far.\n\n\n\n\n\nFigure¬†11.4: A linear model annotated with the terms used in modelling. Measured response values are in pink, the predictions are in green, and the differences between these, known as the residuals, are in blue. The estimated model parameters, \\(\\beta_{0}\\) (the intercept) and \\(\\beta_{1}\\) (the slope) are indicated.\n\n\n\n11.4.1 General linear model assumptions\nThe assumptions of the general linear model are that the residuals are normally distributed and have homogeneity of variance. A residual is the difference between the predicted and observed value\nIf we have a continuous response and a categorical explanatory variable with two groups, we usually apply the general linear model with lm() and then check the assumptions, however, we can sometimes tell when a non-parametric test would be more appropriate before that:\n\nUse common sense - the response should be continuous (or nearly continuous, see Ideas about data: Theory and practice). Consider whether you would expect the response to be continuous\nThere should decimal places and few repeated values.\n\nTo examine the assumptions after fitting the linear model, we plot the residuals and test them against the normal distribution",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>What is a statistical model?</span>"
    ]
  },
  {
    "objectID": "what_statistical_model.html#choice-of-model",
    "href": "what_statistical_model.html#choice-of-model",
    "title": "\n11¬† What is a statistical model?\n",
    "section": "\n11.5 Choice of model",
    "text": "11.5 Choice of model\nbefore: appropriate to the question, type of relationship. assumptions about the type of model\nand after assumption calculations the probability calculations\nimplications of wrong choices: doesn‚Äôt anwser the question p value is inaccurate conclusions are wrong",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>What is a statistical model?</span>"
    ]
  },
  {
    "objectID": "what_statistical_model.html#general-linear-models-in-r",
    "href": "what_statistical_model.html#general-linear-models-in-r",
    "title": "\n11¬† What is a statistical model?\n",
    "section": "\n11.6 General linear models in R",
    "text": "11.6 General linear models in R\nWe use the lm() function in R to analyse data with the general linear model. When you have one explanatory variable the command is:\n lm(data = dataframe, response ~ explanatory) \nThe response ~ explanatory part is known as the model formula. These must be the names of two column in the dataframe.\nWhen you have two explanatory variable we add the second explanatory variable to the formula using a + or a *. The command is:\n lm(data = dataframe, response ~ explanatory1 + explanatory2) \nor\n lm(data = dataframe, response ~ explanatory1 * explanatory2) \nA model with explanatory1 + explanatory2 considers the effects of the two variables independently. A model with explanatory1 * explanatory2 considers the effects of the two variables and any interaction between them. You will learn more about independent effects and interactions in Two-way ANOVA\nWe usually assign the output of an lm() command to an object and view it with summary(). The typical workflow would be:\n mod &lt;- lm(data = dataframe, response ~ explanatory)\nsummary(mod) \nThere are two sorts of statistical tests in the output of summary(mod):\n\ntests of whether each coefficient is significantly different from zero and,\nan F-test of the model fit overall\n\nThe F-test in the last line of the output indicates whether the relationship modelled between the response and the set of explanatory variables is statistically significant. i.e., whether it explains a significant amount of variation.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>What is a statistical model?</span>"
    ]
  },
  {
    "objectID": "what_statistical_model.html#checking-assumptions",
    "href": "what_statistical_model.html#checking-assumptions",
    "title": "\n11¬† What is a statistical model?\n",
    "section": "\n11.7 Checking assumptions",
    "text": "11.7 Checking assumptions\nThe assumptions relate to the type of relationship chosen and the hypothesis testing about the parameters. For a general linear model we assume the relationship between diameter and nutrients is linear and we examine this by plotting our data before running any tests.\nThe assumptions of the hypothesis testing in a general linear model are that residuals are normally distributed and have homogeneity of variance. A residual is the difference between the predicted and observed value. We usually check these assumptions after fitting the linear model by using the plot() function. This produces diagnostic plots to explore the distribution of the residuals. These cannot prove the assumptions are met but allow us to quickly determine if the assumptions are plausible, and if not, how the assumptions are violated and what data points contribute to the violation.\nThe two diagnostic plots which are most useful are the ‚ÄúQ-Q‚Äù plot (plot 2) and the ‚ÄúResiduals vs Fitted‚Äù plot (plot 1). These are given as values to the which argument of plot().\n\n11.7.1 The Q-Q plot\nThe Q-Q plot is a scatterplot of the residuals (standardised to a mean of zero and a standard deviation of 1) against what is expected if the residuals are normally distributed.\n\nplot(mod, which = 2)\n\n\n\n\n\n\n\nThe points should fall roughly on the line if the residuals are normally distributed. In the example above, the residuals appear normally distributed.\nThe following are two examples in which the residuals are not normally distributed.\n\n\n\n\n\n\n\n\nIf you see patterns like these you should find an alternative to a general linear model such as a non-parametric test or a generalised linear model. Sometimes, applying a transformation to the response variable will result in better meeting the assumptions.\n\n11.7.2 The Residuals vs Fitted plot\n\n\n\n\n\n\n\n\nThe Residuals vs Fitted plot shows if residuals have homogeneous variance or non-linear patterns. Non-linear relationships between explanatory variables and the response will usually show in this plot if the model does not capture the non-linear relationship. For the assumptions to be met, the residuals should be equally spread around a horizontal line as they are here:\n\nplot(mod, which = 1)\n\n\n\n\n\n\n\nThe following are two examples in which the residuals do not have homogeneous variance and display non-linear patterns.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>What is a statistical model?</span>"
    ]
  },
  {
    "objectID": "what_statistical_model.html#reporting",
    "href": "what_statistical_model.html#reporting",
    "title": "\n11¬† What is a statistical model?\n",
    "section": "\n11.8 Reporting",
    "text": "11.8 Reporting\nWhen reporting the results of statistical tests we need to make sure we tell the reader everything they need to know and give the evidence it to support it. What they need to know is given in statements describing what difference or effect is significant and the evidence is from the test statistic and p-value from the test. You can think the statistical test values as being the evidence for the statements in your results sections, just as citations are the evidence for the statements in your introduction.\nIn reporting the result of a test we give:\n\nthe significance of effect\nthe direction of effect\nthe magnitude of effect\n\nFigures should demonstrate the statement. Ideally they will include all the data and the ‚Äòmodel‚Äô, i.e., the means and error bars or the fitted line. Figure legends should be concise but contain all the information needed to understand the figure. I like this blog on How to craft a figure legend for scientific papers",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>What is a statistical model?</span>"
    ]
  },
  {
    "objectID": "what_statistical_model.html#summary",
    "href": "what_statistical_model.html#summary",
    "title": "\n11¬† What is a statistical model?\n",
    "section": "\n11.9 Summary",
    "text": "11.9 Summary\n\nA statistical model is an equation that describes the relationship between a response variable and one or more explanatory variables.\nA statistical model allows you to make predictions about the response variable based on the values of the explanatory variables.\nMany statistical tests are types of ‚ÄúGeneral Linear Model‚Äù including linear regression, t-tests and ANOVA.\nStatistical testing means estimating the model ‚Äúparameters‚Äù and testing whether they are significantly different from zero. The parameters, also known as coefficients, are the intercept and slope (s) in a General Linear Model. A p-value less than 0.05 for the slope means there is a significant relationship between the response and the explanatory variable.\nWe also consider the fit of the model to the data using the R-squared value and the F-test. An R-squared value close to 1 indicates a good fit and p-value less than 0.05 for the F-test indicates the model explains a significant amount of variation.\nThe assumptions of the General Linear Model must be met for the p-values to be accurate. These are: are that the relationship between the response and the explanatory variables is linear and that the residuals are normally distributed and have homogeneity of variance. We check these assumptions by plotting the data and the residuals.\nWe use the lm() function to fit a linear model in R. The summary() function gives us the p-values and R-squared value and the plot() function gives us diagnostic plots to check the assumptions.\nWhen reporting the results of statistical tests give the significance, direction and magnitude of the effect and use figures to demonstrate the statement.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>What is a statistical model?</span>"
    ]
  },
  {
    "objectID": "single_linear_regression.html",
    "href": "single_linear_regression.html",
    "title": "\n12¬† Single linear regression\n",
    "section": "",
    "text": "12.1 Overview\nSingle linear regression is an appropriate way to analyse data when:\nApplying a single linear regression to data means putting a line of best fit through it. The intercept and the slope of the true population relationship is estimated from the sample you have. We test whether those two parameters differ significantly from zero.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Single linear regression</span>"
    ]
  },
  {
    "objectID": "single_linear_regression.html#overview",
    "href": "single_linear_regression.html#overview",
    "title": "\n12¬† Single linear regression\n",
    "section": "",
    "text": "You have two continuous variables\nOne of the variables is explanatory and the other is a response. That is, one variable, the \\(x\\), ‚Äúcauses‚Äù the \\(y\\).\nThe explanatory variable has been chosen, set or manipulated and the other variable is the measured response. This is sometimes described as the \\(x\\) being ‚Äúsampled without error‚Äù\nThe response variable, \\(y\\), is randomly sampled for each \\(x\\) with a normal distribution and those normal distributions have the same variance.\nThe relationship between the variables is linear\n\n\n\n12.1.1 Reporting\nReporting the significance of effect, direction of effect, magnitude of effect for a single linear regression means making the following clear to the reader:\n\nthe significance of effect - whether the slope is significantly different from zero\nthe direction of effect - whether the slope is positive or negative\nthe magnitude of effect - the slope itself\n\nFigures should reflect what you have said in the statements. Ideally they should show both the raw data and the statistical model:\nWe will explore all of these ideas with an example.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Single linear regression</span>"
    ]
  },
  {
    "objectID": "single_linear_regression.html#your-turn",
    "href": "single_linear_regression.html#your-turn",
    "title": "\n12¬† Single linear regression\n",
    "section": "\n12.2 üé¨ Your turn!",
    "text": "12.2 üé¨ Your turn!\nIf you want to code along you will need to start a new RStudio project, add a data-raw folder and open a new script. You will also need to load the tidyverse package (Wickham et al. 2019).",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Single linear regression</span>"
    ]
  },
  {
    "objectID": "single_linear_regression.html#single-linear-regression",
    "href": "single_linear_regression.html#single-linear-regression",
    "title": "\n12¬† Single linear regression\n",
    "section": "\n12.3 Single linear regression",
    "text": "12.3 Single linear regression\nThree replicates water baths were set up at each of five temperatures (10, 11C, 12C, 13C, 14C). Ten Brine Shrimp (Artemia salina) were placed in each and their average respiration rate per water bath was measured (in arbitrary units). The data are in shrimp.txt.\n\n12.3.1 Import and explore\nImport the data:\n\nshrimp &lt;- read_table(\"data-raw/shrimp.txt\")\n\n\n\n\n\n\ntemperature\nrespiration\n\n\n\n6\n11.94\n\n\n6\n9.54\n\n\n6\n10.22\n\n\n8\n11.01\n\n\n8\n12.94\n\n\n8\n11.93\n\n\n10\n12.30\n\n\n10\n15.95\n\n\n10\n14.47\n\n\n12\n14.28\n\n\n12\n17.56\n\n\n12\n15.56\n\n\n14\n16.88\n\n\n14\n18.96\n\n\n14\n17.78\n\n\n\n\n\n\n\nThese data are in tidy format (Wickham 2014) - all the respiration values are in one column with another column indicating the water bath temperature. There is only one water bath per row. This means they are well formatted for analysis and plotting.\nIn the first instance, it is sensible to create a rough plot of our data (See Figure¬†12.1). Plotting data early helps us in multiple ways:\n\nit helps identify whether there missing or extreme values\nit allows us to see if the relationship is roughly linear\nit tells us whether any relationship positive or negative\n\nScatter plots (geom_point()) are a good choice for exploratory plotting with data like these.\n\nggplot(data = shrimp,\n       aes(x = temperature, y = respiration)) +\n  geom_point()\n\n\n\n\n\n\nFigure¬†12.1: A default scatter plot of the relationship between temperature and respiration rate in brine shrimp is enough for us to see that respiration rate seems to increase with temperature approximately linearly.\n\n\n\n\nThe figure suggests that respiration rate increases with temperature and there are no particularly extreme values. We can also see that any relationship is roughly linear.\n\n12.3.2 Do a regression with lm()\n\nWe can create a single linear regression model like this:\n\nmod &lt;- lm(data = shrimp, respiration ~ temperature)\n\nAnd examine the model with:\n\nsummary(mod)\n## \n## Call:\n## lm(formula = respiration ~ temperature, data = shrimp)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.7880 -0.8780 -0.1773  0.9393  1.8620 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   4.8613     1.1703   4.154  0.00113 ** \n## temperature   0.9227     0.1126   8.194 1.72e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.234 on 13 degrees of freedom\n## Multiple R-squared:  0.8378, Adjusted R-squared:  0.8253 \n## F-statistic: 67.13 on 1 and 13 DF,  p-value: 1.719e-06\n\nWhat do all these results mean?\nThe Estimate in the Coefficients table give:\n\nthe (Intercept) known as \\(\\beta_0\\), which is the value of the y (the response) when the value of x (the explanatory) is zero.\nthe slope labelled temperature known as \\(\\beta_1\\), which is the amount of y you add for each unit of x. temperature is positive so respiration rate increases with temperature\n\nFigure¬†12.2 shows the model and its parameters.\nThe p-values on each line are tests of whether that coefficient is different from zero. Thus it is:\ntemperature  0.91850    0.09182  10.003 1.79e-07 ***\nthat tells us the slope is significantly different from zero and thus there is a significant relationship between temperature and respiration rate.\nThe F value and p-value in the last line are a test of whether the model as a whole explains a significant amount of variation in the response variable. For a regression, this is exactly equivalent to the test of the slope against zero and the two p-values will be the same.\n\n\n\n\n\n\n\nFigure¬†12.2: In a linear model, the first estimate is the intercept and the second estimate is the ‚Äòslope‚Äô.\n\n\n\n\n\n12.3.3 Check assumptions\nCheck the assumptions: All general linear models assume the ‚Äúresiduals‚Äù are normally distributed and have ‚Äúhomogeneity‚Äù of variance.\nOur first check of these assumptions is to use common sense: respiration is a continuous variable and we would expect it to be normally distributed thus we would expect the residuals to be normally distributed\nWe then proceed by plotting residuals. The plot() function can be used to plot the residuals against the fitted values (See Figure¬†12.3). This is a good way to check for homogeneity of variance.\n\nplot(mod, which = 1)\n\n\n\n\n\n\nFigure¬†12.3: A plot of the residuals against the fitted values shows no obvious pattern as the points are roughly evenly distributed around the line. This is a good sign for the assumption of homogeneity of variance.\n\n\n\n\nWe can also use a histogram to check for normality (See Figure¬†12.4).\n\nggplot(mapping = aes(x = mod$residuals)) + \n  geom_histogram(bins = 5)\n\n\n\n\n\n\nFigure¬†12.4: A histogram of residuals is symetrical and seems consistent with a normal distribution. This is a good sign for the assumption of normally distributed residuals.\n\n\n\n\nFinally, we can use the Shapiro-Wilk test to test for normality.\n\nshapiro.test(mod$residuals)\n## \n##  Shapiro-Wilk normality test\n## \n## data:  mod$residuals\n## W = 0.95115, p-value = 0.5428\n\nThe p-value is greater than 0.05 so this test of the normality assumption is not significant. Note that ‚Äúnot significant‚Äù means not significantly different from a normal distribution. It does not mean definitely normally distributed.\nTaken together, these results suggest that the assumptions of normality and homogeneity of variance are not violated.\n\n12.3.4 Report\nThe temperature explained a significant amount of the variation in respiration rate (ANOVA: F = 67; d.f. = 1, 13; p &lt; 0.001). The regression line is: Respiration rate = 4.86 + 0.92 * temperature. See Figure¬†12.5.\nCodeggplot(data = shrimp, \n                aes(x = temperature, y = respiration)) +\n  geom_point(size = 2) +   \n  geom_smooth(method = \"lm\", \n              se = FALSE,\n              colour = \"black\") +\n  scale_x_continuous(expand = c(0,0),\n                     limits = c(0, 15.5),\n                      name = \"Temperature (C)\") +\n  scale_y_continuous(expand = c(0,0), \n                     limits = c(0, 20),\n                     name = \"Respiration (units)\") +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†12.5: Respiration rate of Artemia salina increases with temperature. Three replicate water baths were set up at each of five temperatures (10, 11C, 12C, 13C, 14C). Ten Brine Shrimp (Artemia salina) were placed in each and their average respiration rate per water bath was measured. There was a significant effect of altering water bath temperature on the average respiration rate (ANOVA: F = 67; d.f. = 1, 13; p &lt; 0.001). The regression line is: Respiration rate = 4.86 + 0.92 * temperature. Data analysis was conducted in R (R Core Team 2023) with tidyverse packages (Wickham et al. 2019).\n\n\n\n\n\n\nR Core Team. 2023. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nWickham, Hadley. 2014. ‚ÄúTidy Data.‚Äù Journal of Statistical Software, Articles 59 (10): 1‚Äì23. https://vita.had.co.nz/papers/tidy-data.pdf.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D‚ÄôAgostino McGowan, Romain Fran√ßois, Garrett Grolemund, et al. 2019. ‚ÄúWelcome to the Tidyverse‚Äù 4: 1686. https://doi.org/10.21105/joss.01686.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Single linear regression</span>"
    ]
  },
  {
    "objectID": "two_sample_tests.html",
    "href": "two_sample_tests.html",
    "title": "\n13¬† Two-Sample tests\n",
    "section": "",
    "text": "13.1 Overview\nIn the last chapter we learned about single linear regression, a technique used when the explanatory variable is continuous. We now turn our attention to cases where our explanatory variable is categorical and has two groups. For example, we might want to know if there is a difference in mass between two subspecies of chaffinch, or if marks in two subjects are the same.\nWe use lm() or wilcox.test() to carry out a two-sample test depending on whether the assumptions of lm() are met. The general linear models applied with lm() are based on the normal distribution and known as parametric tests because they use the parameters of the normal distribution (the mean and standard deviation) to determine if an effect is significant. Null hypotheses are about a mean or difference between means. The assumptions need to be met for the p-values generated to be accurate.\nIf the assumptions are not met, we can use alternatives known as non-parametric tests. Non-parametric tests are based on the ranks of values rather than the actual values themselves. Null hypotheses are about the mean rank rather than the mean. These tests have fewer assumptions and can be used in more situations but the downside is they tend to be less powerful. This means they are less able to detect a difference where one exists.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Two-Sample tests</span>"
    ]
  },
  {
    "objectID": "two_sample_tests.html#overview",
    "href": "two_sample_tests.html#overview",
    "title": "\n13¬† Two-Sample tests\n",
    "section": "",
    "text": "13.1.1 Independent samples and paired samples.\nAn important consideration in conducting tests is whether the values in one group are independent of the values in another group. Non-independence occurs when the two measures are linked in some way. The link could be that they are from the same individual, the same time or the same location. For example, if we want to evaluate a treatment for high blood pressure we might measure blood pressure before and after the treatment on the same individuals. This would mean the before and after measures were not independent. If pairs of observations in different groups have something in common that make them more similar to each other than to other observations, then those observations are no independent. We use a different testing approach for independent and non-independent samples.\n\n13.1.2 T-tests\nA linear model with one explanatory variable with two groups is also known as a two-sample t-test when the samples are independent and as a paired-samples t-test when they are not. R does have a t.test() function which allows you to fit a linear model with just two groups. However, here we teach you to use and interpret the lm() function because it is more generalisable. You can use lm() when you have three or more groups or additional explanatory variables. The output of lm() is also in the same form as many other statistical functions in R. This means what you learn in performing t-tests with lm() will help you learn other methods more easily. However, it is definitely not wrong to use t.test() rather than lm() for two-group situations - the procedures are identical and the p-values will be the same.\n\n13.1.3 Model assumptions\nThe assumptions of the general linear model are that the residuals are normally distributed and have homogeneity of variance. A residual is the difference between the predicted value and the observed value.\nIf we have a continuous response and a categorical explanatory variable with two groups, we usually apply the general linear model with lm() and then check the assumptions, however, we can sometimes tell when a non-parametric test would be more appropriate before that:\n\nUse common sense - the response should be continuous (or nearly continuous, see Ideas about data: Theory and practice). Consider whether you would expect the response to be continuous.\nWe expect decimal places and few repeated values.\n\nTo examine the assumptions after fitting the linear model, we plot the residuals and test them against the normal distribution in the same way as we did for single linear regression.\n\n13.1.4 Reporting\nIn reporting the result of two-sample test we give:\n\n\nthe significance of effect - whether there is there a difference between the groups\n\nparametric: whether there is there a difference between the groups means\nnon-parametric: whether there is there a difference between the group medians\n\n\nthe direction of effect - which of the means/medians is greater\n\nthe magnitude of effect - how big is the difference between the means/medians\n\nparametric: the means and standard errors for each group or the mean difference for paired samples\nnon-parametric: the medians for each group or the median difference for paired samples\n\n\n\nFigures should reflect what you have said in the statements. Ideally they should show both the raw data and the statistical model:\n\nparametric: means and standard errors\nnon-parametric: boxplots with medians and interquartile range\n\nWe will explore all of these ideas with some examples.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Two-Sample tests</span>"
    ]
  },
  {
    "objectID": "two_sample_tests.html#your-turn",
    "href": "two_sample_tests.html#your-turn",
    "title": "\n13¬† Two-Sample tests\n",
    "section": "\n13.2 üé¨ Your turn!",
    "text": "13.2 üé¨ Your turn!\nIf you want to code along you will need to start a new RStudio project, add a data-raw folder and open a new script. You will also need to load the tidyverse package (Wickham et al. 2019).",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Two-Sample tests</span>"
    ]
  },
  {
    "objectID": "two_sample_tests.html#two-independent-samples-parametric",
    "href": "two_sample_tests.html#two-independent-samples-parametric",
    "title": "\n13¬† Two-Sample tests\n",
    "section": "\n13.3 Two independent samples, parametric",
    "text": "13.3 Two independent samples, parametric\nA number of subspecies of the common chaffinch have been described, based principally on the differences in the pattern and colour of the adult male plumage (Su√°rez et al. 2009). Two of groups (See Figure¬†13.1) of these subspecies are :\n\n‚Äúcoelebs group‚Äù that occurs in Europe and Asia\n‚Äúcanariensis group‚Äù that occurs on the Canary Islands\n\n\n\n\n\n\n\n\n\n\n(a) F. c. coelebs\n\n\n\n\n\n\n\n\n\n(b) F. c. palmae\n\n\n\n\n\n\nFigure¬†13.1: Adult male Fringilla coelebs of the coelebs group on the left (Andreas Trepte, CC BY-SA 2.5 https://creativecommons.org/licenses/by-sa/2.5, via Wikimedia Commons) and of the canariensis group on the right (H. Zell, CC BY-SA 3.0 https://creativecommons.org/licenses/by-sa/3.0, via Wikimedia Commons).\n\n\nThe data in chaff.txt give the masses of twenty individuals from each subspecies. We want to know if the subspecies differ in mass. These groups are independent - there is no link between values in one group and any value in the other group.\n\n13.3.1 Import and explore\nImport the data:\n\nchaff &lt;- read_table(\"data-raw/chaff.txt\")\n\n\n\n\n\n\nsubspecies\nmass\n\n\n\ncoelebs\n18.3\n\n\ncoelebs\n22.1\n\n\ncoelebs\n22.4\n\n\ncoelebs\n18.5\n\n\ncoelebs\n22.2\n\n\ncoelebs\n19.3\n\n\ncoelebs\n17.8\n\n\ncoelebs\n20.2\n\n\ncoelebs\n22.1\n\n\ncoelebs\n16.6\n\n\ncoelebs\n20.7\n\n\ncoelebs\n18.7\n\n\ncoelebs\n22.6\n\n\ncoelebs\n21.5\n\n\ncoelebs\n21.7\n\n\ncoelebs\n19.9\n\n\ncoelebs\n23.1\n\n\ncoelebs\n17.8\n\n\ncoelebs\n19.5\n\n\ncoelebs\n24.6\n\n\ncanariensis\n22.7\n\n\ncanariensis\n20.6\n\n\ncanariensis\n25.4\n\n\ncanariensis\n20.4\n\n\ncanariensis\n21.6\n\n\ncanariensis\n17.0\n\n\ncanariensis\n26.4\n\n\ncanariensis\n20.4\n\n\ncanariensis\n24.7\n\n\ncanariensis\n21.8\n\n\ncanariensis\n23.4\n\n\ncanariensis\n24.4\n\n\ncanariensis\n21.0\n\n\ncanariensis\n23.4\n\n\ncanariensis\n20.5\n\n\ncanariensis\n21.4\n\n\ncanariensis\n21.5\n\n\ncanariensis\n23.7\n\n\ncanariensis\n23.4\n\n\ncanariensis\n21.8\n\n\n\n\n\n\n\nThese data are in tidy format (Wickham 2014) - all the mass values are in one column with another column indicating the subspecies. This means they are well formatted for analysis and plotting.\nIn the first instance, it is always sensible to create a rough plot of our data. This is to give us an overview and help identify if there are any issues like missing or extreme values. It also gives us idea what we are expecting from the analysis which will make it easier for us to identify if we make some mistake in applying that analysis.\nViolin plots (geom_violin()), box plots (geom_boxplot(), see Figure¬†13.2) or scatter plots (geom_point()) all make good choices for exploratory plotting and it does not matter which of these you choose.\n\nggplot(data = chaff,\n       aes(x = subspecies, y = mass)) +\n  geom_boxplot()\n\n\n\n\n\n\nFigure¬†13.2: The mass of two subspecies of chaffinch. A boxplot is a useful way to get an overview of the data and helps us identify any issues such as missing or extreme values. It also tells us what to expect from the analysis.\n\n\n\n\nR will order the groups alphabetically by default.\nThe figure suggests that the canariensis group is heavier than the coelebs group.\nSummarising the data for each subspecies group is the next sensible step. The most useful summary statistics are the means, standard deviations, sample sizes and standard errors. I recommend the group_by() and summarise() approach:\n\nchaff_summary &lt;- chaff |&gt; \n  group_by(subspecies) |&gt; \n  summarise(mean = mean(mass),\n            std = sd(mass),\n            n = length(mass),\n            se = std/sqrt(n))\n\nWe have save the results to chaff_summary so that we can use the means and standard errors in our plot later.\n\nchaff_summary\n## # A tibble: 2 √ó 5\n##   subspecies   mean   std     n    se\n##   &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n## 1 canariensis  22.3  2.15    20 0.481\n## 2 coelebs      20.5  2.14    20 0.478\n\n\n13.3.2 Apply lm()\n\nWe can create a two-sample model like this:\n\nmod &lt;- lm(data = chaff, mass ~ subspecies)\n\nAnd examine the model with:\n\nsummary(mod)\n## \n## Call:\n## lm(formula = mass ~ subspecies, data = chaff)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.2750 -1.7000 -0.3775  1.6200  4.1250 \n## \n## Coefficients:\n##                   Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)        22.2750     0.4795  46.456   &lt;2e-16 ***\n## subspeciescoelebs  -1.7950     0.6781  -2.647   0.0118 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.144 on 38 degrees of freedom\n## Multiple R-squared:  0.1557, Adjusted R-squared:  0.1335 \n## F-statistic: 7.007 on 1 and 38 DF,  p-value: 0.01175\n\nThe Estimates in the Coefficients table give:\n\n(Intercept) known as \\(\\beta_0\\). The mean of the canariensis group (Figure¬†13.3). Just as the intercept is the value of the y (the response) when the value of x (the explanatory) is zero in a simple linear regression, this is the value of mass when the subspecies is at its first level. The order of the levels is alphabetical by default.\nsubspeciescoelebs, known as \\(\\beta_1\\), is what needs to be added to the mean of the canariensis group to get the mean of the coelebs group (Figure¬†13.3). Just as the slope is amount of y that needs to be added for each unit of x in a simple linear regression, this is the amount of mass that needs to be added when the subspecies goes from its first level to its second level (i.e., one unit). The subspeciescoelebs estimate is negative so the the coelebs group mean is lower than the canariensis group mean\n\nThe p-values on each line are tests of whether that coefficient is different from zero. Thus it is:\nsubspeciescoelebs  -1.7950     0.6781  -2.647   0.0118 *\nthat tells us the difference between the means is significant.\nThe F value and p-value in the last line are a test of whether the model as a whole explains a significant amount of variation in the response variable. For a two-sample test, just like a regression, this is exactly equivalent to the test of the slope against zero and the two p-values will be the same.\n\n\n\n\n\n\n\nFigure¬†13.3: In a two-sample linear model, the first estimate is the intercept which is the mean of the first group. The second estimate is the ‚Äòslope‚Äô which is what has to added to the intercept to get the second group mean. Note that y axis starts at 15 to create more space for the annotations.\n\n\n\n\n\n13.3.3 Check assumptions\nCheck the assumptions: All general linear models assume the ‚Äúresiduals‚Äù are normally distributed and have ‚Äúhomogeneity‚Äù of variance.\nOur first check of these assumptions is to use common sense: mass is a continuous variable and we would expect it to be normally distributed thus we would also expect the residuals to be normally distributed.\nWe then plot the residuals. The plot() function can be used to plot the residuals against the fitted values (See Figure¬†13.4). This is a good way to check for homogeneity of variance.\n\nplot(mod, which = 1)\n\n\n\n\n\n\nFigure¬†13.4: A plot of the residuals against the fitted values shows the points are distributed similarly in each group. This is a good sign for the assumption of homogeneity of variance.\n\n\n\n\nWe can also use a histogram to check for normality (See Figure¬†13.5).\n\nggplot(mapping = aes(x = mod$residuals)) + \n  geom_histogram(bins = 10)\n\n\n\n\n\n\nFigure¬†13.5: A histogram of residuals is symetrical and seems consistent with a normal distribution. This is a good sign for the assumption of normally distributed residuals.\n\n\n\n\nFinally, we can use the Shapiro-Wilk test to test for normality.\n\nshapiro.test(mod$residuals)\n## \n##  Shapiro-Wilk normality test\n## \n## data:  mod$residuals\n## W = 0.98046, p-value = 0.7067\n\nThe p-value is greater than 0.05 so this test of the normality assumption is not significant.\nTaken together, these results suggest that the assumptions of normality and homogeneity of variance are not violated.\n\n13.3.4 Report\nCanariensis chaffinches (\\(\\bar{x} \\pm s.e\\): 22.48 \\(\\pm\\) 0.48) were significantly heavier than Coelebs (20.28 \\(\\pm\\) 0.48 ) (t = 2.65; d.f. = 38; p = 0.012). See Figure¬†13.6.\nCodeggplot() +\n  geom_point(data = chaff, aes(x = subspecies, y = mass),\n             position = position_jitter(width = 0.1, height = 0),\n             colour = \"gray50\") +\n  geom_errorbar(data = chaff_summary, \n                aes(x = subspecies, ymin = mean - se, ymax = mean + se),\n                width = 0.3) +\n  geom_errorbar(data = chaff_summary, \n                aes(x = subspecies, ymin = mean, ymax = mean),\n                width = 0.2) +\n  scale_y_continuous(name = \"Mass (g)\", \n                     limits = c(0, 30), \n                     expand = c(0, 0)) +\n  scale_x_discrete(name = \"Subspecies\", \n                   labels = c(\"Canariensis\", \"Coelebs\")) +\n  annotate(\"segment\", x = 1, xend = 2, \n           y = 28, yend = 28,\n           colour = \"black\") +\n  annotate(\"text\", x = 1.5,  y = 29, \n           label = expression(italic(p)~\"= 0.012\")) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†13.6: Canariensis chaffinches are heavier than Coelebs chaffinches. The mean mass of 20 randomly sampled males from each subspecies was determined. Error bars are \\(\\pm\\) 1 standard error. Canariensis chaffinches were significantly heavier than Coelebs (t = 2.65; d.f. = 38; p = 0.012). Data analysis was conducted in R (R Core Team 2023) with tidyverse packages (Wickham et al. 2019).",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Two-Sample tests</span>"
    ]
  },
  {
    "objectID": "two_sample_tests.html#two-independent-samples-non-parametric",
    "href": "two_sample_tests.html#two-independent-samples-non-parametric",
    "title": "\n13¬† Two-Sample tests\n",
    "section": "\n13.4 Two independent samples, non-parametric",
    "text": "13.4 Two independent samples, non-parametric\nThe non-parametric equivalent of the linear model with two independent samples is the ‚ÄúWilcoxon rank sum test‚Äù (Wilcoxon 1945). It is commonly also known as the Mann-Whitney or Wilcoxon‚ÄìMann‚ÄìWhitney.\nThe general question you have about your data - are these two groups different - is the same, but one of more of the following is true:\n\nthe response variable is not continuous\nthe residuals are not normally distributed\nthe sample size is too small to tell if they are normally distributed.\nthe variance is not homogeneous\n\nThe test is a applied in R with the wilcox.test() function.\nThe data in arabidopsis.txt give the number of leaves on eight wildtype and eight mutant Arabidopsis thaliana plants. We want to know if the two types of plants have differing numbers of leaves. These are counts, so they are not continuous and the sample sizes are quite small. A non-parametric test is a safer option.\n\n13.4.1 Import and explore\n\narabidopsis &lt;- read_table(\"data-raw/arabidopsis.txt\")\n\nThese data are in tidy format (Wickham 2014) - the numbers of leaves are in one column with another column indicating whether the observation comes from a wildtype or mutant Arabidopsis. This means they are well formatted for analysis and plotting.\nCreate a quick plot of the data:\n\nggplot(data = arabidopsis, \n       aes(x = type, y = leaves)) +\n  geom_boxplot()\n\n\n\n\n\n\nFigure¬†13.7: The number of leaves on mutant and wildtype plants. A boxplot is a useful way to get an overview of the data and helps us identify any issues such as missing or extreme values. It also tells us what to expect from the analysis.\n\n\n\n\nOur rough plot shows that the mutant plants have fewer leaves than the wildtype plants.\nSummarising the data using the median and interquartile range is more aligned to the type of data and the type of analysis than using means and standard deviations:\n\narabidopsis_summary &lt;- arabidopsis |&gt; \n  group_by(type) |&gt; \n  summarise(median = median(leaves),\n            interquartile  = IQR(leaves),\n            n = length(leaves))\n\nView the results:\n\narabidopsis_summary\n## # A tibble: 2 √ó 4\n##   type   median interquartile     n\n##   &lt;chr&gt;   &lt;dbl&gt;         &lt;dbl&gt; &lt;int&gt;\n## 1 mutant    5             2.5     8\n## 2 wild      8.5           1.5     8\n\n\n13.4.2 Apply wilcox.test()\n\nWe pass the dataframe and variables to wilcox.test() in the same way as we did for lm(). We give the data argument and a ‚Äúformula‚Äù which says leaves ~ type meaning ‚Äúexplain leaves by type‚Äù.\n\nwilcox.test(data = arabidopsis, leaves ~ type)\n## \n##  Wilcoxon rank sum test with continuity correction\n## \n## data:  leaves by type\n## W = 5, p-value = 0.005051\n## alternative hypothesis: true location shift is not equal to 0\n\nThe warning message ‚ÄúWarning: cannot compute exact p-value with ties‚Äù is not something to worry about too much. It is a warning rather than an indication that your results are incorrect. It means the p -value is based on an approximation rather than being exact because there are ties (some values are the same).\nThe result of the test is given on this line: W = 5, p-value = 0.005051. W is the test statistic. The p-value is less than 0.05 meaning there is a significant difference in the number of leaves on wildtype and mutant plants.\n\n13.4.3 Report\nThere are significantly more leaves on wildtype (median = 8.5) than mutant (median = 5) plants (Wilcoxon rank sum test: W = 5, \\(n_1\\) = 8, \\(n_2\\) = 8, p = 0.005). See Figure¬†13.8.\nggplot(data = arabidopsis, \n       aes(x = type, y = leaves)) +\n  geom_boxplot() +\n  scale_y_continuous(name = \"Number of leaves\", \n                     limits = c(0, 12), \n                     expand = c(0, 0)) +\n  scale_x_discrete(name = \"\", \n                   labels = c(\"Mutatnt\", \"Wildtype\")) +\n  annotate(\"segment\", x = 1, xend = 2, \n           y = 10.5, yend = 10.5,\n           colour = \"black\") +\n  annotate(\"text\", x = 1.5,  y = 11, \n           label = expression(italic(p)~\"= 0.005\")) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†13.8: Mutant Arabidopsis thaliana have fewer leaves. There are significantly more leaves on wildtype than mutant plants (Wilcoxon rank sum test: W = 5, \\(n_1\\) = 8, \\(n_2\\) = 8, p = 0.005). The heavy lines indicate the median of leaves, boxes indicate the interquartile range and whiskers the range. Data analysis was conducted in R (R Core Team 2023) with tidyverse packages (Wickham et al. 2019).",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Two-Sample tests</span>"
    ]
  },
  {
    "objectID": "two_sample_tests.html#two-paired-samples-parametric",
    "href": "two_sample_tests.html#two-paired-samples-parametric",
    "title": "\n13¬† Two-Sample tests\n",
    "section": "\n13.5 Two paired-samples, parametric",
    "text": "13.5 Two paired-samples, parametric\nThe data in marks.csv give the marks for ten students in two subjects: Data Analysis and Biology. These data are paired because we have two marks from one student so that a mark in one group has a closer relationship with one of the marks in the other group than with any of the the other values. We want to know if students to equally well in both subjects.\n\n13.5.1 Import and explore\nImport the data:\n\nmarks &lt;- read_csv(\"data-raw/marks.csv\")\n\nSince these data are paired, it makes sense to highlight how the marks differ for each student. One way of doing that is to draw a line linking their marks in each subject. This is known as a spaghetti plot. We can use two geoms: geom_point() and geom_line(). To join a student‚Äôs marks we need to set the group aesthetic to student.1\n\nggplot(data = marks, aes(x = subject, y = mark)) +\n  geom_point() +\n  geom_line(aes(group = student))\n\n\n\n\n\n\n\nSummarise the data so that we can use the means in plots later:\n\nmarks_summary &lt;- marks |&gt; \n  group_by(subject) |&gt; \n  summarise(mean = mean(mark))\n\nA paired test requires us to test whether the difference in marks between the two subjects is zero on average. One handy way to achieve this is to organise our groups into two columns. The pivot_wider() function will do this for us. We need to tell it what column gives the identifiers (i.e. what column matches the pairs). This is the student number in this example. We also need to say which variable contains the values that will become the column names and which contains the values.\nPivot the data so there is a column for each subject:\n\n\nmarks_wide &lt;- marks |&gt; \n  pivot_wider(id_cols = student,\n              names_from = subject,\n              values_from = mark)\n\nWe can summarise the difference between subject, DataAnalysis - Biology as follows:\n\nmarks_wide |&gt; \n  summarise(mean_diff = mean(DataAnalysis - Biology),\n            sd_diff = sd(DataAnalysis - Biology),\n            n_diff = length(DataAnalysis - Biology),\n            se_diff = sd_diff/sqrt(n_diff))\n## # A tibble: 1 √ó 4\n##   mean_diff sd_diff n_diff se_diff\n##       &lt;dbl&gt;   &lt;dbl&gt;  &lt;int&gt;   &lt;dbl&gt;\n## 1         7    8.96     10    2.83\n\nThe mean difference is positive meaning students have higher marks in Data Analysis on average.\n\n13.5.2 Apply lm()\n\nWe can create a paired-sample model with the lm() function2 like this:\n\nmod &lt;- lm(data = marks_wide, DataAnalysis - Biology ~ 1)\n\nThe response here is the differences DataAnalysis - Biology and the ~ 1 indicates we only have one group so we are only interested in testing a single mean against a value of zero. Another way of saying this is we have an ‚Äúintercept only model‚Äù meaning were are estimating only \\(\\beta_0\\). There is no \\(\\beta_1\\).\n\nsummary(mod)\n## \n## Call:\n## lm(formula = DataAnalysis - Biology ~ 1, data = marks_wide)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -13.00  -7.75   1.50   5.75  11.00 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)  \n## (Intercept)    7.000      2.832   2.471   0.0355 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 8.957 on 9 degrees of freedom\n\nThe Estimates in the Coefficients table gives the (Intercept) which is the mean of DataAnalysis - Biology. The average difference between subjects is 7.0 marks. The p-value of 0.0355 means 7.0 does differ significantly from zero. Individual students get significantly higher marks in Data Analysis than in Biology.\n\n13.5.3 Check assumptions\nWe might expect marks, and thus the difference between marks to be normally distributed. However, this is a very small sample and choosing a non-parametric test instead would be sensible. We will continue with this example to demonstrate how to interpret and report on the result of a parametric paired-samples test (paired-samples t-test).\nWe do not need to plot the residuals against the fitted values (plot(mod, which = 1)) for a paired test. The purpose of a residuals vs fitted plot is to check the variance is same for all fitted values. For a paired test, we only have one fitted value, the mean difference.\nThe normality of the residuals should be checked.\n\nggplot(mapping = aes(x = mod$residuals)) + \n  geom_histogram(bins = 3)\n\n\n\n\n\n\n\nWe only have 10 values so the distribution is never going to look smooth. We can‚Äôt draw strong conclusions from this but we do at least have a peak at 0. Similarly a normality test is likely to be non-significant because of the small sample size which means the test is not very powerful. This means a non-significant result is not strong evidence of the residuals following a normal distribution:\n\nshapiro.test(mod$residuals)\n## \n##  Shapiro-Wilk normality test\n## \n## data:  mod$residuals\n## W = 0.90358, p-value = 0.2398\n\n\n13.5.4 Report\nIndividual students score significantly higher in Data Analysis than in Biology (t = 2.47; d.f. = 9; p = 0.0355) with an average difference of 6.5%. See Figure¬†13.9\nCodeggplot(data = marks, aes(x = subject, y = mark)) +\n  geom_point(pch = 1, size = 3) + \n  geom_line(aes(group = student), linetype = 3) +\n  geom_point(data = marks_summary, \n             aes(x = subject, y = mean), \n             size = 3) +\n  scale_x_discrete(name = \"\") +\n  scale_y_continuous(name = \"Mark\",\n                     expand = c(0, 0),\n                     limits = c(0, 110)) +\n  annotate(\"segment\", x = 1, xend = 2, \n           y = 105, yend = 105,\n           colour = \"black\") +\n  annotate(\"text\", x = 1.5,  y = 108, \n           label = expression(italic(p)~\"= 0.0355\")) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†13.9: Students score higher in Data Analysis than in Biology. Open circles indicate an individual student‚Äôs marks in each subject with dashed lines joining their marks in each subject. The filled circles indicate the mean mark for each subject. Individual students score significantly higher in Data Analysis than in Biology (t = 2.47; d.f. = 9; p = 0.0355) with an average. difference of 6.5%. Data analysis was conducted in R (R Core Team 2023) with tidyverse packages (Wickham et al. 2019).",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Two-Sample tests</span>"
    ]
  },
  {
    "objectID": "two_sample_tests.html#two-paired-samples-non-parametric",
    "href": "two_sample_tests.html#two-paired-samples-non-parametric",
    "title": "\n13¬† Two-Sample tests\n",
    "section": "\n13.6 Two paired-samples, non-parametric",
    "text": "13.6 Two paired-samples, non-parametric\nWe have the marks for just 10 students. This sample is too small for us to judge whether the marks are normally distributed. We will use a non-parametric test instead. The ‚ÄúWilcoxon signed-rank‚Äù test is the non-parametric equivalent of the paired-samples t-test. This is often referred to as the paired-sample Wilcoxon test, or just the Wilcoxon test.\nThe test is also applied in R with the wilcox.test() function but we add the paired = TRUE argument. We can also use the tidy format of the data, i.e., the long format with marks in one column and another column indicating the subject.\n\n13.6.1 Apply wilcox.test()\n\nTo apply a paired test with wilcox.test() we need to use the long format data (tidy) and add the paired = TRUE argument.\n\nwilcox.test(data = marks, mark ~ subject, paired = TRUE)\n## \n##  Wilcoxon signed rank test with continuity correction\n## \n## data:  mark by subject\n## V = 6.5, p-value = 0.03641\n## alternative hypothesis: true location shift is not equal to 0\n\n\n13.6.2 Report\nIndividual students score significantly higher in Data Analysis than in Biology (Wilcoxon signed rank test: V = 6.55; \\(n\\) = 10; p = 0.036). See@fig-marks",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Two-Sample tests</span>"
    ]
  },
  {
    "objectID": "two_sample_tests.html#summary",
    "href": "two_sample_tests.html#summary",
    "title": "\n13¬† Two-Sample tests\n",
    "section": "\n13.7 Summary",
    "text": "13.7 Summary\n\n\n\n\nR Core Team. 2023. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nSu√°rez, Nicol√°s M., Eva Betancor, Tilman E. Klassert, Teresa Almeida, Mariano Hern√°ndez, and Jos√© J. Pestano. 2009. ‚ÄúPhylogeography and Genetic Structure of the Canarian Common Chaffinch (Fringilla Coelebs) Inferred with mtDNA and Microsatellite Loci.‚Äù Molecular Phylogenetics and Evolution 53 (2): 556‚Äì64. https://doi.org/10.1016/j.ympev.2009.07.018.\n\n\nWickham, Hadley. 2014. ‚ÄúTidy Data.‚Äù Journal of Statistical Software, Articles 59 (10): 1‚Äì23. https://vita.had.co.nz/papers/tidy-data.pdf.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D‚ÄôAgostino McGowan, Romain Fran√ßois, Garrett Grolemund, et al. 2019. ‚ÄúWelcome to the Tidyverse‚Äù 4: 1686. https://doi.org/10.21105/joss.01686.\n\n\nWilcoxon, Frank. 1945. ‚ÄúIndividual Comparisons by Ranking Methods.‚Äù Biometrics Bulletin 1 (6): 80‚Äì83. https://doi.org/10.2307/3001968.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Two-Sample tests</span>"
    ]
  },
  {
    "objectID": "two_sample_tests.html#footnotes",
    "href": "two_sample_tests.html#footnotes",
    "title": "\n13¬† Two-Sample tests\n",
    "section": "",
    "text": "You might like to try removing aes(group = student) to see what ggplot does when the lines are not grouped by student.‚Ü©Ô∏é\nThis is not the only way to apply a paired test. When there are only two groups and no other explanatory variables we can use t.test(data = marks, mark ~ subject, paired = TRUE). A more general method which works when you have two or more non-independent values (e.g., more than two subjects) or additional explanatory variables, is to create a ‚Äúlinear mixed model‚Äù with lmer()‚Ü©Ô∏é",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Two-Sample tests</span>"
    ]
  },
  {
    "objectID": "one_way_anova_and_kw.html",
    "href": "one_way_anova_and_kw.html",
    "title": "\n14¬† One-way ANOVA and Kruskal-Wallis\n",
    "section": "",
    "text": "14.1 Overview\nIn the last chapter, we learnt how to use and interpret the general linear model when the x variable was categorical with two groups. You will now extend that to situations when there are more than two groups. This is often known as the one-way ANOVA (analysis of variance). We will also learn about the Kruskal-Wallis test (Kruskal and Wallis 1952) which can be used when the assumptions of the general linear model are not met.\nWe use lm() to carry out a one-way ANOVA. General linear models applied with lm() are based on the normal distribution and known as parametric tests because they use the parameters of the normal distribution (the mean and standard deviation) to determine if an effect is significant. Null hypotheses are about a mean or difference between means. The assumptions need to be met for the p-values generated to be accurate.\nIf the assumptions are not met, we can use the non-parametric equivalent known as the Kruskal-Wallis test. Like other non-parametric tests, the Kruskal-Wallis test :\nThe process of using lm() to conduct a one-way ANOVA is very like the process for using lm() to conduct a two-sample t-test but with an important addition. When we get a signifcant effect of our explanatory variable, it only tells us that at least two of the means differ. To find out which means differ, we need a post-hoc test. A post-hoc (‚Äúafter this‚Äù) test is done after a significant ANOVA test. There are several possible post-hoc tests and we will be using Tukey‚Äôs HSD (honestly significant difference) test (Tukey 1949) implemented in the emmeans (Lenth 2023) package. Post-hoc tests make adjustments to the p-values to account for the fact that we are doing multiple comparisons. A Type I error happens when we reject a null hypothesis that is true and occurs with a probability of 0.05. Doing lots of comparisons makes it more likely we will get a significant result just by chance. The post-hoc test adjusts the p-values to account for this increased risk.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>One-way ANOVA and Kruskal-Wallis</span>"
    ]
  },
  {
    "objectID": "one_way_anova_and_kw.html#overview",
    "href": "one_way_anova_and_kw.html#overview",
    "title": "\n14¬† One-way ANOVA and Kruskal-Wallis\n",
    "section": "",
    "text": "is based on the ranks of values rather than the actual values themselves\nhas a null hypothesis about the mean rank rather than the mean\nhas fewer assumptions and can be used in more situations\ntends to be less powerful than a parametric test when the assumptions are met\n\n\n\n14.1.1 Model assumptions\nThe assumptions for a general linear model where the explanatory variable has two or more groups, are the same as for two groups: the residuals are normally distributed and have homogeneity of variance.\nIf we have a continuous response and a categorical explanatory variable with three or more groups, we usually apply the general linear model with lm() and then check the assumptions, however, we can sometimes tell when a non-parametric test would be more appropriate before that:\n\nUse common sense - the response should be continuous (or nearly continuous, see Ideas about data: Theory and practice). Consider whether you would expect the response to be continuous\nThere should decimal places and few repeated values.\n\nTo examine the assumptions after fitting the linear model, we plot the residuals and test them against the normal distribution in the same way as we did for single linear regression.\n\n14.1.2 Reporting\nIn reporting the result of one-way ANOVA or Kruskal-Wallis test, we include:\n\n\nthe significance of effect - whether there is there a difference between the groups\n\nparametric: whether there is there a difference between the groups means\nnon-parametric: whether there is there a difference between the group medians\n\n\nthe direction of effect - which of the means/medians is greater\n\nthe magnitude of effect - how big is the difference between the means/medians\n\nparametric: the means and standard errors for each group\nnon-parametric: the medians for each group\n\n\n\nFigures should reflect what you have said in the statements. Ideally they should show both the raw data and the statistical model:\n\nparametric: means and standard errors\nnon-parametric: boxplots with medians and interquartile range\n\nWe will explore all of these ideas with some examples.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>One-way ANOVA and Kruskal-Wallis</span>"
    ]
  },
  {
    "objectID": "one_way_anova_and_kw.html#your-turn",
    "href": "one_way_anova_and_kw.html#your-turn",
    "title": "\n14¬† One-way ANOVA and Kruskal-Wallis\n",
    "section": "\n14.2 üé¨ Your turn!",
    "text": "14.2 üé¨ Your turn!\nIf you want to code along you will need to start a new RStudio project, add a data-raw folder and open a new script. You will also need to load the tidyverse package (Wickham et al. 2019).",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>One-way ANOVA and Kruskal-Wallis</span>"
    ]
  },
  {
    "objectID": "one_way_anova_and_kw.html#one-way-anova",
    "href": "one_way_anova_and_kw.html#one-way-anova",
    "title": "\n14¬† One-way ANOVA and Kruskal-Wallis\n",
    "section": "\n14.3 One-way ANOVA",
    "text": "14.3 One-way ANOVA\nResearchers wanted the determine the best growth medium for growing bacterial cultures. They grew bacterial cultures on three different media formulations and measured the diameter of the colonies. The three formulations were:\n\nControl - a generic medium as formulated by the manufacturer\nsugar added - the generic medium with added sugar\nsugar and amino acids added - the generic medium with added sugar and amino acids\n\nThe data are in culture.csv.\n\n14.3.1 Import and explore\nImport the data:\n\nculture &lt;- read_csv(\"data-raw/culture.csv\")\n\n\n\n\n\n\ndiameter\nmedium\n\n\n\n11.22\ncontrol\n\n\n9.35\ncontrol\n\n\n9.15\ncontrol\n\n\n10.35\ncontrol\n\n\n9.63\ncontrol\n\n\n10.96\ncontrol\n\n\n10.07\ncontrol\n\n\n10.40\ncontrol\n\n\n10.33\ncontrol\n\n\n9.24\ncontrol\n\n\n8.90\nsugar added\n\n\n10.75\nsugar added\n\n\n11.95\nsugar added\n\n\n9.85\nsugar added\n\n\n10.12\nsugar added\n\n\n10.05\nsugar added\n\n\n9.60\nsugar added\n\n\n10.10\nsugar added\n\n\n10.20\nsugar added\n\n\n10.88\nsugar added\n\n\n10.45\nsugar and amino acids added\n\n\n13.19\nsugar and amino acids added\n\n\n11.84\nsugar and amino acids added\n\n\n13.35\nsugar and amino acids added\n\n\n11.22\nsugar and amino acids added\n\n\n9.86\nsugar and amino acids added\n\n\n10.27\nsugar and amino acids added\n\n\n10.62\nsugar and amino acids added\n\n\n11.78\nsugar and amino acids added\n\n\n11.43\nsugar and amino acids added\n\n\n\n\n\n\n\nThe Response variable is colony diameters in millimetres and we would expect it to be continuous. The Explanatory variable is type of media and is categorical with 3 groups. It is known ‚Äúone-way ANOVA‚Äù or ‚Äúone-factor ANOVA‚Äù because there is only one explanatory variable. It would still be one-way ANOVA if we had 4, 20 or 100 media.\nThese data are in tidy format (Wickham 2014) - all the diameter values are in one column with another column indicating the media. This means they are well formatted for analysis and plotting.\nIn the first instance it is sensible to create a rough plot of our data. This is to give us an overview and help identify if there are any issues like missing or extreme values. It also gives us idea what we are expecting from the analysis which will make it easier for us to identify if we make some mistake in applying that analysis.\nViolin plots (geom_violin(), see Figure¬†14.1), box plots (geom_boxplot()) or scatter plots (geom_point()) all make good choices for exploratory plotting and it does not matter which of these you choose.\n\nggplot(data = culture,\n       aes(x = medium, y = diameter)) +\n  geom_violin()\n\n\n\n\n\n\nFigure¬†14.1: The diameters of bacterial colonies when grown in one of three media. A violin plot is a useful way to get an overview of the data and helps us identify any issues such as missing or extreme values. It also tells us what to expect from the analysis.\n\n\n\n\nR will order the groups alphabetically by default.\nThe figure suggests that adding sugar and amino acids to the medium increases the diameter of the colonies.\nSummarising the data for each medium is the next sensible step. The most useful summary statistics are the means, standard deviations, sample sizes and standard errors. I recommend the group_by() and summarise() approach:\n\nculture_summary &lt;- culture %&gt;%\n  group_by(medium) %&gt;%\n  summarise(mean = mean(diameter),\n            std = sd(diameter),\n            n = length(diameter),\n            se = std/sqrt(n))\n\nWe have save the results to culture_summary so that we can use the means and standard errors in our plot later.\n\nculture_summary\n## # A tibble: 3 √ó 5\n##   medium                       mean   std     n    se\n##   &lt;chr&gt;                       &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n## 1 control                      10.1 0.716    10 0.226\n## 2 sugar added                  10.2 0.818    10 0.259\n## 3 sugar and amino acids added  11.4 1.18     10 0.373\n\n\n14.3.2 Apply lm()\n\nWe can create a one-way ANOVA model like this:\n\nmod &lt;- lm(data = culture, diameter ~ medium)\n\nAnd examine the model with:\n\nsummary(mod)\n## \n## Call:\n## lm(formula = diameter ~ medium, data = culture)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -1.541 -0.700 -0.080  0.424  1.949 \n## \n## Coefficients:\n##                                   Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)                        10.0700     0.2930  34.370  &lt; 2e-16 ***\n## mediumsugar added                   0.1700     0.4143   0.410  0.68483    \n## mediumsugar and amino acids added   1.3310     0.4143   3.212  0.00339 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.9265 on 27 degrees of freedom\n## Multiple R-squared:  0.3117, Adjusted R-squared:  0.2607 \n## F-statistic: 6.113 on 2 and 27 DF,  p-value: 0.00646\n\nThe Estimates in the Coefficients table give:\n\n(Intercept) known as \\(\\beta_0\\). The mean of the control group (Figure¬†14.2). Just as the intercept is the value of the y (the response) when the value of x (the explanatory) is zero in a simple linear regression, this is the value of diameter when the medium is at its first level. The order of the levels is alphabetical by default.\nmediumsugar added known as \\(\\beta_1\\). This is what needs to be added to the mean of the control group to get the mean of the ‚Äòmedium sugar added‚Äô group (Figure¬†13.3). Just as the slope is amount of y that needs to be added for each unit of x in a simple linear regression, this is the amount of diameter that needs to be added when the medium goes from its first level to its second level (i.e., one unit). The mediumsugar added estimate is positive so the the ‚Äòmedium sugar added‚Äô group mean is higher than the control group mean\nmediumsugar and amino acids added known as \\(\\beta_2\\) is what needs to be added to the mean of the control group to get the mean of the ‚Äòmedium sugar and amino acids added‚Äô group (Figure¬†13.3). Note that it is the amount added to the intercept (the control in this case). The mediumsugar and amino acids added estimate is positive so the the ‚Äòmedium sugar and amino acids added‚Äô group mean is higher than the control group mean\n\nIf we had more groups, we would have more estimates and all would be compared to the control group mean.\nThe p-values on each line are tests of whether that coefficient is different from zero.\n\n\n(Intercept)                     10.0700     0.2930  34.370  &lt; 2e-16 *** tells us that the control group mean is significantly different from zero. This is not a very interesting, it just means the control colonies have a diameter.\n\nmediumsugar added                 0.1700     0.4143   0.410  0.68483 tells us that the ‚Äòmedium sugar added‚Äô group mean is not significantly different from the control group mean.\n\nmediumsugar and amino acids added   1.3310     0.4143   3.212  0.00339 ** tells us that the ‚Äòmedium sugar and amino acids added‚Äô group mean is significantly different from the control group mean.\n\nNote: none of this output tells us whether the medium sugar and amino acids added‚Äô group mean is significantly different from the ‚Äòmedium sugar added‚Äô group mean. We need to do a post-hoc test for that.\nThe F value and p-value in the last line are a test of whether the model as a whole explains a significant amount of variation in the response variable.\n\n\n\n\n\n\n\nFigure¬†14.2: In an one-way ANOVA model with three groups, the first estimate is the intercept which is the mean of the first group. The second estimate is the ‚Äòslope‚Äô which is what has to added to the intercept to get the second group mean. The third estimate is the ‚Äòslope‚Äô which is what has to added to the intercept to get the third group mean. Note that y axis starts at 15 to create more space for the annotations.\n\n\n\n\nThe ANOVA is significant but this only tells us that growth medium matters, meaning at least two of the means differ. To find out which means differ, we need a post-hoc test. A post-hoc (‚Äúafter this‚Äù) test is done after a significant ANOVA test. There are several possible post-hoc tests and we will be using Tukey‚Äôs HSD (honestly significant difference) test (Tukey 1949) implemented in the emmeans (Lenth 2023) package.\nWe need to load the package:\n\nlibrary(emmeans)\n\nThen carry out the post-hoc test:\n\nemmeans(mod, ~ medium) |&gt; pairs()\n##  contrast                                  estimate    SE df t.ratio p.value\n##  control - sugar added                        -0.17 0.414 27  -0.410  0.9117\n##  control - sugar and amino acids added        -1.33 0.414 27  -3.212  0.0092\n##  sugar added - sugar and amino acids added    -1.16 0.414 27  -2.802  0.0244\n## \n## P value adjustment: tukey method for comparing a family of 3 estimates\n\nEach row is a comparison between the two means in the ‚Äòcontrast‚Äô column. The ‚Äòestimate‚Äô column is the difference between those means and the ‚Äòp.value‚Äô indicates whether that difference is significant.\nA plot can be used to visualise the result of the post-hoc which can be especially useful when there are very many comparisons.\n\nemmeans(mod, ~ medium) |&gt; plot()\n\n\n\n\n\n\n\nWhere the purple bars overlap, there is no significant difference.\nWe have found that colony diameters are significantly greater when sugar and amino acids are added but that adding sugar alone does not significantly increase colony diameter.\n\n14.3.3 Check assumptions\nCheck the assumptions: All general linear models assume the ‚Äúresiduals‚Äù are normally distributed and have ‚Äúhomogeneity‚Äù of variance.\nOur first check of these assumptions is to use common sense: diameter is a continuous and we would expect it to be normally distributed thus we would expect the residuals to be normally distributed thus we would expect the residuals to be normally distributed\nWe then proceed by plotting residuals. The plot() function can be used to plot the residuals against the fitted values (See Figure¬†14.3). This is a good way to check for homogeneity of variance.\n\nplot(mod, which = 1)\n\n\n\n\n\n\nFigure¬†14.3: A plot of the residuals against the fitted values shows whether the points are distributed similarly in each group. Any difference seems small but perhaps the residuals are more variable for the highest mean.\n\n\n\n\nPerhaps the variance is higher for the highest mean?\nWe can also use a histogram to check for normality (See Figure¬†14.4).\n\nggplot(mapping = aes(x = mod$residuals)) + \n  geom_histogram(bins = 8)\n\n\n\n\n\n\nFigure¬†14.4: A histogram of residuals is symetrical and seems consistent with a normal distribution. This is a good sign for the assumption of normally distributed residuals.\n\n\n\n\nFinally, we can use the Shapiro-Wilk test to test for normality.\n\nshapiro.test(mod$residuals)\n## \n##  Shapiro-Wilk normality test\n## \n## data:  mod$residuals\n## W = 0.96423, p-value = 0.3953\n\nThe p-value is greater than 0.05 so this test of the normality assumption is not significant.\nTaken together, these results suggest that the assumptions of normality and homogeneity of variance are probably not violated.\n\n14.3.4 Report\nThere is a significant effect of media on the diameter of bacterial colonies (F = 6.11; d.f. = 2, 27; p = 0.006) with colonies growing significantly better when both sugar and amino acids are added to the medium. Post-hoc testing with Tukey‚Äôs Honestly Significant Difference test (Tukey 1949) revealed the colony diameters were significantly larger when grown with both sugar and amino acids (\\(\\bar{x} \\pm s.e\\): 11.4 \\(\\pm\\) 0.37 mm) than with neither (10.2 \\(\\pm\\) 0.26 mm; p = 0.0092) or just sugar (10.1 \\(\\pm\\) 0.23 mm; p = 0.0244). See Figure¬†14.5.\n\nggplot() +\n  geom_point(data = culture, aes(x = medium, y = diameter),\n             position = position_jitter(width = 0.1, height = 0),\n             colour = \"gray50\") +\n  geom_errorbar(data = culture_summary, \n                aes(x = medium, ymin = mean - se, ymax = mean + se),\n                width = 0.3) +\n  geom_errorbar(data = culture_summary, \n                aes(x = medium, ymin = mean, ymax = mean),\n                width = 0.2) +\n  scale_y_continuous(name = \"Diameter (mm)\", \n                     limits = c(0, 16.5), \n                     expand = c(0, 0)) +\n  scale_x_discrete(name = \"Medium\", \n                   labels = c(\"Control\", \"Sugar added\", \"Sugar and amino acids added\")) +\n  annotate(\"segment\", x = 2, xend = 3, \n           y = 14, yend = 14,\n           colour = \"black\") +\n  annotate(\"text\", x = 2.5,  y = 14.5, \n           label = expression(italic(p)~\"= 0.0244\")) +\n    annotate(\"segment\", x = 1, xend = 3, \n           y = 15.5, yend = 15.5,\n           colour = \"black\") +\n  annotate(\"text\", x = 2,  y = 16, \n           label = expression(italic(p)~\"= 0.0092\")) +\n  theme_classic()\n\n\n\n\n\n\nFigure¬†14.5: Diameters of bacterial colonies grown on three types of media: control, with sugar added and with both sugar and amino acids added. Errors bars are ¬± 1 s.e.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>One-way ANOVA and Kruskal-Wallis</span>"
    ]
  },
  {
    "objectID": "two_way_anova.html",
    "href": "two_way_anova.html",
    "title": "\n15¬† Two-way ANOVA\n",
    "section": "",
    "text": "15.1 Overview\n‚Äì&gt;",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Two-way ANOVA</span>"
    ]
  },
  {
    "objectID": "two_way_anova.html#your-turn",
    "href": "two_way_anova.html#your-turn",
    "title": "\n15¬† Two-way ANOVA\n",
    "section": "\n15.2 üé¨ Your turn!",
    "text": "15.2 üé¨ Your turn!\nIf you want to code along you will need to start a new RStudio project, add a data-raw folder and open a new script. You will also need to load the tidyverse package (Wickham et al. 2019).",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Two-way ANOVA</span>"
    ]
  },
  {
    "objectID": "two_way_anova.html#two-way-anova",
    "href": "two_way_anova.html#two-way-anova",
    "title": "\n15¬† Two-way ANOVA\n",
    "section": "\n15.3 Two-way ANOVA",
    "text": "15.3 Two-way ANOVA\nResearchers have collected live specimens of two species of periwinkle (See Figure¬†15.1) from sites in northern England in the Spring and Summer. They take a measure of the gut parasite load by examining a slide of gut contents. The data are in periwinkle.txt. The data were collected to determine whether there was an effect of season or species on parasite load and whether these effects were independent.\n\n\n\n\n\nFigure¬†15.1: Periwinkles are marine gastropod molluscs (slugs and snails). A) Littorina brevicula (PD files - Public Domain, https://commons.wikimedia.org/w/index.php?curid=30577419) B) Littorina littorea. (photographed by Guttorm Flatab√∏ (user:dittaeva). - Photograph taken with an Olympus Camedia C-70 Zoom digital camera. Metainformation edited with Irfanview, possibly cropped with jpegcrop., CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=324769\n\n\n\n15.3.1 Import and explore\nImport the data:\n\nperiwinkle &lt;- read_delim(\"data-raw/periwinkle.txt\", delim = \"\\t\")\n\n\n\n\n\n\npara\nseason\nspecies\n\n\n\n58.86\nSpring\nLittorina brevicula\n\n\n51.73\nSpring\nLittorina brevicula\n\n\n54.99\nSpring\nLittorina brevicula\n\n\n39.84\nSpring\nLittorina brevicula\n\n\n65.05\nSpring\nLittorina brevicula\n\n\n67.46\nSpring\nLittorina brevicula\n\n\n60.38\nSpring\nLittorina brevicula\n\n\n54.42\nSpring\nLittorina brevicula\n\n\n47.71\nSpring\nLittorina brevicula\n\n\n66.87\nSpring\nLittorina brevicula\n\n\n51.02\nSpring\nLittorina brevicula\n\n\n43.95\nSpring\nLittorina brevicula\n\n\n62.40\nSpring\nLittorina brevicula\n\n\n55.67\nSpring\nLittorina brevicula\n\n\n58.45\nSpring\nLittorina brevicula\n\n\n43.52\nSpring\nLittorina brevicula\n\n\n69.29\nSpring\nLittorina brevicula\n\n\n71.22\nSpring\nLittorina brevicula\n\n\n64.60\nSpring\nLittorina brevicula\n\n\n58.53\nSpring\nLittorina brevicula\n\n\n51.15\nSpring\nLittorina brevicula\n\n\n70.62\nSpring\nLittorina brevicula\n\n\n55.10\nSpring\nLittorina brevicula\n\n\n47.00\nSpring\nLittorina brevicula\n\n\n54.48\nSpring\nLittorina brevicula\n\n\n43.52\nSpring\nLittorina littorea\n\n\n63.26\nSpring\nLittorina littorea\n\n\n58.76\nSpring\nLittorina littorea\n\n\n61.38\nSpring\nLittorina littorea\n\n\n63.70\nSpring\nLittorina littorea\n\n\n45.69\nSpring\nLittorina littorea\n\n\n45.20\nSpring\nLittorina littorea\n\n\n77.23\nSpring\nLittorina littorea\n\n\n57.12\nSpring\nLittorina littorea\n\n\n49.32\nSpring\nLittorina littorea\n\n\n72.92\nSpring\nLittorina littorea\n\n\n47.95\nSpring\nLittorina littorea\n\n\n78.70\nSpring\nLittorina littorea\n\n\n77.01\nSpring\nLittorina littorea\n\n\n71.70\nSpring\nLittorina littorea\n\n\n69.36\nSpring\nLittorina littorea\n\n\n66.46\nSpring\nLittorina littorea\n\n\n76.35\nSpring\nLittorina littorea\n\n\n75.55\nSpring\nLittorina littorea\n\n\n54.59\nSpring\nLittorina littorea\n\n\n62.02\nSpring\nLittorina littorea\n\n\n58.50\nSpring\nLittorina littorea\n\n\n76.09\nSpring\nLittorina littorea\n\n\n68.85\nSpring\nLittorina littorea\n\n\n84.55\nSpring\nLittorina littorea\n\n\n61.60\nSummer\nLittorina brevicula\n\n\n70.84\nSummer\nLittorina brevicula\n\n\n68.89\nSummer\nLittorina brevicula\n\n\n67.68\nSummer\nLittorina brevicula\n\n\n88.68\nSummer\nLittorina brevicula\n\n\n64.64\nSummer\nLittorina brevicula\n\n\n69.71\nSummer\nLittorina brevicula\n\n\n70.65\nSummer\nLittorina brevicula\n\n\n56.85\nSummer\nLittorina brevicula\n\n\n80.06\nSummer\nLittorina brevicula\n\n\n80.93\nSummer\nLittorina brevicula\n\n\n53.62\nSummer\nLittorina brevicula\n\n\n56.77\nSummer\nLittorina brevicula\n\n\n101.81\nSummer\nLittorina brevicula\n\n\n81.65\nSummer\nLittorina brevicula\n\n\n88.73\nSummer\nLittorina brevicula\n\n\n67.50\nSummer\nLittorina brevicula\n\n\n73.75\nSummer\nLittorina brevicula\n\n\n75.80\nSummer\nLittorina brevicula\n\n\n74.70\nSummer\nLittorina brevicula\n\n\n68.26\nSummer\nLittorina brevicula\n\n\n89.29\nSummer\nLittorina brevicula\n\n\n75.09\nSummer\nLittorina brevicula\n\n\n74.99\nSummer\nLittorina brevicula\n\n\n76.03\nSummer\nLittorina brevicula\n\n\n66.67\nSummer\nLittorina littorea\n\n\n61.31\nSummer\nLittorina littorea\n\n\n51.00\nSummer\nLittorina littorea\n\n\n67.43\nSummer\nLittorina littorea\n\n\n78.12\nSummer\nLittorina littorea\n\n\n79.26\nSummer\nLittorina littorea\n\n\n69.03\nSummer\nLittorina littorea\n\n\n76.14\nSummer\nLittorina littorea\n\n\n86.81\nSummer\nLittorina littorea\n\n\n97.56\nSummer\nLittorina littorea\n\n\n64.64\nSummer\nLittorina littorea\n\n\n68.64\nSummer\nLittorina littorea\n\n\n65.95\nSummer\nLittorina littorea\n\n\n49.50\nSummer\nLittorina littorea\n\n\n62.22\nSummer\nLittorina littorea\n\n\n57.34\nSummer\nLittorina littorea\n\n\n70.30\nSummer\nLittorina littorea\n\n\n62.75\nSummer\nLittorina littorea\n\n\n80.48\nSummer\nLittorina littorea\n\n\n81.74\nSummer\nLittorina littorea\n\n\n85.83\nSummer\nLittorina littorea\n\n\n77.51\nSummer\nLittorina littorea\n\n\n61.12\nSummer\nLittorina littorea\n\n\n59.83\nSummer\nLittorina littorea\n\n\n66.52\nSummer\nLittorina littorea\n\n\n\n\n\n\n\nThe Response variable is parasite load and it appears to be continuous. The Explanatory variables are species and season and each has two levels. It is known ‚Äútwo-way ANOVA‚Äù or ‚Äútwo-factor ANOVA‚Äù because there are two explanatory variables.\nThese data are in tidy format (Wickham 2014) - all the parasite load values are in one column (para) with the other columns indicating the species and the season. This means they are well formatted for analysis and plotting.\nIn the first instance it is sensible to create a rough plot of our data. This is to give us an overview and help identify if there are any issues like missing or extreme values. It also gives us idea what we are expecting from the analysis which will make it easier for us to identify if we make some mistake in applying that analysis.\nViolin plots (geom_violin(), see Figure¬†15.2), box plots (geom_boxplot()) or scatter plots (geom_point()) all make good choices for exploratory plotting and it does not matter which of these you choose.\n\nggplot(data = periwinkle, \n       aes(x = season, y = para, fill = species)) +\n  geom_violin()\n\n\n\n\n\n\nFigure¬†15.2: The parasite load for two species of Littorina indicated by the fill colour, in the Spring and Summer. Parasite load seems to be higher for both species in the summer and that effect looks bigger in L.brevicula - it has the lowest spring mean but the highest summer mean.\n\n\n\n\nR will order the groups alphabetically by default.\nThe figure suggests that parasite load is higher for both species in the summer and that effect looks bigger in L.brevicula - it has the lowest spring mean but the highest summer mean.\nSummarising the data for each species-season combination is the next sensible step. The most useful summary statistics are the means, standard deviations, sample sizes and standard errors. I recommend the group_by() and summarise() approach:\n\nperi_summary &lt;- periwinkle |&gt;  \n  group_by(season, species) |&gt;  \n  summarise(mean = mean(para),\n            sd = sd(para),\n            n = length(para),\n            se = sd / sqrt(n))\n\nWe have save the results to peri_summary so that we can use the means and standard errors in our plot later.\n\nperi_summary\n## # A tibble: 4 √ó 6\n## # Groups:   season [2]\n##   season species              mean    sd     n    se\n##   &lt;chr&gt;  &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n## 1 Spring Littorina brevicula  57.0  8.83    25  1.77\n## 2 Spring Littorina littorea   64.2 11.9     25  2.38\n## 3 Summer Littorina brevicula  73.5 11.2     25  2.24\n## 4 Summer Littorina littorea   69.9 11.5     25  2.30\n\nThe summary confirms both species have a higher mean in the summer and that the difference between the species is reversed - L.brevicula minus L.littorea is -7.2588 in the spring but 3.6328 in summer.\n\n15.3.2 Apply lm()\n\nWe can create a two-way ANOVA model like this:\n\nmod &lt;- lm(data = periwinkle, para ~ species * season)\n\nAnd examine the model with:\n\nsummary(mod)\n## \n## Call:\n## lm(formula = para ~ species * season, data = periwinkle)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -20.711  -6.308  -1.120   8.085  28.269 \n## \n## Coefficients:\n##                                        Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)                              56.972      2.185  26.073  &lt; 2e-16 ***\n## speciesLittorina littorea                 7.259      3.090   2.349   0.0209 *  \n## seasonSummer                             16.568      3.090   5.362 5.68e-07 ***\n## speciesLittorina littorea:seasonSummer  -10.892      4.370  -2.492   0.0144 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 10.93 on 96 degrees of freedom\n## Multiple R-squared:  0.2547, Adjusted R-squared:  0.2314 \n## F-statistic: 10.94 on 3 and 96 DF,  p-value: 3.043e-06\n\nThe Estimates in the Coefficients table give:\n\n(Intercept) known as \\(\\beta_0\\). This the mean of the group with the first level of both explanatory variables, i.e., the mean of L.brevicula group in Spring. Just as the intercept is the value of y (the response) when the value of x (the explanatory) is zero in a simple linear regression, this is the value of para when both season and species are at their first levels. The order of the levels is alphabetical by default.\nspeciesLittorina littorea known as \\(\\beta_1\\). This is what needs to be added to the L.brevicula Spring mean (the intercept) when the species goes from its first level to its second. That is, it is the difference between the L.brevicula and L.littorea means in Spring. The speciesLittorina littorea estimate is positive so the the L.littorea mean is higher than the L.brevicula mean in Spring. If we had more species we would have more estimates beginning species... and all would be comparisons to the intercept.\nseasonSummer known as \\(\\beta_2\\). This is what needs to be added to the L.brevicula Spring mean (the intercept) when the season goes from its first level to its second. That is, it is the difference between the Spring and Summer means for L.brevicula. The seasonSummer estimate is positive so the the Summer mean is higher than the Spring mean. If we had more seasons we would have more estimates beginning season... and all would be comparisons to the intercept.\nspeciesLittorina littorea:seasonSummer known as \\(\\beta_3\\). This is interaction effect. It is an additional effect. Going from L.brevicula to L.littorea adds \\(\\beta_1\\) to the intercept. Going from Spring to Summer adds \\(\\beta_2\\) to the intercept. Going from L.brevicula in Spring to L.littorea in Summer adds \\(\\beta_1 + \\beta_2 + \\beta_3\\) to the intercept. If \\(\\beta_3\\) is zero then the effect of species is the same in both seasons. If \\(\\beta_3\\) is not zero then the effect of species is different in the two seasons.\n\nThe p-values on each line are tests of whether that coefficient is different from zero.\nThe F value and p-value in the last line are a test of whether the model as a whole explains a significant amount of variation in the response variable. The model of season and species overall explains a significant amount of the variation in parasite load (p-value: 3.043e-06). To see which of the three effects are significant we can use the anova() function on our model.\nDetermine which effects are significant:\n\nanova(mod)\n## Analysis of Variance Table\n## \n## Response: para\n##                Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \n## species         1    82.2   82.17  0.6884   0.40876    \n## season          1  3092.8 3092.81 25.9108 1.778e-06 ***\n## species:season  1   741.4  741.42  6.2114   0.01441 *  \n## Residuals      96 11458.9  119.36                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe parasite load is significantly greater in Summer (F = 25.9; d.f. = 1, 96; p &lt; 0.0001) but this effect differs between species (F = 6.2; d.f. = 1,96; p = 0.014) with a greater increase in parasite load in L.brevicula than in L.littorea.\nWe need a post-hoc test to see which comparisons are significant and can again use then emmeans (Lenth 2023) package.\nLoad the package\n\nlibrary(emmeans)\n\nCarry out the post-hoc test\n\nemmeans(mod, ~ species * season) |&gt; pairs()\n##  contrast                                                estimate   SE df\n##  Littorina brevicula Spring - Littorina littorea Spring     -7.26 3.09 96\n##  Littorina brevicula Spring - Littorina brevicula Summer   -16.57 3.09 96\n##  Littorina brevicula Spring - Littorina littorea Summer    -12.94 3.09 96\n##  Littorina littorea Spring - Littorina brevicula Summer     -9.31 3.09 96\n##  Littorina littorea Spring - Littorina littorea Summer      -5.68 3.09 96\n##  Littorina brevicula Summer - Littorina littorea Summer      3.63 3.09 96\n##  t.ratio p.value\n##   -2.349  0.0943\n##   -5.362  &lt;.0001\n##   -4.186  0.0004\n##   -3.013  0.0172\n##   -1.837  0.2625\n##    1.176  0.6436\n## \n## P value adjustment: tukey method for comparing a family of 4 estimates\n\nEach row is a comparison between the two means in the ‚Äòcontrast‚Äô column. The ‚Äòestimate‚Äô column is the difference between those means and the ‚Äòp.value‚Äô indicates whether that difference is significant.\nA plot can be used to visualise the result of the post hoc which can be especially useful when there are very many comparisons.\nPlot the results of the post-hoc test:\n\nemmeans(mod, ~ species * season) |&gt; plot()\n\n\n\n\n\n\n\nWe have significant differences between:\n\n\nL.brevicula in the Spring and Summer p &lt;.0001\n\n\nL.brevicula in the Spring and L.littorea in the Summer p = 0.0004\n\n\nL.littorea in the Spring L.brevicula in the Summer p = 0.0172\n\n\n15.3.3 Check assumptions\nCheck the assumptions: All general linear models assume the ‚Äúresiduals‚Äù are normally distributed and have ‚Äúhomogeneity‚Äù of variance.\nOur first check of these assumptions is to use common sense: diameter is a continuous and we would expect it to be normally distributed thus we would expect the residuals to be normally distributed thus we would expect the residuals to be normally distributed\nWe then proceed by plotting residuals. The plot() function can be used to plot the residuals against the fitted values (See Figure¬†15.3). This is a good way to check for homogeneity of variance.\n\nplot(mod, which = 1)\n\n\n\n\n\n\nFigure¬†15.3: A plot of the residuals against the fitted values shows whether the points are distributed similarly in each group. Any difference seems small but perhaps the residuals are less variable for the lowest mean.\n\n\n\n\nWe can also use a histogram to check for normality (See Figure¬†15.4).\n\nggplot(mapping = aes(x = mod$residuals)) + \n  geom_histogram(bins = 10)\n\n\n\n\n\n\nFigure¬†15.4: A histogram of residuals is symetrical and seems consistent with a normal distribution. This is a good sign for the assumption of normally distributed residuals.\n\n\n\n\nFinally, we can use the Shapiro-Wilk test to test for normality.\n\nshapiro.test(mod$residuals)\n## \n##  Shapiro-Wilk normality test\n## \n## data:  mod$residuals\n## W = 0.98424, p-value = 0.2798\n\nThe p-value is greater than 0.05 so this test of the normality assumption is not significant.\nTaken together, these results suggest that the assumptions of normality and homogeneity of variance are probably not violated.\n\n15.3.4 Report\nWe might report this result as:\nThe parasite load is significantly greater in Summer (F = 25.9; d.f. = 1, 96; p &lt; 0.0001) but this effect differs between species (F = 6.2; d.f. = 1,96; p = 0.014) with a greater increase in parasite load in L.brevicula (from \\(\\bar{x} \\pm s.e\\): 57.0 \\(\\pm\\) 1.77 units to 73.5 \\(\\pm\\) 2.24 units) than in L.littorea (from 64.2 \\(\\pm\\) 2.38 units to 69.9 \\(\\pm\\) 2.30 units) . See Figure¬†15.5.\n\nggplot() +\n  geom_point(data = periwinkle, aes(x = season,\n                                    y = para,\n                                    shape = species),\n             position = position_jitterdodge(dodge.width = 1,\n                                             jitter.width = 0.4,\n                                             jitter.height = 0),\n             size = 3,\n             colour = \"gray50\") +\n  geom_errorbar(data = peri_summary, \n                aes(x = season, ymin = mean - se, ymax = mean + se, group = species),\n                linewidth = 0.4, size = 1,\n                position = position_dodge(width = 1)) +\n  geom_errorbar(data = peri_summary, \n                aes(x = season, ymin = mean, ymax = mean, group = species),\n                linewidth = 0.3, size = 1,\n                position = position_dodge(width = 1) ) +\n  scale_x_discrete(name = \"Season\") +\n  scale_y_continuous(name = \"Number of parasites\",\n                     expand = c(0, 0),\n                     limits = c(0, 130)) +\n  scale_shape_manual(values = c(19, 1),\n                     name = NULL,\n                     labels = c(bquote(italic(\"L.brevicula\")),\n                                bquote(italic(\"L.littorea\")))) +\n  # *L.brevicula* in the Spring and Summer `p &lt;0.0001`\n  annotate(\"segment\",\n           x = 0.75, xend = 1.75,\n           y = 115, yend = 115,\n           colour = \"black\") +\n  annotate(\"text\",\n           x = 1.25,  y = 119,\n           label = \"p &lt; 0.0001\") +\n  # # *L.brevicula* in the Spring and  *L.littorea* in the Summer `p = 0.0004`\n  annotate(\"segment\",\n           x = 0.75, xend = 2.25,\n           y = 125, yend = 125,\n           colour = \"black\") +\n  annotate(\"text\", x = 1.5,  y = 129,\n           label = \"p = 0.0004\") +\n  # *L.littorea* in the Spring *L.brevicula* in the Summer `p = 0.0172`\n  annotate(\"segment\",\n           x = 1.25, xend = 1.75,\n           y = 105, yend = 105,\n           colour = \"black\") +\n  annotate(\"text\", x = 1.5,  y = 109,\n           label = \"p = 0.0172\") +\n  theme_classic() +\n  theme(legend.title = element_blank(),\n        legend.position = c(0.85, 0.15)) \n\n\n\n\n\n\nFigure¬†15.5: Gut parasite load for two species of periwinkle Errors bars are ¬± 1 s.e.\n\n\n\n\n\n\n\n\nLenth, Russell V. 2023. ‚ÄúEmmeans: Estimated Marginal Means, Aka Least-Squares Means.‚Äù https://CRAN.R-project.org/package=emmeans.\n\n\nWickham, Hadley. 2014. ‚ÄúTidy Data.‚Äù Journal of Statistical Software, Articles 59 (10): 1‚Äì23. https://vita.had.co.nz/papers/tidy-data.pdf.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D‚ÄôAgostino McGowan, Romain Fran√ßois, Garrett Grolemund, et al. 2019. ‚ÄúWelcome to the Tidyverse‚Äù 4: 1686. https://doi.org/10.21105/joss.01686.",
    "crumbs": [
      "Statistical Analysis - Part 1",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Two-way ANOVA</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "\n16¬† Summary\n",
    "section": "",
    "text": "Incomplete\n\n\n\nYou are reading a work in progress. This page is a dumping ground for ideas. Sections maybe missing, or contain a list of points to include.",
    "crumbs": [
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "keyboard_shortcuts_tips.html",
    "href": "keyboard_shortcuts_tips.html",
    "title": "\n17¬† Keyboard short cuts and other tips\n",
    "section": "",
    "text": "Incomplete\n\n\n\nYou are reading a work in progress. This page is a dumping ground for ideas. Sections maybe missing, or contain a list of points to include.\n\n\n\n\n\n\n\n\n\n\nDescription\nItem\nWindows/ & Linux\nMac\n\n\n\nAll the Keyboard Shortcuts\n\nAlt+Shift+K\nOption+Shift+K\n\n\nInsert the Assignment operator\n&lt;-\nAlt+-\nOption+-\n\n\nOpen a new script\n\nCtrl+Shift+N\nShift+Command+N\n\n\nRun current line/selection\n\nCtrl-Enter\nCommand-Return\n\n\nComment out current line/selection\n\nCtrl+Shift+C\nShift+Command+C\n\n\nOpen help on current function\n\nF1\nF1\n\n\nInsert the pipe operator\n|&gt;\nCtrl+Shift+M\nShift+Command+M\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlt+Shift+K",
    "crumbs": [
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Keyboard short cuts and other tips</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Allaire, J. J., Charles Teague, Carlos Scheidegger, Yihui Xie, and\nChristophe Dervieux. 2022. Quarto. https://doi.org/10.5281/zenodo.5960048.\n\n\nAustralian Curriculum, Assessment, and Reporting Authority. 2015.\n‚ÄúNational Assessment Program ICT\nLiteracy.‚Äù https://nap.edu.au/_resources/D15_8761__NAP-ICT_2014_Public_Report_Final.pdf.\n\n\nBryan, Jennifer, Jim Hester, Shannon Pileggi, and E. David Aja. n.d.\nWhat They Forgot to Teach You About\nR. Accessed September 26, 2019.\n\n\nDrossel, Kerstin, and Birgit Eickelmann. 2017. ‚ÄúThe Use of Tablets\nin Secondary Schools and Its Relationship with Computer\nLiteracy.‚Äù In, 114‚Äì24. Springer International Publishing. https://doi.org/10.1007/978-3-319-74310-3_14.\n\n\nDunn, Olive Jean. 1964. ‚ÄúMultiple Comparisons Using Rank\nSums.‚Äù Technometrics 6 (3): 241‚Äì52. https://doi.org/10.1080/00401706.1964.10490181.\n\n\nKruskal, William H., and W. Allen Wallis. 1952. ‚ÄúUse of Ranks in\nOne-Criterion Variance Analysis.‚Äù Journal of the American\nStatistical Association 47 (260): 583‚Äì621. https://doi.org/10.1080/01621459.1952.10483441.\n\n\nLenth, Russell V. 2023. ‚ÄúEmmeans: Estimated Marginal Means, Aka\nLeast-Squares Means.‚Äù https://CRAN.R-project.org/package=emmeans.\n\n\nLikert, R. 1932. ‚ÄúA Technique for the Measurement of\nAttitudes.‚Äù Archives of Psychology 22 140: 55‚Äì55.\n\n\nOgle, Derek H., Jason C. Doll, A. Powell Wheeler, and Alexis Dinno.\n2023. ‚ÄúFSA: Simple Fisheries Stock Assessment Methods.‚Äù https://CRAN.R-project.org/package=FSA.\n\n\nR Core Team. 2023. R: A Language and Environment for Statistical\nComputing. Vienna, Austria: R Foundation for Statistical Computing.\nhttps://www.R-project.org/.\n\n\nRand, Emma, Chong, James, Buenabad-Chavez, Jorge, Cansdale, Annabel,\nForrester, Sarah, and Greeves, Evelyn. 2022.\n‚ÄúCloud-SPAN/00genomics: Cloud-SPAN Genomics Course\nOverview,‚Äù May. https://doi.org/10.5281/ZENODO.6564314.\n\n\nSu√°rez, Nicol√°s M., Eva Betancor, Tilman E. Klassert, Teresa Almeida,\nMariano Hern√°ndez, and Jos√© J. Pestano. 2009. ‚ÄúPhylogeography and\nGenetic Structure of the Canarian Common Chaffinch (Fringilla Coelebs)\nInferred with mtDNA and Microsatellite Loci.‚Äù Molecular\nPhylogenetics and Evolution 53 (2): 556‚Äì64. https://doi.org/10.1016/j.ympev.2009.07.018.\n\n\nTukey, John W. 1949. ‚ÄúComparing Individual Means in the Analysis\nof Variance.‚Äù Biometrics 5 (2): 99‚Äì114. https://doi.org/10.2307/3001913.\n\n\nWickham, Hadley. 2014. ‚ÄúTidy Data.‚Äù Journal of\nStatistical Software, Articles 59 (10): 1‚Äì23. https://vita.had.co.nz/papers/tidy-data.pdf.\n\n\n‚Äî‚Äî‚Äî. 2016. Ggplot2: Elegant Graphics for Data Analysis.\nSpringer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD‚ÄôAgostino McGowan, Romain Fran√ßois, Garrett Grolemund, et al. 2019.\n‚ÄúWelcome to the Tidyverse‚Äù 4:\n1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, and Jennifer Bryan. 2023. ‚ÄúReadxl: Read Excel\nFiles.‚Äù https://CRAN.R-project.org/package=readxl.\n\n\nWickham, Hadley, Romain Fran√ßois, Lionel Henry, Kirill M√ºller, and Davis\nVaughan. 2023. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2023. Readr: Read\nRectangular Text Data. https://CRAN.R-project.org/package=readr.\n\n\nWickham, Hadley, Davis Vaughan, and Maximilian Girlich. 2023. Tidyr:\nTidy Messy Data. https://CRAN.R-project.org/package=tidyr.\n\n\nWilcoxon, Frank. 1945. ‚ÄúIndividual Comparisons by Ranking\nMethods.‚Äù Biometrics Bulletin 1 (6): 80‚Äì83. https://doi.org/10.2307/3001968.\n\n\nXie, Yihui. 2022. ‚ÄúKnitr: A General-Purpose Package for Dynamic\nReport Generation in r.‚Äù https://yihui.org/knitr/.\n\n\nZhu, Hao. 2021. ‚ÄúkableExtra: Construct Complex Table with ‚ÄôKable‚Äô\nand Pipe Syntax.‚Äù https://CRAN.R-project.org/package=kableExtra.",
    "crumbs": [
      "References"
    ]
  }
]
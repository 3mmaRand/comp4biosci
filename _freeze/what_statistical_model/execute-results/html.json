{
  "hash": "c849674035940fac780fe624b223b5d8",
  "result": {
    "engine": "knitr",
    "markdown": "# What is a statistical model?\n\n\n\n\n\n::: {.callout-tip} \n## Just needs proof reading\nYou are reading a work in progress. This page is compete but needs final proof reading.\n:::\n\n\n\n\n## Overview\n\nThis section discusses statistical models which are equations\nrepresenting relationships between variables. Statistical models help us\ntest hypotheses and make predictions. The process involves estimating\nmodel \"parameters\" from data and assessing \"model fit\". Linear models\ninclude regression, *t*-tests, and ANOVA, known collectively as the\nGeneral Linear Model. The assumptions of the general linear model are\nthat the \"residuals\" are normally distribution and variance is\nhomogeneous. If the assumptions are violated we can use non-parametric\ntests.\n\n## What is a statistical model?\n\nA statistical model is a mathematical equation that helps us understand\nthe relationships between variables. We evaluate how well our data fit a\nparticular model so we can infer something about how the values arose or\nmake predictions about future values.\n\nThe equation states what has to be done to the explanatory values to get\nthe response value. For example, a simple model of plant growth might be\nthat a plant grows by 2 cm a day week after it is two weeks old. This\nmodel would be written as:\n\n$$\nh_{(t)} = h_{(0) }+ 2t\n$$ {#eq-plantgrowth-linear}\n\nWhere:\n\n-   $t$ is the time in days after the plant is two weeks old\n-   $h_{(t)}$ is the height of the plant at time $t$\n-   $h_{(0)}$ is the height of the plant at $t=0$, *i.e.*, at two weeks\n\nThis model is a linear model because the relationship between the\nresponse variable, height, and the explanatory variable, time, is linear\n(See @fig-model-1). In a linear model, the gradient of the line is the\nsame no matter what the value of $t$. In this case, it is fixed at 2 cm\nper day.\n\nOne alternative is a simple exponential model. In an exponential model,\nthe height might increase by 12% each day and the gradient of the line\nwould increase over time. This model is written as:\n\n$$\nh_{(t)} = h_{(0)}1.2^t\n$$\n\nWhere:\n\n-   $t$ is the time in days after the plant is two weeks old\n-   $h_{(t)}$ is the height of the plant at time $t$\n-   $h_{(0)}$ is the height of the plant at $t=0$, *i.e.*, at two weeks\n\nThis model is not a straight line (See @fig-model-2). The gradient of\nthe line increase as time goes on.\n\n\n\n\n::: {#fig-model .cell layout-ncol=\"2\"}\n::: {.cell-output-display}\n![Linear](what_statistical_model_files/figure-html/fig-model-1.png){#fig-model-1 width=576}\n:::\n\n::: {.cell-output-display}\n![Expontential](what_statistical_model_files/figure-html/fig-model-2.png){#fig-model-2 width=576}\n:::\n\nTwo possible models of plant growth.\n:::\n\n\n\n\n## Statistical Models and Hypothesis Testing\n\nWhen we conduct statistical analysis, we are using a statistical\nmodel â€”a mathematical framework that describes the relationship between\nexplanatory and response variables. Every model makes assumptions about\nthis relationship, and we estimate the parameters of the model from the\ndata. For example, in a simple linear model of plant growth, the key\nparameters are the intercept and the slope.\n\nStatistical hypothesis testing is directly tied to these models. When we\ntest a hypothesis, we are often assessing whether a particular model\nparameter is significantly different from zero. This is equivalent to\ntesting whether an explanatory variable has an effect on the response\nvariable.\n\nIn the context of hypothesis testing, the null hypothesis ($H_0$)\ntypically states that a given parameter is equal to zero, meaning there\nis no effect or no relationship. The alternative hypothesis ($H_1$)\nsuggests that the parameter is different from zero, indicating an\neffect.\n\nTo evaluate this, we calculate the probability of obtaining our\nestimated parameter value, or a more extreme value, *if the true\nparameter were actually zero* (*i.e.*, if $H_0$ were true). This\nprobability is the *p*-value.\n\n### Assumptions and Model Fit\n\nBecause hypothesis tests depend on probability calculations, they also\ndepend on certain assumptions about the data. A key assumption is\nthat the estimated parameter follows a normal distribution, which\nallows us to use standard probability models to determine significance.\nThis assumption is reasonable in many cases, but if it is violated, the\n*p*-values may be misleading.\n\nFor this reason, checking model assumptions is crucial. If the\nassumptions are not met, we might need to transform the data, choose\na different model, or use **non-parametric tests**, which make fewer\nassumptions about data distribution. Non-parametric methods do not\nestimate parameters like an intercept and slope, which makes them useful\nwhen standard parametric assumptions are questionable.\n\n\n## Using a linear model in practice.\n\nImagine we are studying a population of bacteria and want understand how\nnutrient availability influences its growth. We could grow the bacteria\nwith different levels of nutrients and measure the diameter of bacterial\ncolonies on agar plates in a controlled environment so that everything\nexcept the nutrient availability was identical. We could then plot the\ndiameters against the nutrient levels.\n\nWe might expect the relationship between nutrient level and growth to be\nlinear and add a line of best fit. See @fig-linear-bact.\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {#fig-linear-bact .cell layout-ncol=\"2\"}\n::: {.cell-output-display}\n![Data without a model.](what_statistical_model_files/figure-html/fig-linear-bact-1.png){#fig-linear-bact-1 width=576}\n:::\n\n::: {.cell-output-display}\n![Data with a line of best fit](what_statistical_model_files/figure-html/fig-linear-bact-2.png){#fig-linear-bact-2 width=576}\n:::\n\nThe effect of nutrient level on bacterial colony diamters without (1) and with (2) a linear model.\n:::\n\n\n\n\nThe equation of this line is a statistical model that allows us to make\npredictions about colony diameter from nutrient levels. A line - or\nlinear model - has the form:\n\n$$\ny = \\beta_{0} + \\beta_{1}x\n$$ {#eq-linear-model}\n\nWhere:\n\n-   $y$ is the response variable and $x$ is the explanatory variable.\n-   $\\beta_{0}$ is the value of $y$ when $x = 0$ usually known as the\n    intercept\n-   $\\beta_{1}$ is the amount added to $y$ for each unit increase in $x$\n    usually known as the slope\n\n$\\beta_{0}$ and $\\beta_{1}$ are called the *coefficients* - or\n*parameters* - of the model.\n\nIn this case $$\nDiameter = \\beta_{0} + \\beta_{1}Nutrient\n$$ {#eq-linear-model-bact}\n\nLinear models are amongst the most commonly used statistics. Regression,\n*t*-tests and ANOVA are all linear models collectively known as the\n*General Linear Model*.\n\n## Model fitting\n\nThe process of estimating the parameters $\\beta_{0}$ and $\\beta_{1}$\nfrom data is known as *fitting a linear model*. The line gives the\npredicted values of $y$. The actual measured value of $y$ will differ\nfrom the predicted value and this difference is called a *residual* or\nan *error*. The line is a best fit in the sense that $\\beta_{0}$ and\n$\\beta_{1}$ minimise the *sum of the squared residuals*, $SSE$.\n\n$$\nSSE = \\sum(y_{i}-\\hat{y})^2\n$$ {#eq-sse}\n\nWhere:\n\n-   $y_{i}$ represents each of the measured $y$ values from the 1st to\n    the *i*th\n-   $\\hat{y}$ is predicted value\n\nSince $\\beta_{0}$ and $\\beta_{1}$ are those that minimise the $SSE$,\nthey are described as *least squares estimates*. You do not need to\nworry about this too much but it is a useful piece of statistical jargon\nto have heard of because it pops up often. The mean of a sample is also\na least squares estimate - the sum of the squared differences between\neach value and the mean is smaller than the sum of the squared\ndifferences between each value and any other value.\n\nThe role played by $SSE$ in estimating our parameters means that it is\nalso used in determining how well our model fits our data. Our model can\nbe considered useful if the difference between the actual measured value\nof $y$ and the predicted value is small but $SSE$ will also depend on\nthe size of $y$ and the sample size. This means we express $SSE$ as a\n*proportion* of the total variation in $y$. The total variation in $y$\nis denoted $SST$:\n\n$$\n\\frac{SSE}{SST}\n$$ {#eq-residual-var}\n\n$\\frac{SSE}{SST}$ is called the *residual variation*. It is the\nproportion of variance remaining after the model fitting. In contrast,\nthe proportion of the total variance that is explained by the model is\ncalled R-squared, $R^2$. It is:\n\n$$\nR^2=1-\\frac{SSE}{SST}\n$$ {#eq-r-squared}\n\nIf there were no explanatory variables, the value we would predict for\nthe response variable is its mean. In other words, if you did not know\nthe nutrient level for a randomly chosen bacterial colony the best guess\nyou could make for its eventual diameter is the mean diameter. Thus, a\ngood model should fit the response better than the mean - that is, a\ngood model should fit the response better than a best guess. The output\nof `lm()` includes the $R^2$. It represents the proportional improvement\nin the predictions from the regression model relative to the mean model.\nIt ranges from zero, the model is no better than the mean, to 1, the\npredictions are perfect. See @fig-lm-fit\n\n![A linear model with different fits. A) the model is a poor fit - the\nexplanatory variable is no better than the response mean for predicting\nthe response. B) the model is good fit - the explanatory variable\nexplains a high proportion of the variance in the response. C) the model\nis a perfect fit - the response can be predicted perfectly from the\nexplanatory variable. Measured response values are in pink, the\npredictions are in green and the dashed blue line gives the mean of the\nresponse.](images/lm_fit.svg){#fig-lm-fit\nwidth=\"400\"}\n\nSince the distribution of the responses for a given $x$ is assumed to be\nnormal and the variances of those distributions are assumed to be\nhomogeneous, both are also true of the residuals. It is our examination\nof the residuals which allows us to evaluate whether the assumptions are\nmet.\n\nSee @fig-lm-annotated for a graphical representation of linear modelling\nterms introduced so far.\n\n![A linear model annotated with the terms used in modelling. Measured\nresponse values are in pink, the predictions are in green, and the\ndifferences between these, known as the residuals, are in blue. The\nestimated model parameters, $\\beta_{0}$ (the intercept) and $\\beta_{1}$\n(the slope) are\nindicated.](images/generic_lm.svg){#fig-lm-annotated\nwidth=\"400\"}\n\n### General linear model assumptions\n\nThe assumptions of the general linear model are that the residuals are\nnormally distributed and have homogeneity of variance. A residual is the\ndifference between the predicted and observed value\n\nIf we have a continuous response and a categorical explanatory variable\nwith two groups, we usually apply the general linear model with `lm()`\nand then check the assumptions, however, we can sometimes tell when a\nnon-parametric test would be more appropriate before that:\n\n-   Use common sense - the response should be continuous (or nearly\n    continuous, see [Ideas about data: Theory and\n    practice](ideas_about_data.html#theory-and-practice)). Consider\n    whether you would expect the response to be continuous\n-   There should decimal places and few repeated values.\n\nTo examine the assumptions after fitting the linear model, we plot the\nresiduals and test them against the normal distribution\n\n## Choice of model\n\nbefore: appropriate to the question, type of relationship. assumptions\nabout the type of model\n\nand after assumption calculations the probability calculations\n\nimplications of wrong choices: doesn't anwser the question p value is\ninaccurate conclusions are wrong\n\n## General linear models in R\n\nWe use the `lm()` function in R to analyse data with the general linear\nmodel. When you have one explanatory variable the command is:\n\n<code> lm(data = *dataframe*, *response* \\~ *explanatory*) </code>\n\nThe `response ~ explanatory` part is known as the model **formula**.\nThese must be the names of two column in the dataframe.\n\nWhen you have two explanatory variable we add the second explanatory\nvariable to the formula using a `+` or a `*`. The command is:\n\n<code> lm(data = *dataframe*, *response* \\~ *explanatory1* +\n*explanatory2*) </code>\n\nor\n\n<code> lm(data = *dataframe*, *response* \\~ *explanatory1* \\*\n*explanatory2*) </code>\n\nA model with `explanatory1 + explanatory2` considers the effects of the\ntwo variables independently. A model with `explanatory1 * explanatory2`\nconsiders the effects of the two variables *and* any interaction between\nthem. You will learn more about independent effects and interactions in\n[Two-way ANOVA](two_way_anova.html)\n\nWe usually assign the output of an `lm()` command to an object and view\nit with `summary()`. The typical workflow would be:\n\n<code> mod \\<- lm(data = *dataframe*, *response* \\~ *explanatory*)\\\nsummary(mod) </code>\n\nThere are two sorts of statistical tests in the output of\n`summary(mod)`:\n\n1.  tests of whether each coefficient is significantly different from\n    zero and,\n2.  an *F*-test of the model fit overall\n\nThe *F*-test in the last line of the output indicates whether the\nrelationship modelled between the response and the set of explanatory\nvariables is statistically significant. i.e., whether it explains a\nsignificant amount of variation.\n\n## Checking assumptions\n\nThe assumptions relate to the type of relationship chosen and the\nhypothesis testing about the parameters. For a general linear model we\nassume the relationship between diameter and nutrients is linear and we\nexamine this by plotting our data before running any tests.\n\nThe assumptions of the hypothesis testing in a general linear model are\nthat residuals are normally distributed and have homogeneity of\nvariance. A residual is the difference between the predicted and\nobserved value. We usually check these assumptions after fitting the\nlinear model by using the `plot()` function. This produces diagnostic\nplots to explore the distribution of the residuals. These cannot prove\nthe assumptions are met but allow us to quickly determine if the\nassumptions are plausible, and if not, how the assumptions are violated\nand what data points contribute to the violation.\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\nThe two diagnostic plots which are most useful are the \"Q-Q\" plot (plot\n2) and the \"Residuals vs Fitted\" plot (plot 1). These are given as\nvalues to the `which` argument of `plot()`.\n\n### The Q-Q plot\n\nThe Q-Q plot is a scatterplot of the residuals (standardised to a mean\nof zero and a standard deviation of 1) against what is expected if the\nresiduals are normally distributed.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(mod, which = 2)\n```\n\n::: {.cell-output-display}\n![](what_statistical_model_files/figure-html/unnamed-chunk-6-1.png){width=576}\n:::\n:::\n\n\n\n\nThe points should fall roughly on the line if the residuals are normally\ndistributed. In the example above, the residuals appear normally\ndistributed.\n\nThe following are two examples in which the residuals are not normally\ndistributed.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](what_statistical_model_files/figure-html/unnamed-chunk-7-1.png){width=576}\n:::\n:::\n\n\n\n\nIf you see patterns like these you should find an alternative to a\ngeneral linear model such as a non-parametric test or a generalised\nlinear model. Sometimes, applying a transformation to the response\nvariable will result in better meeting the assumptions.\n\n### The Residuals vs Fitted plot\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](what_statistical_model_files/figure-html/unnamed-chunk-8-1.png){width=576}\n:::\n:::\n\n\n\n\nThe Residuals vs Fitted plot shows if residuals have homogeneous\nvariance or non-linear patterns. Non-linear relationships between\nexplanatory variables and the response will usually show in this plot if\nthe model does not capture the non-linear relationship. For the\nassumptions to be met, the residuals should be equally spread around a\nhorizontal line as they are here:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(mod, which = 1)\n```\n\n::: {.cell-output-display}\n![](what_statistical_model_files/figure-html/unnamed-chunk-9-1.png){width=576}\n:::\n:::\n\n\n\n\nThe following are two examples in which the residuals do not have\nhomogeneous variance and display non-linear patterns.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](what_statistical_model_files/figure-html/unnamed-chunk-10-1.png){width=576}\n:::\n\n::: {.cell-output-display}\n![](what_statistical_model_files/figure-html/unnamed-chunk-10-2.png){width=576}\n:::\n:::\n\n\n\n\n## Reporting\n\nWhen reporting the results of statistical tests we need to make sure we\ntell the reader everything they need to know and give the evidence it to\nsupport it. What they need to know is given in statements describing\nwhat difference or effect is significant and the evidence is from the\ntest statistic and *p*-value from the test. You can think the\nstatistical test values as being the evidence for the statements in your\nresults sections, just as citations are the evidence for the statements\nin your introduction.\n\nIn reporting the result of a test we give:\n\n1.  the significance of effect\n\n2.  the direction of effect\n\n3.  the magnitude of effect\n\nFigures should demonstrate the statement. Ideally they will include all\nthe data and the 'model', i.e., the means and error bars or the fitted\nline. Figure legends should be concise but contain all the information\nneeded to understand the figure. I like this blog on [How to craft a\nfigure legend for scientific\npapers](https://bioturing.medium.com/how-to-craft-a-figure-legend-for-scientific-papers-b26a77fa890)\n\n## Summary\n\n1.  A statistical model is an equation that describes the relationship\n    between a response variable and one or more explanatory variables.\n\n2.  A statistical model allows you to make predictions about the\n    response variable based on the values of the explanatory variables.\n\n3.  Many statistical tests are types of \"General Linear Model\" including\n    linear regression, *t*-tests and ANOVA.\n\n4.  Statistical testing means estimating the model \"parameters\" and\n    testing whether they are significantly different from zero. The\n    parameters, also known as coefficients, are the intercept and\n    slope (s) in a General Linear Model. A *p*-value less than 0.05 for\n    the slope means there is a significant relationship between the\n    response and the explanatory variable.\n\n5.  We also consider the fit of the model to the data using the\n    *R*-squared value and the *F*-test. An *R*-squared value close to 1\n    indicates a good fit and *p*-value less than 0.05 for the *F*-test\n    indicates the model explains a significant amount of variation.\n\n6.  The assumptions of the General Linear Model must be met for the\n    *p*-values to be accurate. These are: are that the relationship\n    between the response and the explanatory variables is linear and\n    that the residuals are normally distributed and have homogeneity of\n    variance. We check these assumptions by plotting the data and the\n    residuals.\n\n7.  We use the `lm()` function to fit a linear model in R. The\n    `summary()` function gives us the test statistic, *p*-values \n    and *R*-squared value and the `plot()` function gives us diagnostic\n    plots to check the assumptions.\n    \n8.  If the assumptions are not met we apply a non-parametric test\n    using `wilcox.test` or `kruskal.test()`. These also give us a test\n    statistic and a *p*-value.\n\n9.  When reporting the results of statistical tests give the\n    significance, direction and magnitude of the effect and use figures\n    to demonstrate the statement.\n",
    "supporting": [
      "what_statistical_model_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
{
  "hash": "25e81be20884f27018876a7fb92e01e3",
  "result": {
    "engine": "knitr",
    "markdown": "# One-way ANOVA and Kruskal-Wallis\n\n\n\n\n\n::: {.callout-tip} \n## Just needs proof reading\nYou are reading a work in progress. This page is compete but needs final proof reading.\n:::\n\n\n\n\n## Overview\n\nIn the last chapter, we learnt how to use and interpret the general\nlinear model when the *x* variable was categorical with two groups. You\nwill now extend that to situations when there are more than two groups.\nThis is often known as the one-way ANOVA (**an**alysis **o**f\n**var**iance). We will also learn about the Kruskal-Wallis test\n[@kruskal1952], a non-parametric test that can be used when the\nassumptions of the general linear model are not met.\n\nWe use `lm()` to carry out a one-way ANOVA. General linear models\napplied with `lm()` are based on the normal distribution and known as\nparametric tests because they use the parameters of the normal\ndistribution (the mean and standard deviation) to determine if an effect\nis significant. Null hypotheses are about a mean or difference between\nmeans. The assumptions need to be met for the *p*-values generated to be\naccurate.\n\nIf the assumptions are not met, we can use the non-parametric equivalent\nknown as the Kruskal-Wallis test. Like other non-parametric tests, the\nKruskal-Wallis test :\n\n-   is based on the ranks of values rather than the actual values\n    themselves\n-   has a null hypothesis about the mean rank rather than the mean\n-   has fewer assumptions and can be used in more situations\n-   tends to be less powerful than a parametric test when the\n    assumptions are met\n\n<!-- Why not do several two-sample tests? ANOVA terminology and concepts -->\n\nThe process of using `lm()` to conduct a one-way ANOVA is very similar\nto using `lm()` for a two-sample *t*-test, but with an important\ndistinction. When we obtain a significant effect of our explanatory\nvariable, it only indicates that at least two group means differ‚Äîit does\nnot specify which ones. To determine where the differences lie, we need\na **post-hoc test**.\n\nA post-hoc (\"after this\") test is performed after a significant ANOVA\nresult. There are several options for post-hoc tests, and we will use\nTukey's HSD (honestly significant difference) test [@tukey1949],\nimplemented in the **`emmeans`** [@emmeans] package.\n\nPost-hoc tests adjust *p*-values to account for multiple comparisons. A\nType I error occurs when we incorrectly reject a true null hypothesis,\nwith a probability of 0.05. Conducting multiple comparisons increases\nthe likelihood of obtaining a significant result by chance. The post-hoc\ntest corrects for this increased risk, ensuring more reliable\nconclusions.\n\n### Model assumptions\n\nThe assumptions for a general linear model where the explanatory\nvariable has two or more groups, are the same as for two groups: the\nresiduals are normally distributed and have homogeneity of variance.\n\nIf we have a continuous response and a categorical explanatory variable\nwith three or more groups, we usually apply the general linear model\nwith `lm()` and *then* check the assumptions, however, we can sometimes\ntell when a non-parametric test would be more appropriate before that:\n\n-   Use common sense - the response should be continuous (or nearly\n    continuous, see [Ideas about data: Theory and\n    practice](ideas_about_data.html#theory-and-practice)). Consider\n    whether you would expect the response to be continuous\n-   There should decimal places and few repeated values.\n\nTo examine the assumptions after fitting the linear model, we plot the\nresiduals and test them against the normal distribution in the same way\nas we did for single linear regression.\n\n### Reporting\n\nIn reporting the result of one-way ANOVA or Kruskal-Wallis test, we\ninclude:\n\n1.  the significance of the effect\n\n    -   parametric (GLM): The *F*-statistic and *p*-value\n    -   non-parametric (Kruskal-Wallis): The Chi-squared statistic and\n        *p*-value\n\n2.  the direction of effect - which mean/median is greater in a the\n    pairwise commparison\n\n    -   Post-hoc test\n\n3.  the magnitude of effect - how big is the difference between the\n    means/medians\n\n    -   parametric: the means and standard errors for each group\n    -   non-parametric: the medians for each group\n\nFigures should reflect what you have said in the statements. Ideally\nthey should show both the raw data and the statistical model:\n\n-   parametric: means and standard errors\n-   non-parametric: boxplots with medians and interquartile range\n\nWe will explore all of these ideas with some examples.\n\n## üé¨ Your turn!\n\nIf you want to code along you will need to start a new [RStudio\nproject](workflow_rstudio.html#rstudio-projects), add a `data-raw`\nfolder and open a new script. You will also need to load the\n**`tidyverse`** package [@tidyverse].\n\n## One-way ANOVA\n\nResearchers wanted the determine the best growth medium for growing\nbacterial cultures. They grew bacterial cultures on three different\nmedia formulations and measured the diameter of the colonies. The three\nformulations were:\n\n-   Control - a generic medium as formulated by the manufacturer\n-   sugar added - the generic medium with added sugar\n-   sugar and amino acids added - the generic medium with added sugar\n    and amino acids\n\nThe data are in [culture.csv](data-raw/culture.csv).\n\nIn this scenario, our null hypothesis, $H_0$, is that there is no\ndifference in colony diameter between the three groups or that group\nmembership has no effect on diameter. This is written as:\n$H_0: \\beta_1 = \\beta_2 = 0$\n\nThis means that none of the group means differ significantly from each\nother.\n\nAnother way of describing, $H_0$, is to say the variation caused by\nmedia is no greater than the the random background variation. This is\nexpressed with the ***F*****-statistic**, a variance ratio that compares\nthe variance explained by the model to the variance within groups. If\n$H_0$ is true, the variance between groups is no greater than the\nvariance within groups. That is, *F*, $\\frac{Var(between)}{Var(within)}$\nequals 1. This is written as: $H_0: F = 1$\n\nBoth versions of the null hypothesis describe the same idea: that \nhere is **no real difference** between the groups.\n\n-   **Using $\\beta$ coefficients:** This version focuses on the \n    **linear model**. It says that the group variable has no \n    effect, meaning all groups have the same average value. \n    If the coefficients ($\\beta_1$, $\\beta_2$, etc.) are **zero**, \n    then the model isn‚Äôt capturing any meaningful differences between \n    groups.\n\n-   **Using the F-statistic:** This version looks at **variance**. It \n    compares the variation **between groups** to the variation \n    **within groups**. If the F-value is **close to 1**, it means the \n    differences between groups are about the same as random variation \n    within groups, so there‚Äôs no real effect.\n\nBoth approaches test the same hypothesis‚Äîif we reject the null, \nwe conclude that at least one group is different from the others.\n\n\n### Import and explore\n\nImport the data:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nculture <- read_csv(\"data-raw/culture.csv\")\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<div style=\"border: 1px solid #ddd; padding: 0px; overflow-y: scroll; height:200px; \"><table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> diameter </th>\n   <th style=\"text-align:left;position: sticky; top:0; background-color: #FFFFFF;\"> medium </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 11.22 </td>\n   <td style=\"text-align:left;\"> control </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 9.35 </td>\n   <td style=\"text-align:left;\"> control </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 9.15 </td>\n   <td style=\"text-align:left;\"> control </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 10.35 </td>\n   <td style=\"text-align:left;\"> control </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 9.63 </td>\n   <td style=\"text-align:left;\"> control </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 10.96 </td>\n   <td style=\"text-align:left;\"> control </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 10.07 </td>\n   <td style=\"text-align:left;\"> control </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 10.40 </td>\n   <td style=\"text-align:left;\"> control </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 10.33 </td>\n   <td style=\"text-align:left;\"> control </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 9.24 </td>\n   <td style=\"text-align:left;\"> control </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 8.90 </td>\n   <td style=\"text-align:left;\"> sugar added </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 10.75 </td>\n   <td style=\"text-align:left;\"> sugar added </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 11.95 </td>\n   <td style=\"text-align:left;\"> sugar added </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 9.85 </td>\n   <td style=\"text-align:left;\"> sugar added </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 10.12 </td>\n   <td style=\"text-align:left;\"> sugar added </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 10.05 </td>\n   <td style=\"text-align:left;\"> sugar added </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 9.60 </td>\n   <td style=\"text-align:left;\"> sugar added </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 10.10 </td>\n   <td style=\"text-align:left;\"> sugar added </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 10.20 </td>\n   <td style=\"text-align:left;\"> sugar added </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 10.88 </td>\n   <td style=\"text-align:left;\"> sugar added </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 10.45 </td>\n   <td style=\"text-align:left;\"> sugar and amino acids added </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 13.19 </td>\n   <td style=\"text-align:left;\"> sugar and amino acids added </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 11.84 </td>\n   <td style=\"text-align:left;\"> sugar and amino acids added </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 13.35 </td>\n   <td style=\"text-align:left;\"> sugar and amino acids added </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 11.22 </td>\n   <td style=\"text-align:left;\"> sugar and amino acids added </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 9.86 </td>\n   <td style=\"text-align:left;\"> sugar and amino acids added </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 10.27 </td>\n   <td style=\"text-align:left;\"> sugar and amino acids added </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 10.62 </td>\n   <td style=\"text-align:left;\"> sugar and amino acids added </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 11.78 </td>\n   <td style=\"text-align:left;\"> sugar and amino acids added </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 11.43 </td>\n   <td style=\"text-align:left;\"> sugar and amino acids added </td>\n  </tr>\n</tbody>\n</table></div>\n\n`````\n:::\n:::\n\n\n\n\nThe Response variable is colony diameters in millimetres and we would\nexpect it to be continuous. The Explanatory variable is type of media\nand is categorical with 3 groups. It is known ‚Äúone-way ANOVA‚Äù or\n‚Äúone-factor ANOVA‚Äù because there is only one explanatory variable. It\nwould still be one-way ANOVA if we had 4, 20 or 100 media.\n\nThese data are in tidy format [@Wickham2014-nl] - all the diameter\nvalues are in one column with another column indicating the media. This\nmeans they are well formatted for analysis and plotting.\n\nIn the first instance it is sensible to create a rough plot of our data.\nThis is to give us an overview and help identify if there are any issues\nlike missing or extreme values. It also gives us idea what we are\nexpecting from the analysis which will make it easier for us to identify\nif we make some mistake in applying that analysis.\n\nViolin plots (`geom_violin()`, see @fig-culture-rough), box plots\n(`geom_boxplot()`) or scatter plots (`geom_point()`) all make good\nchoices for exploratory plotting and it does not matter which of these\nyou choose.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = culture,\n       aes(x = medium, y = diameter)) +\n  geom_violin()\n```\n\n::: {.cell-output-display}\n![The diameters of bacterial colonies when grown in one of three media. A violin plot is a useful way to get an overview of the data and helps us identify any issues such as missing or extreme values. It also tells us what to expect from the analysis.](one_way_anova_and_kw_files/figure-html/fig-culture-rough-1.png){#fig-culture-rough width=576}\n:::\n:::\n\n\n\n\nR will order the groups alphabetically by default.\n\nThe figure suggests that adding sugar and amino acids to the medium\nincreases the diameter of the colonies.\n\nSummarising the data for each medium is the next sensible step. The most\nuseful summary statistics are the means, standard deviations, sample\nsizes and standard errors. I recommend the `group_by()` and\n`summarise()` approach:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nculture_summary <- culture %>%\n  group_by(medium) %>%\n  summarise(mean = mean(diameter),\n            std = sd(diameter),\n            n = length(diameter),\n            se = std/sqrt(n))\n```\n:::\n\n\n\n\nWe have save the results to `culture_summary` so that we can use the\nmeans and standard errors in our plot later.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nculture_summary\n## # A tibble: 3 √ó 5\n##   medium                       mean   std     n    se\n##   <chr>                       <dbl> <dbl> <int> <dbl>\n## 1 control                      10.1 0.716    10 0.226\n## 2 sugar added                  10.2 0.818    10 0.259\n## 3 sugar and amino acids added  11.4 1.18     10 0.373\n```\n:::\n\n\n\n\n### Apply `lm()`\n\nWe can create a one-way ANOVA model like this:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod <- lm(data = culture, diameter ~ medium)\n```\n:::\n\n\n\n\nAnd examine the model with:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(mod)\n## \n## Call:\n## lm(formula = diameter ~ medium, data = culture)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -1.541 -0.700 -0.080  0.424  1.949 \n## \n## Coefficients:\n##                                   Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)                        10.0700     0.2930  34.370  < 2e-16 ***\n## mediumsugar added                   0.1700     0.4143   0.410  0.68483    \n## mediumsugar and amino acids added   1.3310     0.4143   3.212  0.00339 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.9265 on 27 degrees of freedom\n## Multiple R-squared:  0.3117,\tAdjusted R-squared:  0.2607 \n## F-statistic: 6.113 on 2 and 27 DF,  p-value: 0.00646\n```\n:::\n\n\n\n\nThe Estimates in the Coefficients table give:\n\n-   `(Intercept)` known as $\\beta_0$. The mean of the control group\n    (@fig-one-way-anova-lm-model). Just as the intercept is the value of\n    the *y* (the response) when the value of *x* (the explanatory) is\n    zero in a simple linear regression, this is the value of `diameter`\n    when the `medium` is at its first level. The order of the levels is\n    alphabetical by default.\n\n-   `mediumsugar added` known as $\\beta_1$. This is what needs to be\n    added to the mean of the control group to get the mean of the\n    'medium sugar added' group (@fig-two-sample-lm-model). Just as the\n    slope is amount of *y* that needs to be added for each unit of *x*\n    in a simple linear regression, this is the amount of `diameter` that\n    needs to be added when the `medium` goes from its first level to its\n    second level (*i.e.*, one unit). The `mediumsugar added` estimate is\n    positive so the the 'medium sugar added' group mean is higher than\n    the control group mean\n\n-   `mediumsugar and amino acids added` known as $\\beta_2$ is what needs\n    to be added to the mean of the control group to get the mean of the\n    'medium sugar and amino acids added' group\n    (@fig-two-sample-lm-model). Note that it is the amount added to the\n    *intercept* (the control in this case). The\n    `mediumsugar and amino acids added` estimate is positive so the the\n    'medium sugar and amino acids added' group mean is higher than the\n    control group mean\n\nIf we had more groups, we would have more estimates and all would be\ncompared to the control group mean.\n\nThe *p*-values on each line are tests of whether that coefficient is\ndifferent from zero.\n\n-   `(Intercept)                     10.0700     0.2930  34.370  < 2e-16 ***`\n    tells us that the control group mean is significantly different from\n    zero. This is not a very interesting, it just means the control\n    colonies have a diameter.\n-   `mediumsugar added                 0.1700     0.4143   0.410  0.68483`\n    tells us that the 'medium sugar added' group mean is not\n    significantly different from the control group mean.\n-   `mediumsugar and amino acids added   1.3310     0.4143   3.212  0.00339 **`\n    tells us that the 'medium sugar and amino acids added' group mean\n    *is* significantly different from the control group mean.\n\nNote: none of this output tells us whether the medium sugar and amino\nacids added' group mean *is* significantly different from the 'medium\nsugar added' group mean. We need to do a post-hoc test for that.\n\nThe *F* value and *p*-value in the last line are a test of whether the\nmodel as a whole explains a significant amount of variation in the\nresponse variable.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![In an one-way ANOVA model with three groups, the first estimate is the intercept which is the mean of the first group. The second estimate is the 'slope' which is what has to added to the intercept to get the second group mean. The third estimate is the 'slope' which is what has to added to the intercept to get the third group mean. Note that y axis starts at 15 to create more space for the annotations.](one_way_anova_and_kw_files/figure-html/fig-one-way-anova-lm-model-1.png){#fig-one-way-anova-lm-model width=576}\n:::\n:::\n\n\n\n\nThe ANOVA is significant but this only tells us that growth medium\nmatters, meaning at least two of the means differ. To find out which\nmeans differ, we need a post-hoc test. A post-hoc (\"after this\") test is\ndone after a significant ANOVA test. There are several possible post-hoc\ntests and we will be using Tukey's HSD (honestly significant difference)\ntest [@tukey1949] implemented in the **`emmeans`** [@emmeans] package.\n\nWe need to load the package:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(emmeans)\n```\n:::\n\n\n\n\nThen carry out the post-hoc test:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemmeans(mod, ~ medium) |> pairs()\n##  contrast                                  estimate    SE df t.ratio p.value\n##  control - sugar added                        -0.17 0.414 27  -0.410  0.9117\n##  control - sugar and amino acids added        -1.33 0.414 27  -3.212  0.0092\n##  sugar added - sugar and amino acids added    -1.16 0.414 27  -2.802  0.0244\n## \n## P value adjustment: tukey method for comparing a family of 3 estimates\n```\n:::\n\n\n\n\nEach row is a comparison between the two means in the 'contrast' column.\nThe 'estimate' column is the difference between those means and the\n'p.value' indicates whether that difference is significant.\n\nA plot can be used to visualise the result of the post-hoc which can be\nespecially useful when there are very many comparisons.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemmeans(mod, ~ medium) |> plot()\n```\n\n::: {.cell-output-display}\n![](one_way_anova_and_kw_files/figure-html/unnamed-chunk-12-1.png){width=576}\n:::\n:::\n\n\n\n\nWhere the purple bars overlap, there is no significant difference.\n\nWe have found that colony diameters are significantly greater when sugar\nand amino acids are added but that adding sugar alone does not\nsignificantly increase colony diameter.\n\n### Check assumptions\n\nCheck the assumptions: All general linear models assume the \"residuals\"\nare normally distributed and have \"homogeneity\" of variance.\n\nOur first check of these assumptions is to use common sense: diameter is\na continuous and we would expect it to be normally distributed thus we\nwould expect the residuals to be normally distributed thus we would\nexpect the residuals to be normally distributed\n\nWe then proceed by plotting residuals. The `plot()` function can be used\nto plot the residuals against the fitted values (See @fig-anova1-plot1).\nThis is a good way to check for homogeneity of variance.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(mod, which = 1)\n```\n\n::: {.cell-output-display}\n![A plot of the residuals against the fitted values shows whether the points are distributed similarly in each group. Any difference seems small but perhaps the residuals are more variable for the highest mean.](one_way_anova_and_kw_files/figure-html/fig-anova1-plot1-1.png){#fig-anova1-plot1 width=576}\n:::\n:::\n\n\n\n\nPerhaps the variance is higher for the highest mean?\n\nWe can also use a histogram to check for normality (See\n@fig-anova1-plot2).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(mapping = aes(x = mod$residuals)) + \n  geom_histogram(bins = 8)\n```\n\n::: {.cell-output-display}\n![A histogram of residuals is symetrical and seems consistent with a normal distribution. This is a good sign for the assumption of normally distributed residuals.](one_way_anova_and_kw_files/figure-html/fig-anova1-plot2-1.png){#fig-anova1-plot2 width=576}\n:::\n:::\n\n\n\n\nFinally, we can use the Shapiro-Wilk test to test for normality.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshapiro.test(mod$residuals)\n## \n## \tShapiro-Wilk normality test\n## \n## data:  mod$residuals\n## W = 0.96423, p-value = 0.3953\n```\n:::\n\n\n\n\nThe p-value is greater than 0.05 so this test of the normality\nassumption is not significant.\n\nTaken together, these results suggest that the assumptions of normality\nand homogeneity of variance are probably not violated.\n\n### Report\n\nThere was a significant effect of media on the diameter of bacterial\ncolonies (*F* = 6.11; *d.f.* = 2, 27; *p* = 0.006). Post-hoc testing\nwith Tukey's Honestly Significant Difference test [@tukey1949] revealed\nthe colony diameters were significantly larger when grown with both\nsugar and amino acids ($\\bar{x} \\pm s.e$: 11.4 $\\pm$ 0.37 mm) than with\nneither (10.2 $\\pm$ 0.26 mm; *p* = 0.0092) or just sugar (10.1 $\\pm$\n0.23 mm; *p* = 0.0244). See @fig-culture.\n\n::: {#fig-culture}\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot() +\n  geom_point(data = culture, aes(x = medium, y = diameter),\n             position = position_jitter(width = 0.1, height = 0),\n             colour = \"gray50\") +\n  geom_errorbar(data = culture_summary, \n                aes(x = medium, ymin = mean - se, ymax = mean + se),\n                width = 0.3) +\n  geom_errorbar(data = culture_summary, \n                aes(x = medium, ymin = mean, ymax = mean),\n                width = 0.2) +\n  scale_y_continuous(name = \"Diameter (mm)\", \n                     limits = c(0, 16.5), \n                     expand = c(0, 0)) +\n  scale_x_discrete(name = \"Medium\", \n                   labels = c(\"Control\", \n                              \"Sugar added\", \n                              \"Sugar and amino acids added\")) +\n  annotate(\"segment\", x = 2, xend = 3, \n           y = 14, yend = 14,\n           colour = \"black\") +\n  annotate(\"text\", x = 2.5,  y = 14.5, \n           label = expression(italic(p)~\"= 0.0244\")) +\n    annotate(\"segment\", x = 1, xend = 3, \n           y = 15.5, yend = 15.5,\n           colour = \"black\") +\n  annotate(\"text\", x = 2,  y = 16, \n           label = expression(italic(p)~\"= 0.0092\")) +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](one_way_anova_and_kw_files/figure-html/unnamed-chunk-16-1.png){width=576}\n:::\n:::\n\n\n\n\n**Medium affects bacterial colony diameter**. Ten replicate colonies\nwere grown on three types of media: control, with sugar added and with\nboth sugar and amino acids added. Error bars are means $\\pm$ 1 standard\nerror. There was a significant effect of media on the diameter of\nbacterial colonies (*F* = 6.11; *d.f.* = 2, 27; *p* = 0.006). Post-hoc\ntesting with Tukey's Honestly Significant Difference test [@tukey1949]\nrevealed the colony diameters were significantly larger when grown with\nboth sugar and amino acids than with neither or just sugar. Data\nanalysis was conducted in R [@R-core] with tidyverse packages\n[@tidyverse].\n:::\n\n# Kruskal-Wallis\n\nOur examination of the assumptions revealed a possible violation of the\nassumption of homogeneity of variance. We might reasonably apply a\nnon-parametric test to this data instead.\n\nThe Kruskal-Wallis [@kruskal1952] is non-parametric equivalent of a\none-way ANOVA. The general question you have about your data - do these\ngroups differ (or does the medium effect diameter) - is the same, but\none of more of the following is true:\n\n-   the response variable is not continuous\n-   the residuals are not normally distributed\n-   the sample size is too small to tell if they are normally\n    distributed.\n-   the variance is not homogeneous\n\nSummarising the data using the median and interquartile range is more\naligned to the type of analysis than using means and standard\ndeviations:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nculture_summary <- culture |> \n  group_by(medium) |> \n  summarise(median = median(diameter),\n            interquartile  = IQR(diameter),\n            n = length(diameter))\n```\n:::\n\n\n\n\nView the results:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nculture_summary\n## # A tibble: 3 √ó 4\n##   medium                      median interquartile     n\n##   <chr>                        <dbl>         <dbl> <int>\n## 1 control                       10.2         0.968    10\n## 2 sugar added                   10.1         0.713    10\n## 3 sugar and amino acids added   11.3         1.33     10\n```\n:::\n\n\n\n\n### Apply `kruskal.test()`\n\nWe pass the dataframe and variables to `kruskal.test()` in the same way\nas we did for `lm()`. We give the data argument and a \"formula\" which\nsays `diameter ~ medium` meaning \"explain diameter by medium\".\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkruskal.test(data = culture, diameter ~ medium)\n## \n## \tKruskal-Wallis rank sum test\n## \n## data:  diameter by medium\n## Kruskal-Wallis chi-squared = 8.1005, df = 2, p-value = 0.01742\n```\n:::\n\n\n\n\nThe result of the test is given on this line:\n`Kruskal-Wallis chi-squared = 8.1005, df = 2, p-value = 0.01742`.\n`Chi-squared` is the test statistic. The *p*-value is less than 0.05\nmeaning there is a significant effect of medium on diameter.\n\nNotice that the *p*-value is a little larger than for the ANOVA. This is\nbecause non-parametric tests are generally more conservative (less\npowerful) than their parametric equivalents.\n\nA significant Kruskal-Wallis tells us at least two of the groups differ\nbut where do the differences lie? The Dunn test [@dunn1964] is a\npost-hoc multiple comparison test for a significant Kruskal-Wallis. It\nis available in the package **`FSA`** [@FSA]\n\nLoad the package using:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(FSA)\n```\n:::\n\n\n\n\nThen run the post-hoc test with:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndunnTest(data = culture, diameter ~ medium)\n##                                  Comparison          Z    P.unadj      P.adj\n## 1                     control - sugar added -0.2159242 0.82904681 0.82904681\n## 2     control - sugar and amino acids added -2.5656880 0.01029714 0.03089142\n## 3 sugar added - sugar and amino acids added -2.3497637 0.01878533 0.03757066\n```\n:::\n\n\n\n\nThe `P.adj` column gives *p*-value for the comparison listed in the\nfirst column. `Z` is the test statistic. The *p*-values are a little\nlarger for the `control - sugar and amino acids added` comparison and\nthe `sugar added - sugar and amino acids added` comparison but they are\nstill less than 0.05. This means our conclusions are the same as for the\nANOVA.\n\n### Report\n\nThere is a significant effect of media on the diameter of bacterial\ncolonies (Kruskal-Wallis: *chi-squared* = 6.34; *df* = 2; *p*-value =\n0.042) with colonies growing significantly better when both sugar and\namino acids are added to the medium. Post-hoc testing with the Dunn test\n[@dunn1964] revealed the colony diameters were significantly larger when\ngrown with both sugar and amino acids (median = 11.3 mm) than with\nneither (median = 10.2 mm; *p* = 0.031) or just sugar (median = 10.2 mm;\n*p* = 0.038). See @fig-culture-kw.\n\n::: {#fig-culture-kw}\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(data = culture, aes(x = medium, y = diameter)) +\n geom_boxplot() +\n  scale_y_continuous(name = \"Diameter (mm)\", \n                     limits = c(0, 16.5), \n                     expand = c(0, 0)) +\n  scale_x_discrete(name = \"Medium\", \n                   labels = c(\"Control\", \n                              \"Sugar added\", \n                              \"Sugar and amino acids added\")) +\n  annotate(\"segment\", x = 2, xend = 3, \n           y = 14, yend = 14,\n           colour = \"black\") +\n  annotate(\"text\", x = 2.5,  y = 14.5, \n           label = expression(italic(p)~\"= 0.038\")) +\n    annotate(\"segment\", x = 1, xend = 3, \n           y = 15.5, yend = 15.5,\n           colour = \"black\") +\n  annotate(\"text\", x = 2,  y = 16, \n           label = expression(italic(p)~\"= 0.031\")) +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](one_way_anova_and_kw_files/figure-html/unnamed-chunk-22-1.png){width=576}\n:::\n:::\n\n\n\n\n**Medium affects bacterial colony diameter**. Ten replicate colonies\nwere grown on three types of media: control, with sugar added and with\nboth sugar and amino acids added. The heavy lines indicate median\ndiameter, boxes indicate the interquartile range and whiskers the range.\nThere was a significant effect of media on the diameter of bacterial\ncolonies (Kruskal-Wallis: *chi-squared* = 6.34, *df* = 2, *p*-value =\n0.042). Post-hoc testing with the Dunn test [@dunn1964] revealed the\ncolony diameters were significantly larger when grown with both sugar\nand amino acids than with neither or just sugar. Data analysis was\nconducted in R [@R-core] with tidyverse packages [@tidyverse].\n:::\n\n# Summary\n\n1.  A linear model with one explanatory variable with two or more groups\n    is also known as a **one-way ANOVA**.\n\n2.  We estimate the **coefficients** (also called the **parameters**) of\n    the model. For a one-way ANOVA with three groups these are the mean\n    of the first group, $\\beta_0$, the difference between the means of\n    the first and second groups, $\\beta_1$, and the difference between\n    the means of the first and third groups, $\\beta_2$. We test whether\n    the parameters differ significantly from zero\n\n3.  We can use `lm()` to one-way ANOVA in R.\n\n4.  In the output of `lm()` the coefficients are listed in a table in\n    the Estimates column. The *p*-value for each coefficient is in the\n    test of whether it differs from zero. At the bottom of the output\n    there is an $F$ test of the model *overall*. Now we have more than\n    two parameters, this is different from the test on any one\n    parameter. The R-squared value is the proportion of the variance in\n    the response variable that is explained by the model. It tells us is\n    the explanatory variable is useful in predicting the response\n    variable overall.\n\n5.  When the $F$ test is significant there is a significant effect of\n    the explanatory variable on the response variable. To find out which\n    means differ, we need a **post-hoc** test. Here we use Tukey‚Äôs HSD\n    applied with the `emmeans()` and `pairs()` functions from the\n    **`emmeans`** package. Post-hoc tests make adjustments to the\n    *p*-values to account for the fact that we are doing multiple tests.\n\n6.  The assumptions of the general linear model are that the residuals\n    are normally distributed and have homogeneity of variance. A\n    residual is the difference between the predicted value and the\n    observed value.\n\n7.  We examine a histogram of the residuals and use the Shapiro-Wilk\n    normality test to check the normality assumption. We check the\n    variance of the residuals is the same for all fitted values with a\n    residuals vs fitted plot.\n\n8.  If the assumptions are not met, we can use the Kruskal-Wallis test\n    applied with `kruskal.test()` in R and follow it with The Dunn test\n    applied with `dunnTest()` in the package **`FSA`**.\n\n9.  When reporting the results of a test we give the significance,\n    direction and size of the effect. Our figures and the values we give\n    should reflect the type of test we have used. We use means and\n    standard errors for parametric tests and medians and interquartile\n    ranges for non-parametric tests. We also give the test statistic,\n    the degrees of freedom (parametric) or sample size (non-parametric)\n    and the p-value. We annotate our figures with the p-value, making\n    clear which comparison it applies to.\n",
    "supporting": [
      "one_way_anova_and_kw_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
# What is a statistical model?

```{r}
#| results: "asis"
#| echo: false

source("_common.R")
status("polishing")
```

## What is a statistical model?

A statistical model is a mathematical equation that helps us understand the relationships between variables. We evaluate how well our data fit a particular model so we can infer something about how the values arose or make predictions about future values. 

The equation states what has to be done to the explanatory values to get the response value. For example, a simple model of plant growth might be that a plant grows by 2 cm a day week after it is two weeks old. This model would be written as:

$$
h_{(t)} = h_{(0) }+ 2t
$$ {#eq-plantgrowth-linear}

Where:

-   $t$ is the time in days after the plant is two weeks old
-   $h_{(t)}$ is the height of the plant at time $t$
-   $h_{(0)}$ is the height of the plant at $t=0$, *i.e.*, at two weeks


This model is a linear model because the relationship between the response variable, height, and the explanatory variable, time, is linear (@fig-model-1). The gradient of the line is fixed at 2 cm per day. 


Alternatively, the height might increase by 12% each day. This would be a simple exponential model written as:


$$
h_{(t)} = h_{(0)}1.2^t
$$ {#eq-plantgrowth-linear}

Where:

-   $t$ is the time in days after the plant is two weeks old
-   $h_{(t)}$ is the height of the plant at time $t$
-   $h_{(0)}$ is the height of the plant at $t=0$, *i.e.*, at two weeks

This model is not a straight line (@fig-model-2). The gradient of the line increase as time goes on.

```{r}
#| echo: false
#| label: fig-model
#| layout-ncol: 2
#| fig-cap: "Two possible models of plant growth."
#| fig-subcap: 
#|   - "Linear"
#|   - "Expontential"
b0 <- 3
b1 <-  2
df <- data.frame(days = 0:20)
df <- df |> mutate(height = b0 + b1 * days)
ggplot(df, aes(x = days, y = height)) +
  geom_line(colour = pal3[1], linewidth = 1) +
  scale_x_continuous(name = "Time (days after 2 weeks)",
                     expand = c(0, 0),
                     limits = c(0, 20)) +
  scale_y_continuous(name = "Height (cm)",
                     expand = c(0, 0),
                     limits = c(0, 45)) +
  ggtitle(expression(h[(t)]~"="~h[(0)]~+~2*t)) +
  theme_classic()


b0 <- 3
b1 <-  1.12
df <- data.frame(days = 0:20)
df <- df |> mutate(height = b0 * b1^days)
ggplot(df, aes(x = days, y = height)) +
  geom_line(colour = pal3[1], linewidth = 1) +
  scale_x_continuous(name = "Time (days after 2 weeks)",
                     expand = c(0, 0),
                     limits = c(0, 20)) +
  scale_y_continuous(name = "Height (cm)",
                     expand = c(0, 0),
                     limits = c(0, 45)) +
  ggtitle(expression(h[(t)]~"="~h[(0)]*1.12^t), size = 10) +
  theme_classic()

```

<!-- ## Types of model -->

all linear models have the same form


all exponential models have the same form

but they have different parameters. the parameter values are chosen to fit that model as well as possible to the data

when we do statistics we make assumptions about the type of the model

then estimate the parameter values from the data

statistical testing determines whether the parameters differ from zero and the fit of the data to the model. determining whether the parameters differ from zero relies on calculating the probabaility or getting the parameter value we got if its true value is zero. 

this means as well as making assumptions about the type of relationship we also make assumptions about the distribution of the data. The assumption we make is that parameter is drawn from a normal distribution. This is a reasonable assumption, because many things are. However, if this assumption is  not met, then our probability will not be accurate.

for this reason, we check the assumptions of our model and test before drawing conclusions. If the assumptions are not met we take some action to transform the data,  use a different model with fewer assumptions. Non-parametric tests make fewer assumptions about the distribution of the data. They are called non-parametric because they do not estimate parameters like an intercept and gradient.





## Using a linear model in practice.

Imagine we are studying a population of bacteria and want understand how nutrient availability influences its growth. We could grow the bacteria with different levels of nutrients and measure the diameter of bacterial colonies on agar plates in a controlled environment so that everything except the nutrient availability was identical. We could then plot the diameters against the nutrient levels.

We might expect the relationship between nutrient level and growth to be linear and add a line of best fit. See @fig-linear-bact.

```{r}
#| echo: false
set.seed(123)
df1 <- data.frame(Nutrient = seq(0.1, 3, 0.1))
df1 <- df1 |> 
  mutate(Diameter = (3 + Nutrient * 2) + rnorm(length(Nutrient)))
mod <- lm(data = df1, Diameter ~ Nutrient)
b0 <- mod$coefficients[1] |> round(2)
b1 <- mod$coefficients[2] |> round(2)

```

```{r}
#| echo: false
#| label: fig-linear-bact
#| layout-ncol: 2
#| fig-cap: "The effect of nutrient level on bacterial colony diamters without (1) and with (2) a linear model."
#| fig-subcap: 
#|   - "Data without a model."
#|   - "Data with a line of best fit"
ggplot(df1, aes(x = Nutrient, y = Diameter)) +
  geom_point() +
  scale_x_continuous(expand = c(0, 0),
                     limits = c(0, 3.2)) +
  scale_y_continuous(expand = c(0, 0),
                     limits = c(0, 12)) +
  theme_classic()
ggplot(df1, aes(x = Nutrient, y = Diameter)) +
  geom_point() +
  scale_x_continuous(expand = c(0, 0),
                     limits = c(0, 3.2)) +
  scale_y_continuous(expand = c(0, 0),
                     limits = c(0, 12)) +
  geom_smooth(method = "lm", 
              se = FALSE,
              colour = "black") +
  geom_text(label = glue::glue("Diameter = {b0} + {b1} * Nutrient"),
            x = 1, y = 11) +
  theme_classic()

```

The equation of this line is a statistical model that allows us to make predictions about colony diameter from nutrient levels. A line - or linear model - has the form:

$$
y = \beta_{0} + \beta_{1}x
$$ {#eq-linear-model}

Where:

-   $y$ is the response variable and $x$ is the explanatory variable.
-   $\beta_{0}$ is the value of $y$ when $x = 0$ usually known as the intercept
-   $\beta_{1}$ is the amount added to $y$ for each unit increase in $x$ usually known as the slope

$\beta_{0}$ and $\beta_{1}$ are called the *coefficients* - or *parameters* - of the model.

In this case
$$
Diameter = \beta_{0} + \beta_{1}Nutrient
$$ {#eq-linear-model-bact}

Linear models are amongst the most commonly used statistics. Regression, *t*-tests and ANOVA are all linear models collectively known as the *General Linear Model*.


## Model fitting

The process of estimating the parameters $\beta_{0}$ and $\beta_{1}$ from data is known as *fitting a linear model*. The line gives the predicted values of $y$. The actual measured value of $y$ will differ from the predicted value and this difference is called a *residual* or an *error*. The line is a best fit in the sense that $\beta_{0}$ and $\beta_{1}$ minimise the *sum of the squared residuals*, $SSE$.

$$
SSE = \sum(y_{i}-\hat{y})^2
$$ {#eq-sse}

Where:

-   $y_{i}$ represents each of the measured $y$ values from the 1st to the *i*th
-   $\hat{y}$ is predicted value

Since $\beta_{0}$ and $\beta_{1}$ are those that minimise the $SSE$, they are described as *least squares estimates*. You do not need to worry about this too much but it is a useful piece of statistical jargon to have heard of because it pops up often. The mean of a sample is also a least squares estimate - the sum of the squared differences between each value and the mean is smaller than the sum of the squared differences between each value and any other value.

The role played by $SSE$ in estimating our parameters means that it is also used in determining how well our model fits our data. Our model can be considered useful if the difference between the actual measured value of $y$ and the predicted value is small but $SSE$ will also depend on the size of $y$ and the sample size. This means we express $SSE$ as a *proportion* of the total variation in $y$. The total variation in $y$ is denoted $SST$:

$$
\frac{SSE}{SST}
$$ {#eq-residual-var}

$\frac{SSE}{SST}$ is called the *residual variation*. It is the proportion of variance remaining after the model fitting. In contrast, the proportion of the total variance that is explained by the model is called R-squared, $R^2$. It is:

$$
R^2=1-\frac{SSE}{SST}
$$ {#eq-r-squared}

If there were no explanatory variables, the value we would predict for the response variable is its mean. In other words, if you did not know the nutrient level for a randomly chosen bacterial colony the best guess you could make for its eventual diameter is the mean diameter. Thus, a good model should fit the response better than the mean - better than a best guess. The output of `lm()` includes the $R^2$. It represents the proportional improvement in the predictions from the regression model relative to the mean model. It ranges from zero, the model is no better than the mean, to 1, the predictions are perfect. @fig-lm-fit

![A linear model with different fits. A) the model is a poor fit - the explanatory variable is no better than the response mean for predicting the response. B) the model is good fit - the explanatory variable explains a high proportion of the variance in the response. C) the model is a perfect fit - the response can be predicted perfectly from the explanatory variable. Measured response values are in pink, the predictions are in green and the dashed blue line gives the mean of the response.](images/lm_fit.svg){#fig-lm-fit width="400"}

Since the distribution of the responses for a given $x$ is assumed to be normal and the variances of those distributions are assumed to be homogeneous, both are also true of the residuals. It is our examination of the residuals which allows us to evaluate whether the assumptions are met.

See @fig-lm-annotated for a graphical representation of linear modelling terms introduced so far.


![A linear model annotated with the terms used in modelling. Measured response values are in pink, the predictions are in green, and the differences between these, known as the residuals, are in blue. The estimated model parameters, $\beta_{0}$ (the intercept) and $\beta_{1}$ (the slope) are indicated.](images/generic_lm.svg){#fig-lm-annotated width="400"}




### General linear model assumptions

The assumptions of the general linear model are that the residuals are normally distributed and have homogeneity of variance. A residual is the difference between the predicted and observed value

If we have a continuous response and a categorical explanatory variable with two groups, we usually apply the general linear model with `lm()` and then check the assumptions, however, we can sometimes tell when a non-parametric test would be more appropriate before that:

-   Use common sense - the response should be continuous (or nearly continuous, see [Ideas about data: Theory and practice](ideas_about_data.html#theory-and-practice)). Consider whether you would expect the response to be continuous
-   There should decimal places and few repeated values.

To examine the assumptions after fitting the linear model, we plot the residuals and test them against the normal distribution


## Choice of model

before: appropriate to the question, type of relationship. assumptions about the type of model 

and after assumption calculations the probability calculations

implications of wrong choices: doesn't anwser the question
p value is inaccurate
conclusions are wrong


## General linear models in R


We use the `lm()` function in R to analyse data with the general linear model. When you have one explanatory variable the command is:

<code>
lm(data = *dataframe*, *response* \~ *explanatory*)
</code>

The `response ~ explanatory` part is known as the model **formula**.

When you have two explanatory variable we add the second explanatory variable to the formula using a `+` or a `*`. The command is:

<code>
lm(data = *dataframe*, *response* \~ *explanatory1* + *explanatory2*)
</code>

or

<code>
lm(data = *dataframe*, *response* \~ *explanatory1* \* *explanatory2*)
</code>

A model with `explanatory1 + explanatory2` considers the effects of the two variables independently. A model with `explanatory1 * explanatory2` considers the effects of the two variables *and* any interaction between them. You will learn more about independent effects and interactions in [Two-way ANOVA](docs/two_way_anova.html)

We usually assign the output of `lm()` commands to an object and view it with `summary()`. The typical workflow would be:

<code> 
mod \<- lm(data = *dataframe*, *response* \~ *explanatory*)\
summary(mod) 
</code>

There are two sorts of statistical tests in the output of `summary(mod)`: tests of whether each coefficient is significantly different from zero; and an *F*-test of the model overall.

The *F*-test in the last line of the output indicates whether the relationship modelled between the response and the set of explanatory variables is statistically significant.


## Checking assumptions

TODO

The assumptions relate to the type of relationship chosen and hypothesis testing about the parameters. We assumed the relationship between diameter and nutrients is linear. The general linear model are that the residuals are normally distributed and have homogeneity of variance. A residual is the difference between the predicted and observed value



The assumptions of the model are checked using the `plot()` function which produces diagnostic plots to explore the distribution of the residuals. They are not proof of the assumptions being met but allow us to quickly determine if the assumptions are plausible, and if not, how the assumptions are violated and what data points contribute to the violation.

```{r}
#| echo: false

set.seed(1238)
b0 <- 4
b1 <- 2.5
n <- 100
x <- rnorm(n, mean = 10, sd = 2)
y <-  (b0 + b1 * x) + rnorm(n)
df <- data.frame(x = x, y = y)
mod <- lm(data = df, x ~ y)

```

The two plots which are most useful are the "Q-Q" plot (plot 2) and the "Residuals vs Fitted" plot (plot 1). These are given as values to the `which` argument of `plot()`.

### The Q-Q plot

The Q-Q plot is a scatterplot of the residuals (standardised to a mean of zero and a standard deviation of 1) against what is expected if the residuals are normally distributed.

```{r}
plot(mod, which = 2)
```

The points should fall roughly on the line if the residuals are normally distributed. In the example above, the residuals appear normally distributed.

The following are two examples in which the residuals are not normally distributed.

```{r}
#| echo: false

set.seed(100)
y <-  (b0 + b1 * x) + rexp(n,rate = 5)
df <- data.frame(x = x, y = y)
mod2 <- lm(data = df, x ~ y)
plot(mod2, which = 2)
```

If you see patterns like these you should find an alternative to a general linear model such as a non-parametric test or a generalised linear model. Sometimes, applying a transformation to the response variable will result in better meeting the assumptions.

### The Residuals vs Fitted plot

```{r}
#| echo: false

set.seed(1234)
y <-  (b0 + b1 * x) + rpois(n,lambda = 1)
df <- data.frame(x = x, y = y)
mod3 <- lm(data = df, x ~ y)
plot(mod3, which = 2)
```

The Residuals vs Fitted plot shows if residuals have homogeneous variance or non-linear patterns. Non-linear relationships between explanatory variables and the response will usually show in this plot if the model does not capture the non-linear relationship. For the assumptions to be met, the residuals should be equally spread around a horizontal line as they are here:

```{r}
plot(mod, which = 1)
```

The following are two examples in which the residuals do not have homogeneous variance and display non-linear patterns.

```{r}
#| echo: false

set.seed(100)
y <-  (b0 + b1 * x) + rexp(n,rate = 0.1)
df <- data.frame(x = x, y = y)
mod2 <- lm(data = df, x ~ y)
plot(mod2, which = 1)
plot(mod3, which = 1)
```

## Reporting

When reporting the results of statistical tests we need to make sure we tell the reader everything they need to know and give the evidence it to support it. What they need to know is given in statements describing what difference or effect is significant and the evidence is from the test statistic and *p*-value from the test. You can think the statistical test values as being the evidence for the statements in your results sections, just as citations are the evidence for the statements in your introduction.

In reporting the result of a test we give:

1.  the significance of effect

2.  the direction of effect

3.  the magnitude of effect

Figures should demonstrate the statement. Ideally they will include all the data and the 'model',i.e., means and error bars or the fitted line. Figure legends should be concise but contain all the information needed to understand the figure. https://blog.bioturing.com/2018/05/10/how-to-craft-a-figure-legend-for-scientific-papers/ - refer to figure in text




# The logic of hyothesis testing

```{r}
#| results: "asis"
#| echo: false

source("_common.R")
status("complete")
```

## What is Hypothesis testing?

Hypothesis testing is a statistical method that helps us draw 
conclusions about a population based on a sample. Since we usually 
canâ€™t measure every individual in a population, we take a smaller 
sample and analyze it.

For example, suppose we want to know if babies born to mothers in 
poverty have lower birth weights than the national average. Measuring
the birth weight of every baby in the country would be impossible, 
so we take a sample. However, even if poverty has no real effect, 
our sampleâ€™s average birth weight might be different from the national
average just by chance. Hypothesis testing helps us determine whether
this difference is real or just random variation.

## Samples and populations

Before we dive deeper into hypothesis testing, letâ€™s clarify two 
key terms:

-   A population is the entire group we are interested in studying
    (e.g., all babies in a country).
-   A sample is a smaller group selected from the population 
    (e.g., a few hundred babies chosen for the study).

We use samples because studying an entire population is often 
impractical. The challenge is making sure our sample accurately 
represents the population.

## Logic of hypothesis testing

The logic behind hypothesis testing follows these general steps:

1.  Formulating a "Null Hypothesis" denoted $H_0$. The null hypothesis
    is what we expect to happen if nothing interesting is happening. It
    states that there is no difference between groups or no relationship
    between variables. In contrast, the "Alternative Hypothesis" ($H_1$)
    states that there is a significant difference between groups or a
    relationship between variables.
2.  Designing an experiment and/or collecting data to test the null
    hypothesis.
3.  Finding the probability (the *p*-value) of getting our experimental
    data, or data more extreme, if $H_0$ is true.
4.  Deciding whether to reject or not reject the $H_0$ based on that
    probability:
    -   If $p â‰¤ 0.05$  we reject $H_0$
    -   If $p > 0.05$  do not reject $H_0$

If the null hypothesis is rejected it means we have evidence that $H_0$
is untrue and support for $H_1$. If the null hypothesis is not rejected,
it means there is insufficient evidence to support the alternative
hypothesis. 
It is important to recognise that not rejecting the null hypothesis
does not mean $H_0$ is definitely true. It just means just that $H_0$
cannot be discounted. 

There is a real state to $H_0$, that is, $H_0$ is either true or it is 
not true. The statistical test has us decide whether to reject or
not reject $H_0$ based on the *p*-value. This means we can make mistakes
when testing a hypothesis. These are called Type I and Type II errors.

### Type I and type II errors

Type I and type II errors describe the cases when we make the wrong
decision about the null hypothesis. These errors are inherent in the
approach rather than mistakes you can prevent.

-   A type I error occurs when we reject a null hypothesis that is true.
    This can be thought of as a false positive. It is a real error in
    that we have a real difference or effect. Since we use a probability
    of 0.05 to reject the null hypothesis, we will make a type I error
    5% of the time.
-   A type II error occurs when we do not reject a null hypothesis that
    is false. This is a false negative. It is not a real error in the
    sense that we only conclude we do not have enough evidence to reject
    the null hypothesis.
-   If we reject a null hypothesis that is false we have not made an
    error.
-   If we do not reject a null hypothesis that is true we have not made
    an error.

![Type I and Type II
errors.](images/type-1-2-errors.png){fig-alt="tabular representation of type I and II error. If we reject a true H0 we have made a type I error; if we do not reject a true H0 we have not made an error. If we do not reject a false H0 we have not made an error; if we do reject a false H0 we have made a type II error"}

We can decrease our chance of making a type I error by reducing the the
*p*-value required to reject the null hypothesis. However, this will
increase our chance of making a type II error. We can decrease our
chance of making a type II error by collecting enough data. The amount
of data needed will depend on the the size of the effect relative to the
random variation in the data.

## Sampling distribution of the mean

The sampling distribution of the mean is a fundamental concept in
hypothesis testing and constructing confidence intervals. Parametric
tests such as regression, two-sample tests and ANOVA (all applied with
`lm()`) are based on the sampling distribution of the mean. It is a
theoretical distribution that describes the distribution of the sample
means if an infinite number of samples were taken.

The key characteristics of the sampling distribution of the mean are:

-   The mean of the sampling distribution of the mean is equal to the
    population mean

-   The standard deviation of the sampling distribution of the mean is
    known the standard error of the mean and is always smaller than the
    standard deviation of the values. There is a fixed relationship
    between the standard deviation of a sample or population and the
    standard error of the mean: $s.e. = \frac{s.d.}{\sqrt{n}}$

ðŸ’¡ Why does this matter? When we calculate a p-value, weâ€™re really 
asking: "How likely is it to get a sample mean like ours (or more 
extreme) if $H_0$ is true. This is why understanding the sampling 
distribution is so important - it helps us determine what results we 
should expect by random chance.

```{r}
#| echo: false
#| label: fig-sample-dist-means
#| fig-cap: "The sampling distribution of the means is a theoretical distribution that describes the distribution of the sample means if an infinite number of samples of size n were taken. The sampling distribution of the mean has a standard devation which is smaller than the population and is called the standard error."
mu <- 3300
sd <- 900
n <- 12
se <- sd/sqrt(n)
mean <- 3000


ggplot(data = data.frame(mass = c(mu - 3 * sd, mu + 3 * sd)),
       aes(mass)) +
  stat_function(fun = dnorm, n = 101, 
                args = list(mean = mu, sd = sd)) +
  stat_function(fun = dnorm, n = 101, 
                args = list(mean = mu, sd = se)) +
  geom_segment(x = mu, xend = mu, y = 0, yend = dnorm(mu, mu, se)) +
  scale_y_continuous(breaks = NULL, name = "",
                     expand = c(0, 0),
                     limits = c(0, 0.0016)) +
  scale_x_continuous(breaks = NULL, name = "",
                     expand = c(0, 0),
                     limits = c(1000, 6000)) + 
  annotate("text", size = 5,
           x = 1800, y = 0.00055,
           label = "Distribution\nof the values") +
  annotate("segment",
           x = 1800, y = 0.0005,
           xend = 2000, yend = dnorm(2000, mu, sd),
           arrow = arrow(type = "closed", 
                         length = unit(0.03, "npc"))) +
  annotate("text", size = 5,
           x = 4600, y = 0.0011,
           label = "Distribution of the\nsample means") +
  annotate("segment",
           x = 4600, y = 0.001,
           xend = 3600, yend = dnorm(3600, mu, se),
           arrow = arrow(type = "closed", 
                         length = unit(0.03, "npc"))) +
  theme_classic() +
  theme(axis.line.y = element_blank())
```

### Example

Let's work through this logic using an example.

Question: National average birth weight is 3300 grams with an s.d. = 900
grams. Does maternal poverty influence birth weight?

1.  Set up the null hypothesis. The null hypothesis is what we 
    expect to happen if nothing interesting is happening. In this case,
    that there is no effect of maternal poverty on
    birth weight, *i.e.*, the mean of a sample of babies born into 
    poverty is equal to the national average (@fig-null-example). This 
    is written as $H_0: \bar{x} = 3300$. The alternative hypothesis 
    is that the sample mean is not equal to the national average. This 
    is written as $H_1: \bar{x} \neq 3300$.

```{r}
#| echo: false
#| label: fig-null-example
#| fig-cap: "Distribution of the population of birth weights has a mean of 3300 g and a standard deviation of 900. The null hypothesis is that the mean birth weight of babies born to women in poverty is the same as the national average of 3300 g."

mu <- 3300
sd <- 900
ggplot(data = data.frame(mass = c(mu - 3 * sd, mu + 3 * sd)),
       aes(mass)) +
  stat_function(fun = dnorm, n = 101, 
                args = list(mean = mu, sd = sd)) +
  geom_segment(x = mu, xend = mu, y = 0, yend = dnorm(mu, mu, sd)) +
  scale_y_continuous(breaks = NULL, name = "",
                     expand = c(0, 0),
                     limits = c(0, 0.0016)) +
  scale_x_continuous(breaks = c(1000, 2000, 3000, 4000, 5000, 6000),
                     expand = c(0, 0),
                     limits = c(1000, 6000)) + 
  annotate("text", x = 3300, y = 0.0006, label = glue::glue("mean = {mu}")) +
  theme_classic() +
  theme(axis.line.y = element_blank())
```

2.  Design an experiment that generates data to test the null 
    hypothesis[^logic_hyopthesis_testing-1]. We take a sample 
    of $n = 12$ women who live in poverty and determine the mean birth 
    weight of their babies. We calculate $\bar{x} = 3000 g$. This is 
    lower than the national average but might we get a sample like that
    even if the null hypothesis is true?

[^logic_hyopthesis_testing-1]: In fact this is an observational study
rather than a real experimental study. This means we can potential
conclude birth weight differs in the two groups but we cannot say
maternal poverty causes that difference.


3.  Determine the probability (the *p*-value) of getting our
    experimental data, or more extreme data, if $H_0$ is true.
    (@fig-sample-dist-means-example)

```{r}
#| echo: false
#| label: fig-sample-dist-means-example
#| fig-cap: "The probability of getting a sample mean of 3000 or less is the area under the sampling distribution of the mean to the left of 3000."

n <- 12
se <- sd/sqrt(n)
mean <- 3000

# function for shading under a normal distribution curve
dnorm_limit <- function(x) {
  y <- dnorm(x, mu, se)
  y[x < 2000 |  x > mean] <- NA
  return(y)
}

ggplot(data = data.frame(mass = c(mu - 3 * sd, mu + 3 * sd)),
       aes(mass)) +
  stat_function(fun = dnorm, n = 101, 
                args = list(mean = mu, sd = sd)) +
  stat_function(fun = dnorm_limit,
                geom = "area",
                fill = pal2[1]) +
  stat_function(fun = dnorm, n = 101, 
                args = list(mean = mu, sd = se)) +
  geom_segment(x = mu, xend = mu, y = 0, yend = dnorm(mu, mu, se)) +
  annotate("text", size = 5,
           x = 1800, y = 0.00055,
           label = glue::glue("Probability of {mean}\nor less"),
           colour = pal4[1])  +
  scale_y_continuous(breaks = NULL, name = "",
                     expand = c(0, 0),
                     limits = c(0, 0.0016)) +
  scale_x_continuous(breaks = c(1000, 2000, 3000, 4000, 5000, 6000),
                     expand = c(0, 0),
                     limits = c(1000, 6000)) + 
  theme_classic() +
  theme(axis.line.y = element_blank())
```

```{r}
#| echo: false
#| label: fig-sample-dist-means-example2
#| fig-cap: "The probability of getting a sample mean of 3000 or more extreme is the area under the sampling distribution of the mean to the left of 3000 plus that to the right of 3600. This is because 3600 is as extreme (as far away from the mean) as 3000."


# function for shading the other side
dnorm_limit2 <- function(x) {
  y <- dnorm(x, mu, se)
  y[x < 3600 |  x > 4500] <- NA
  return(y)
}

ggplot(data = data.frame(mass = c(mu - 3 * sd, mu + 3 * sd)),
       aes(mass)) +
  stat_function(fun = dnorm, n = 101, 
                args = list(mean = mu, sd = sd)) +
  stat_function(fun = dnorm_limit,
                geom = "area",
                fill = pal2[1]) +
  stat_function(fun = dnorm_limit2,
                geom = "area",
                fill = pal2[1]) +
  annotate("text", size = 5,
           x = 1800, y = 0.00055,
           label = glue::glue("Probability of {mean}\nor more extreme"),
           colour = pal4[1])  +
  stat_function(fun = dnorm, n = 101, 
                args = list(mean = mu, sd = se)) +
  geom_segment(x = mu, xend = mu, y = 0, yend = dnorm(mu, mu, se)) +
  scale_y_continuous(breaks = NULL, name = "",
                     expand = c(0, 0),
                     limits = c(0, 0.0016)) +
  scale_x_continuous(breaks = c(1000, 2000, 3000, 4000, 5000, 6000),
                     expand = c(0, 0),
                     limits = c(1000, 6000)) + 
  theme_classic() +
  theme(axis.line.y = element_blank())
```

4.  Decide whether to reject or not reject the $H_0$ based on that
    probability. If the shaded area is less than 0.05 we reject the null
    hypothesis and conclude there is a difference in the birth weights 
    between the two groups. If the shaded area is more than 0.05 we do 
    not reject the null hypothesis.
